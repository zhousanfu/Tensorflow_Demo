{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import zhconv #繁体字转换\n",
    "import tensorflow as tf\n",
    "# tf.keras.layers.experimental.preprocessing.TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file):\n",
    "    lines = list(open(data_file, 'r', encoding='utf-8').readlines())\n",
    "    y = [line[:1] for line in lines]\n",
    "    x = [clean_str(line[1:] for lin in lines)]\n",
    "    return [x, y]\n",
    "\n",
    "def cleaan_str(string):\n",
    "    string = re.sub(u'[\\u4e00-\\u9fa5]+', ' ', string)\n",
    "    string = zhconv.convert(string.strip(), 'zh-hans')\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "load_data(data_file='../data/文本分类/sms_pub.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
      "1 1\n",
      "tf.Tensor([0], shape=(1,), dtype=int32) \n",
      " tf.Tensor([b'\\xe5\\x88\\x9a\\xe6\\x89\\x8d\\xe2\\x80\\xa6\\xe6\\x88\\x91\\xe5\\x9c\\xa8\\xe5\\x9c\\xb0\\xe9\\x93\\x81\\xe4\\xb8\\x8a\\xe2\\x80\\xa6\\xe8\\xa2\\xab\\xe8\\xae\\xa9\\xe5\\xba\\xa7\\xe4\\xba\\x86\\xe2\\x80\\xa6\\xe8\\xbf\\x99\\xe2\\x80\\xa6\\xe6\\x88\\x91\\xe4\\xbb\\xa5\\xe4\\xb8\\xba\\xe4\\xbb\\x96\\xe6\\x98\\xaf\\xe4\\xb8\\x8b\\xe4\\xb8\\x80\\xe7\\xab\\x99\\xe4\\xb8\\x8b\\xe8\\xbd\\xa6'], shape=(1,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "  file_pattern='../data/文本分类/sms_pub.csv',\n",
    "  field_delim=',',\n",
    "  batch_size=BATCH_SIZE,\n",
    "  label_name=\"label\",\n",
    "  select_columns=['message', 'label'],\n",
    "  shuffle=True\n",
    "  )#.map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# print(next(iter(dataset)), '\\n', dataset.take(1))\n",
    "print(type(dataset))\n",
    "for feature_batch, label_batch in dataset.take(1):\n",
    "  print(len(label_batch), len(feature_batch['message']))\n",
    "  for i in range(1):\n",
    "    print(label_batch[i], '\\n', feature_batch['message'][i][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 117, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 280, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 445, in build\n        if input_shape.ndims > 1 and not input_shape[-1] == 1:  # pylint: disable=g-comparison-negation\n\n    AttributeError: 'NoneType' object has no attribute 'ndims'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 21\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m cleaned_punctuation\n\u001b[1;32m     13\u001b[0m vectorize_layer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mTextVectorization(\n\u001b[1;32m     14\u001b[0m     standardize \u001b[39m=\u001b[39m clean_text,\n\u001b[1;32m     15\u001b[0m     split \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwhitespace\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     16\u001b[0m     max_tokens \u001b[39m=\u001b[39m MAX_WORDS\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m#有一个留给占位符\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     output_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mint\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     output_sequence_length \u001b[39m=\u001b[39m MAX_LEN)\n\u001b[0;32m---> 21\u001b[0m vectorize_layer\u001b[39m.\u001b[39;49madapt(dataset\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m text, label: text))\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(vectorize_layer\u001b[39m.\u001b[39mget_vocabulary()[\u001b[39m0\u001b[39m:\u001b[39m100\u001b[39m])\n\u001b[1;32m     24\u001b[0m encoder\u001b[39m.\u001b[39mget_vocabulary()[:\u001b[39m20\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py:244\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    243\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m--> 244\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_adapt_function(iterator)\n\u001b[1;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m    246\u001b[0m       context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1129\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1129\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   1130\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1131\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 117, in adapt_step  *\n        self._adapt_maybe_build(data)\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py\", line 280, in _adapt_maybe_build  **\n        self.build(data_shape)\n    File \"/opt/homebrew/Caskroom/miniconda/base/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py\", line 445, in build\n        if input_shape.ndims > 1 and not input_shape[-1] == 1:  # pylint: disable=g-comparison-negation\n\n    AttributeError: 'NoneType' object has no attribute 'ndims'\n"
     ]
    }
   ],
   "source": [
    "MAX_WORDS = 10000  # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 200  # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "\n",
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize = clean_text,\n",
    "    split = 'whitespace',\n",
    "    max_tokens = MAX_WORDS-1, #有一个留给占位符\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = MAX_LEN)\n",
    "\n",
    "\n",
    "vectorize_layer.adapt(dataset.map(lambda text, label: text))\n",
    "print(vectorize_layer.get_vocabulary()[0:100])\n",
    "\n",
    "encoder.get_vocabulary()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "\n",
    "ds_data = tf.keras.utils.text_dataset_from_directory(\n",
    "    directory=['../data/文本分类'],\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "ds_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "    cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        key=str(element['label'][0]),\n",
    "        vocabulary_list=element['message'],\n",
    "        num_oov_buckets=2\n",
    "        )\n",
    "    categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
    "    break\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "preprocessing_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset.shuffle(500), epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
