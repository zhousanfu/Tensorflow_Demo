{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 500\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 2000  # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9  # reward discount in TD error\n",
    "LR_A = 0.001  # learning rate for actor\n",
    "LR_C = 0.001  # learning rate for critic\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"action\")\n",
    "        self.q = tf.placeholder(tf.float32, None, \"q\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,  # output units\n",
    "                activation=tf.nn.softmax,  # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.q)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, q):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.q: q}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})  # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())  # return a int\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features,n_actions, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [None, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32,[None, 1],\"action\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "        self.q_ = tf.placeholder(tf.float32,[None,1],'q_next')\n",
    "\n",
    "        self.a_onehot = tf.one_hot(self.a, n_actions, dtype=tf.float32)\n",
    "        self.a_onehot = tf.squeeze(self.a_onehot,axis=1)\n",
    "\n",
    "        self.input = tf.concat([self.s,self.a_onehot],axis=1)\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.input,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.q = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='Q'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.q_ - self.q\n",
    "            self.loss = tf.square(self.td_error)  # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "        next_a = [[i] for i in range(N_A)]\n",
    "        s_ = np.tile(s_,[N_A,1])\n",
    "        q_ = self.sess.run(self.q, {self.s: s_,self.a:next_a})\n",
    "        q_ = np.max(q_,axis=0,keepdims=True)\n",
    "        q, _ = self.sess.run([self.q, self.train_op],\n",
    "                                    {self.s: s, self.q_: q_, self.r: r,self.a:[[a]]})\n",
    "        return q\n",
    "\n",
    "\n",
    "# action有两个，即向左或向右移动小车\n",
    "# state是四维\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F,n_actions=N_A,lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "res = []\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        q = critic.learn(s, a,r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, q)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            res.append([i_episode,running_reward])\n",
    "\n",
    "            break\n",
    "\n",
    "pd.DataFrame(res,columns=['episode','ac_reward']).to_csv('../ac_reward.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import pandas as pd\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 500\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 2000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr=0.001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.int32, None, \"action\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "# action有两个，即向左或向右移动小车\n",
    "# state是四维\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=LR_A)\n",
    "critic = Critic(sess, n_features=N_F, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "res = []\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "\n",
    "        a = actor.choose_action(s)\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "\n",
    "        if done: r = -20\n",
    "\n",
    "        track_r.append(r)\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            res.append([i_episode, running_reward])\n",
    "            break\n",
    "\n",
    "pd.DataFrame(res,columns=['episode','a2c_reward']).to_csv('../a2c_reward.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "GAME = 'CartPole-v0'\n",
    "OUTPUT_GRAPH = True\n",
    "LOG_DIR = './log'\n",
    "N_WORKERS = multiprocessing.cpu_count()\n",
    "MAX_GLOBAL_EP = 500\n",
    "GLOBAL_NET_SCOPE = 'Global_Net'\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GAMMA = 0.9\n",
    "ENTROPY_BETA = 0.001\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.001    # learning rate for critic\n",
    "GLOBAL_RUNNING_R = []\n",
    "GLOBAL_EP = 0\n",
    "\n",
    "env = gym.make(GAME)\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n\n",
    "\n",
    "\n",
    "class ACNet(object):\n",
    "    def __init__(self, scope, globalAC=None):\n",
    "\n",
    "        if scope == GLOBAL_NET_SCOPE:   # get global network\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_params, self.c_params = self._build_net(scope)[-2:]\n",
    "        else:   # local net, calculate losses\n",
    "            with tf.variable_scope(scope):\n",
    "                self.s = tf.placeholder(tf.float32, [None, N_S], 'S')\n",
    "                self.a_his = tf.placeholder(tf.int32, [None, ], 'A')\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget')\n",
    "\n",
    "                self.a_prob, self.v, self.a_params, self.c_params = self._build_net(scope)\n",
    "\n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                with tf.name_scope('c_loss'):\n",
    "                    self.c_loss = tf.reduce_mean(tf.square(td)) # critic的loss是平方loss\n",
    "\n",
    "                with tf.name_scope('a_loss'):\n",
    "                    # Q * log（\n",
    "                    log_prob = tf.reduce_sum(tf.log(self.a_prob + 1e-5) *\n",
    "                                             tf.one_hot(self.a_his, N_A, dtype=tf.float32),\n",
    "                                             axis=1, keep_dims=True)\n",
    "                    exp_v = log_prob * tf.stop_gradient(td) # 这里的td不再求导，当作是常数\n",
    "                    entropy = -tf.reduce_sum(self.a_prob * tf.log(self.a_prob + 1e-5),\n",
    "                                             axis=1, keep_dims=True)  # encourage exploration\n",
    "                    self.exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "                    self.a_loss = tf.reduce_mean(-self.exp_v)\n",
    "\n",
    "                with tf.name_scope('local_grad'):\n",
    "                    self.a_grads = tf.gradients(self.a_loss, self.a_params)\n",
    "                    self.c_grads = tf.gradients(self.c_loss, self.c_params)\n",
    "\n",
    "            with tf.name_scope('sync'):\n",
    "                with tf.name_scope('pull'): # 把主网络的参数赋予各子网络\n",
    "                    self.pull_a_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.a_params, globalAC.a_params)]\n",
    "                    self.pull_c_params_op = [l_p.assign(g_p) for l_p, g_p in zip(self.c_params, globalAC.c_params)]\n",
    "                with tf.name_scope('push'): # 使用子网络的梯度对主网络参数进行更新\n",
    "                    self.update_a_op = OPT_A.apply_gradients(zip(self.a_grads, globalAC.a_params))\n",
    "                    self.update_c_op = OPT_C.apply_gradients(zip(self.c_grads, globalAC.c_params))\n",
    "\n",
    "    def _build_net(self, scope):\n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        with tf.variable_scope('actor'):\n",
    "            l_a = tf.layers.dense(self.s, 200, tf.nn.relu6, kernel_initializer=w_init, name='la')\n",
    "            a_prob = tf.layers.dense(l_a, N_A, tf.nn.softmax, kernel_initializer=w_init, name='ap') # 得到每个动作的选择概率\n",
    "        with tf.variable_scope('critic'):\n",
    "            l_c = tf.layers.dense(self.s, 100, tf.nn.relu6, kernel_initializer=w_init, name='lc')\n",
    "            v = tf.layers.dense(l_c, 1, kernel_initializer=w_init, name='v')  # 得到每个状态的价值函数\n",
    "        a_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "        c_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        return a_prob, v, a_params, c_params\n",
    "\n",
    "    def update_global(self, feed_dict):  # run by a local\n",
    "        SESS.run([self.update_a_op, self.update_c_op], feed_dict)  # local grads applies to global net\n",
    "\n",
    "    def pull_global(self):  # run by a local\n",
    "        SESS.run([self.pull_a_params_op, self.pull_c_params_op])\n",
    "\n",
    "    def choose_action(self, s):  # run by a local\n",
    "        prob_weights = SESS.run(self.a_prob, feed_dict={self.s: s[np.newaxis, :]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]),\n",
    "                                  p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        return action\n",
    "\n",
    "\n",
    "class Worker(object):\n",
    "    def __init__(self, name, globalAC):\n",
    "        self.env = gym.make(GAME).unwrapped\n",
    "        self.name = name\n",
    "        self.AC = ACNet(name, globalAC)\n",
    "\n",
    "    def work(self):\n",
    "        global GLOBAL_RUNNING_R, GLOBAL_EP\n",
    "        total_step = 1\n",
    "        buffer_s, buffer_a, buffer_r = [], [], []\n",
    "        while not COORD.should_stop() and GLOBAL_EP < MAX_GLOBAL_EP:\n",
    "            s = self.env.reset()\n",
    "            ep_r = 0\n",
    "            while True:\n",
    "                a = self.AC.choose_action(s)\n",
    "                s_, r, done, info = self.env.step(a)\n",
    "                if done: r = -5\n",
    "                ep_r += r\n",
    "                buffer_s.append(s)\n",
    "                buffer_a.append(a)\n",
    "                buffer_r.append(r)\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:   # update global and assign to local net\n",
    "                    if done:\n",
    "                        v_s_ = 0   # terminal\n",
    "                    else:\n",
    "                        v_s_ = SESS.run(self.AC.v, {self.AC.s: s_[np.newaxis, :]})[0, 0]\n",
    "                    buffer_v_target = []\n",
    "                    for r in buffer_r[::-1]:    # reverse buffer r\n",
    "                        v_s_ = r + GAMMA * v_s_ # 使用v(s) = r + v(s+1)计算target_v\n",
    "                        buffer_v_target.append(v_s_)\n",
    "                    buffer_v_target.reverse()\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_v_target = np.vstack(buffer_s), np.array(buffer_a), np.vstack(buffer_v_target)\n",
    "                    feed_dict = {\n",
    "                        self.AC.s: buffer_s,\n",
    "                        self.AC.a_his: buffer_a,\n",
    "                        self.AC.v_target: buffer_v_target,\n",
    "                    }\n",
    "                    self.AC.update_global(feed_dict)\n",
    "\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "                    self.AC.pull_global()\n",
    "\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "                if done:\n",
    "                    if len(GLOBAL_RUNNING_R) == 0:  # record running episode reward\n",
    "                        GLOBAL_RUNNING_R.append(ep_r)\n",
    "                    else:\n",
    "                        GLOBAL_RUNNING_R.append(0.99 * GLOBAL_RUNNING_R[-1] + 0.01 * ep_r)\n",
    "                    print(\n",
    "                        self.name,\n",
    "                        \"Ep:\", GLOBAL_EP,\n",
    "                        \"| Ep_r: %i\" % GLOBAL_RUNNING_R[-1],\n",
    "                          )\n",
    "                    GLOBAL_EP += 1\n",
    "                    break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SESS = tf.Session()\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        OPT_A = tf.train.RMSPropOptimizer(LR_A, name='RMSPropA')\n",
    "        OPT_C = tf.train.RMSPropOptimizer(LR_C, name='RMSPropC')\n",
    "        GLOBAL_AC = ACNet(GLOBAL_NET_SCOPE)  # we only need its params\n",
    "        workers = []\n",
    "        # Create worker\n",
    "        for i in range(N_WORKERS):\n",
    "            i_name = 'W_%i' % i   # worker name\n",
    "            workers.append(Worker(i_name, GLOBAL_AC))\n",
    "\n",
    "    # Coordinator类用来管理在Session中的多个线程，\n",
    "    # 使用 tf.train.Coordinator()来创建一个线程管理器（协调器）对象。\n",
    "    COORD = tf.train.Coordinator()\n",
    "    SESS.run(tf.global_variables_initializer())\n",
    "\n",
    "    if OUTPUT_GRAPH:\n",
    "        if os.path.exists(LOG_DIR):\n",
    "            shutil.rmtree(LOG_DIR)\n",
    "        tf.summary.FileWriter(LOG_DIR, SESS.graph)\n",
    "\n",
    "    worker_threads = []\n",
    "    for worker in workers:\n",
    "        job = lambda: worker.work()\n",
    "        t = threading.Thread(target=job) # 创建一个线程，并分配其工作\n",
    "        t.start() # 开启线程\n",
    "        worker_threads.append(t)\n",
    "    COORD.join(worker_threads) #把开启的线程加入主线程，等待threads结束\n",
    "\n",
    "    res = np.concatenate([np.arange(len(GLOBAL_RUNNING_R)).reshape(-1,1),np.array(GLOBAL_RUNNING_R).reshape(-1,1)],axis=1)\n",
    "    pd.DataFrame(res, columns=['episode', 'a3c_reward']).to_csv('../a3c_reward.csv')\n",
    "\n",
    "    plt.plot(np.arange(len(GLOBAL_RUNNING_R)), GLOBAL_RUNNING_R)\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('Total moving reward')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
