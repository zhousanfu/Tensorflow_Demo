{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhousanfu/machine-learning-demo/blob/master/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E_%E5%9F%BA%E4%BA%8E%E8%AE%BA%E6%96%87%E6%91%98%E8%A6%81%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%B8%8E%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96%E6%8C%91%E6%88%98%E8%B5%9B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTh6x7SiZ3Cw"
      },
      "source": [
        "# 科大讯飞 基于论文摘要的文本分类与关键词抽取挑战赛[🔗](https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&option=ssgy&ch=F5ZbQiB)\n",
        "\n",
        "一、赛事背景\n",
        "\n",
        "医学领域的文献库中蕴含了丰富的疾病诊断和治疗信息，如何高效地从海量文献中提取关键信息，进行疾病诊断和治疗推荐，对于临床医生和研究人员具有重要意义。\n",
        "\n",
        "二、赛事任务\n",
        "\n",
        "本任务分为两个子任务：\n",
        "机器通过对论文摘要等信息的理解，判断该论文是否属于医学领域的文献。\n",
        "提取出该论文标题、作者、摘要、关键词。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqrpB-VDIB3N"
      },
      "source": [
        "**加载网盘文件**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayHlB1zPaIEm",
        "outputId": "093b6590-2bea-432a-c0da-5de79dcc7e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!cp /content/drive/MyDrive/Data/科大讯飞比赛/* ./data"
      ],
      "metadata": {
        "id": "yZ806f_IPrO6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2f7EK5vWgZU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[tensorflow]\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bN4PStWf2WHD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, TFAutoModel, TFBertForSequenceClassification, TFAutoModelForTokenClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4h19VLOZ3C1"
      },
      "source": [
        "## 任务一: 分类"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXWKS4G0hmWD"
      },
      "source": [
        "### 超参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS0uP5d_wVEf"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000  # 词汇表大小\n",
        "embedding_dim = 100  # 词向量维度\n",
        "hidden_size = 128  # LSTM隐藏层大小\n",
        "max_sequence_length = 128\n",
        "batch_size = 16\n",
        "\n",
        "num_classes = 2  # 分类任务类别数，这里为医学文献和非医学文献\n",
        "num_extraction = 10  # 提取任务提取的关键词数目\n",
        "epochs_classes = 10\n",
        "epochs_keywords = 10\n",
        "classification_learning_rate = 0.001\n",
        "extraction_learning_rate = 0.001\n",
        "weight_decay = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据预处理"
      ],
      "metadata": {
        "id": "G-f-So7O3RUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载数据和标签\n",
        "data = pd.read_csv('data/train.csv')[:100].to_dict(orient='list')\n",
        "sentences = data['abstract']\n",
        "labels = data['label']\n",
        "\n",
        "# 划分训练集和验证集\n",
        "train_data, eval_data, train_labels, eval_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 初始化BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 编码训练集和验证集的输入文本\n",
        "train_encodeds = tokenizer.batch_encode_plus(train_data, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "eval_encodeds = tokenizer.batch_encode_plus(eval_data, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "# input_ids = tf.convert_to_tensor(encodeds['input_ids'])\n",
        "# attention_mask = tf.convert_to_tensor(encodeds['attention_mask'])\n",
        "# labels = tf.convert_to_tensor(labels)\n",
        "\n",
        "# 转换为TensorFlow Dataset格式\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': train_encodeds['input_ids'], 'attention_mask': train_encodeds['attention_mask']}, train_labels) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "eval_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': eval_encodeds['input_ids'], 'attention_mask': eval_encodeds['attention_mask']}, eval_labels) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "\n",
        "# 测试数据集\n",
        "data = pd.read_csv('data/test.csv')[:100].to_dict(orient='list')\n",
        "sentences = data['abstract']\n",
        "\n",
        "test_encodeds = tokenizer.batch_encode_plus(sentences, truncation=True, padding='max_length', max_length=max_sequence_length, return_tensors='tf')\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices( \\\n",
        "    ({'input_ids': train_encodeds['input_ids'], 'attention_mask': train_encodeds['attention_mask']}) \\\n",
        "    ).shuffle(num_classes).batch(batch_size)\n",
        "\n",
        "test_dataset"
      ],
      "metadata": {
        "id": "YC-CsEC2Jkzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF11TCKUjUdS"
      },
      "source": [
        "### 构建模型"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义TFBertForSequenceClassification模型\n",
        "classes_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "\n",
        "# 定义优化器和损失函数\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=classification_learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 定义评估指标\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('eval_accuracy')\n",
        "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
        "eval_loss = tf.keras.metrics.Mean('eval_loss', dtype=tf.float32)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "    predictions = None\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = classes_model(inputs, training=True)[0]\n",
        "        loss_value = loss(labels, outputs)\n",
        "        predictions = tf.argmax(outputs, axis=1)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, classes_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, classes_model.trainable_variables))\n",
        "\n",
        "    train_accuracy(labels, outputs)\n",
        "    train_loss(loss_value)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "@tf.function\n",
        "def eval_step(inputs, labels):\n",
        "    outputs = classes_model(inputs, training=False)[0]\n",
        "    loss_value = loss(labels, outputs)\n",
        "\n",
        "    eval_accuracy(labels, outputs)\n",
        "    eval_loss(loss_value)\n",
        "\n",
        "    predictions = tf.argmax(outputs, axis=1)\n",
        "\n",
        "    return predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "Nvl6-sK_I9pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 训练模型"
      ],
      "metadata": {
        "id": "o8HhloKHvgMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "for epoch in range(3):\n",
        "    train_accuracy.reset_states()\n",
        "    train_loss.reset_states()\n",
        "    eval_accuracy.reset_states()\n",
        "    eval_loss.reset_states()\n",
        "\n",
        "    train_predictions = []\n",
        "    eval_predictions = []\n",
        "    train_f1 = 0\n",
        "    eval_f1 = 0\n",
        "\n",
        "    for batch_inputs, batch_labels in train_dataset:\n",
        "        predictions = train_step(batch_inputs, batch_labels)\n",
        "        train_predictions.extend(predictions)\n",
        "\n",
        "    for batch_inputs, batch_labels in eval_dataset:\n",
        "        predictions = eval_step(batch_inputs, batch_labels)\n",
        "        eval_predictions.extend(predictions)\n",
        "\n",
        "    train_f1 = f1_score(train_labels, train_predictions)\n",
        "    eval_f1 = f1_score(eval_labels, eval_predictions)\n",
        "\n",
        "    print('Epoch {}: \\n训练: Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f}, \\n验证: Loss: {:.4f}, Accuracy: {:.4f}, F1: {:.4f},'.format(\n",
        "        epoch + 1, train_loss.result(), train_accuracy.result(), train_f1, eval_loss.result(), eval_accuracy.result(), eval_f1\n",
        "    ))"
      ],
      "metadata": {
        "id": "eED1ZgT7KZQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型预测"
      ],
      "metadata": {
        "id": "qzdxn5gG4bww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = classes_model.predict(test_dataset)\n",
        "probabilities = tf.nn.softmax(outputs.logits, axis=1)\n",
        "\n",
        "for i in probabilities:\n",
        "    predicted_label = tf.argmax([i], axis=1).numpy()[0]\n",
        "    print(\"预测标签:\", predicted_label, i.numpy())\n",
        "    break"
      ],
      "metadata": {
        "id": "E57UJvbhhH5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 任务二: 命名实体抽取"
      ],
      "metadata": {
        "id": "I73UWTdXWrUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_addons transformers\n",
        "!pip install keras-bert\n",
        "!pip install bert-for-tf2\n",
        "!pip install tf2crf"
      ],
      "metadata": {
        "id": "9SS26xDYfdiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from transformers import TFBertModel, BertTokenizer, TFAutoModel, TFBertForSequenceClassification, TFAutoModelForTokenClassification\n",
        "\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import keras\n",
        "# from tf.keras.models import Model, Input, Sequential\n",
        "# from tf.keras.layers import GRU, LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, BatchNormalization, Add\n",
        "# from tf.keras.utils import to_categorical\n",
        "# from tf.keras.callbacks import CSVLogger\n",
        "# from tf.keras.optimizers import Adam\n",
        "# from tf.keras_bert import load_trained_model_from_checkpoint\n",
        "from tf2crf import CRF, ModelWithCRFLoss\n",
        "import bert\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# 加载预训练的BERT模型\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 获取BERT的tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "ZMTwhGViimD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 512\n",
        "hidden_size = 100\n",
        "num_tags = 4\n",
        "lstm_units = 64  # BiLSTM单元数量\n",
        "num_words = 10000  # 词汇表大小\n",
        "embedding_dim = 100  # BERT嵌入维度"
      ],
      "metadata": {
        "id": "ye3YJWvWMo3l"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据预处理\n",
        "\n",
        "首先，需要准备一个含有实体标注的文本语料库\n",
        "对于每个文本，需要将其分成单独的句子，并将每个句子分词。\n",
        "对于每个词，需要将其转化为对应的BERT词表中的ID。\n",
        "对于标注序列，需要将其转化为BIO格式（即Begin，Inside，Outside）。"
      ],
      "metadata": {
        "id": "8YqeKblIv8QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据预处理\n",
        "def preprocess_data(texts, labels, tokenizer, label2id, max_length):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    encoded_labels = []\n",
        "\n",
        "    for text, label in zip(texts, labels):\n",
        "        # 分词，并添加特殊标记[CLS]和[SEP]，并进行id编码和注意力掩码生成\n",
        "        input_encode = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_length,\n",
        "                                             padding='max_length', truncation=True, return_token_type_ids=False)\n",
        "        input_ids.append(input_encode['input_ids'])\n",
        "        attention_masks.append(input_encode['attention_mask'])\n",
        "\n",
        "        # 编码标签序列\n",
        "        encoded_label = [label2id[l] for l in label]\n",
        "        while len(encoded_label) < max_length:\n",
        "            encoded_label.append(1)\n",
        "        encoded_labels.append(encoded_label)\n",
        "\n",
        "    return input_ids, attention_masks, encoded_labels\n",
        "\n",
        "# 构建标签字典\n",
        "def build_label_dict(labels):\n",
        "    unique_labels = set([label for sublist in labels for label in sublist])\n",
        "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
        "    id2label = {i: label for label, i in label2id.items()}\n",
        "\n",
        "    return label2id, id2label\n"
      ],
      "metadata": {
        "id": "abGzeG17M0jJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/train.csv')[:100]\n",
        "texts = df['title'] + ' ' + df['author'] + ' ' + df['abstract']\n",
        "label = df['Keywords']\n",
        "labels = []\n",
        "labels_bio = []\n",
        "\n",
        "for i in label:\n",
        "    tokens = word_tokenize(i)\n",
        "    for t in tokens:\n",
        "        if t != ';' and t != '.':\n",
        "            labels.append(t)\n",
        "\n",
        "labels = list(set(labels))\n",
        "\n",
        "for i in range(len(texts)):\n",
        "    tmp_l = []\n",
        "    tokens = word_tokenize(texts[i])\n",
        "\n",
        "    for t in tokens:\n",
        "        if len(tmp_l) <= max_length:\n",
        "            if t in labels:\n",
        "                tmp_l.append('B-Key')\n",
        "            else:\n",
        "                tmp_l.append('O')\n",
        "        labels_bio.append(tmp_l)\n",
        "\n",
        "# 构建标签字典\n",
        "label2id, id2label = build_label_dict(labels_bio)\n",
        "\n",
        "# 数据预处理\n",
        "input_ids, attention_masks, encoded_labels = preprocess_data(texts, labels_bio, tokenizer, label2id, max_length=max_length)\n",
        "\n",
        "for i in range(len(input_ids)):\n",
        "    if len(input_ids[i]) != len(encoded_labels[i]):\n",
        "        print(len(input_ids[i]), len(encoded_labels[i]))\n",
        "\n",
        "# 划分训练集和验证集\n",
        "train_input_ids, test_input_ids,\\\n",
        "    train_attention_masks, test_attention_masks,\\\n",
        "    train_labels, test_labels = train_test_split(input_ids, attention_masks, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_input_ids = tf.convert_to_tensor(train_input_ids, dtype=tf.int32)\n",
        "train_attention_masks = tf.convert_to_tensor(train_attention_masks, dtype=tf.int32)\n",
        "train_labels = np.array(train_labels)\n",
        "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
        "\n",
        "test_input_ids = tf.convert_to_tensor(test_input_ids, dtype=tf.int32)\n",
        "test_attention_masks = tf.convert_to_tensor(test_attention_masks, dtype=tf.int32)\n",
        "test_labels = np.array(test_labels)\n",
        "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
        "\n",
        "print(train_input_ids.shape)\n",
        "print(train_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "072EBEs1dMy0",
        "outputId": "edfa6658-c486-4141-dce3-dc63eb297e02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 512)\n",
            "(80, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_G6xlQzQC73",
        "outputId": "c1486528-83b6-48aa-fe9e-3e40f6ae91c2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80, 512), dtype=int32, numpy=\n",
              "array([[  101,  3808,  9312, ..., 13004,  3938,   102],\n",
              "       [  101,  8208,  1997, ...,     0,     0,     0],\n",
              "       [  101, 11538,  1996, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  9203,  3496, ...,     0,     0,     0],\n",
              "       [  101, 19002,  1011, ...,     0,     0,     0],\n",
              "       [  101,  2659, 11619, ...,     0,     0,     0]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 构建模型"
      ],
      "metadata": {
        "id": "u8rau2hyhDIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.backend import dtype\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# BERT + BiLSTM + CRF Model\n",
        "def build_model(bert_model, num_tags, max_length, hidden_size, lstm_units):\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_length,), name='input_ids', dtype=tf.int32)\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_length,), name='attention_mask', dtype=tf.int32)\n",
        "    # token_type_ids = tf.keras.layers.Input(shape=(max_length,), name='token_type_ids', dtype=tf.int32)\n",
        "\n",
        "    # BERT layer\n",
        "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
        "\n",
        "    # BiLSTM layer\n",
        "    bilstm_layer = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_size, return_sequences=True), merge_mode=\"sum\")\n",
        "    bilstm_layer_out = bilstm_layer(tf.keras.layers.Dropout(0.5)(bert_output))\n",
        "\n",
        "    #     lstm_layer = LSTM(hidden_size, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)\n",
        "    #     lstm_layer_out = lstm_layer(bilstm_layer_out)\n",
        "\n",
        "    time_distributed_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(num_tags))\n",
        "    time_distributed_layer_out = time_distributed_layer(bilstm_layer_out)\n",
        "\n",
        "    # CRF layer\n",
        "    crf = CRF(dtype='float32') # float32\n",
        "    crf_out = crf(tf.keras.layers.BatchNormalization()(time_distributed_layer_out))\n",
        "\n",
        "    base_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=crf_out)\n",
        "    model = ModelWithCRFLoss(bert_model)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5)) # loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy']\n",
        "    # model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "# 构建模型\n",
        "model = build_model(bert_model, num_tags, max_length, hidden_size, lstm_units)\n",
        "\n",
        "# 训练模型\n",
        "csv_logger = tf.keras.callbacks.CSVLogger('bert-ner-training.log',  append=False)\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"./bert-gru-bilstm-crf.tf\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    verbose=0,\n",
        "    save_best_only=True)\n",
        "\n",
        "model.fit([train_input_ids, train_attention_masks], train_labels,\n",
        "          batch_size=32,\n",
        "          epochs=10,\n",
        "          callbacks=[csv_logger, checkpointer])\n",
        "\n",
        "# 评估模型\n",
        "# evaluation = model.evaluate([test_input_ids, test_attention_masks], test_labels)\n",
        "# loss = evaluation[0]\n",
        "# accuracy = evaluation[1]\n",
        "# print(\"Loss: {:.4f}\".format(loss))\n",
        "# print(\"Accuracy: {:.4f}\".format(accuracy))\n",
        "\n",
        "# 预测\n",
        "predictions = model.predict([test_input_ids, test_attention_masks])"
      ],
      "metadata": {
        "id": "KdfRFjMPjUXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "w4h19VLOZ3C1",
        "tXWKS4G0hmWD",
        "G-f-So7O3RUU",
        "VF11TCKUjUdS",
        "o8HhloKHvgMZ",
        "qzdxn5gG4bww"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}