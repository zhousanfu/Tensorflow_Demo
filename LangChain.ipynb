{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhousanfu/machine-learning-demo/blob/master/LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbEDIhXr5QYP"
      },
      "outputs": [],
      "source": [
        "!pip install langchain faiss-cpu\n",
        "!pip install transformers sentence_transformers sentencepiece cpm_kernels\n",
        "!pip install google-search-results -i pypi.douban.com/simple --trusted-host pypi.douban.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chatglm"
      ],
      "metadata": {
        "id": "7uiRt-JmBtjz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbBCts3O3mX8"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "class chatGLM():\n",
        "    def __init__(self, model_name, quantization_bit=4) -> None:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda().eval()\n",
        "        else:\n",
        "            self.model = model = AutoModel.from_pretrained(model_name, trust_remote_code=True).float()\n",
        "        self.model = self.model.quantize(quantization_bit)\n",
        "\n",
        "    def __call__(self, prompt, history=None) -> Any:\n",
        "        response, history = self.model.chat(self.tokenizer , prompt, history=history) # 这里演示未使用流式接口. stream_chat()\n",
        "        return response, history\n",
        "\n",
        "llm = chatGLM(model_name=\"THUDM/chatglm-6B-int4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response, history = llm(prompt=\"你好\", history=[])\n",
        "print(\"response: %s\"%response)\n",
        "response, history = llm(prompt=\"我最近有点失眠怎么办?\", history=history)\n",
        "print(\"response: %s\"%response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8MrQIdOBhtc",
        "outputId": "1a5783c1-ff3d-4cd5-cfdc-a253c3ef54d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.THUDM.chatglm-6B-int4.6c5205c47d0d2f7ea2e44715d279e537cae0911f.modeling_chatglm:The dtype of attention mask (torch.int64) is not bool\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response: 你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n",
            "response: 失眠的话，建议尝试下述方法进行改善：\n",
            "1. 睡前放松：在睡前半小时尽量避免进行刺激性的活动，放松身心，例如泡个热水澡、听轻柔的音乐等。\n",
            "2. 建立规律的睡眠时间：保持每天固定的睡眠时间和起床时间，帮助身体建立一个健康的睡眠节律。\n",
            "3. 改善睡前环境：例如调暗灯光、保持安静、减少噪音等，或者尝试使用放松性质的布置，如舒适、柔软的床上用品等。\n",
            "4. 避免咖啡因和酒精：这些物质会影响睡眠，尽量避免摄入。\n",
            "5. 锻炼身体：适度的运动可以提高身体的代谢水平，帮助入睡。\n",
            "如果上述方法无法改善失眠，建议咨询医生或专业的医疗机构，寻求更进一步的帮助。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain\n",
        "\n",
        "LangChain是一个强大的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由大型语言模型 (LLM) 和聊天模型提供支持的应用程序的过程。LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源"
      ],
      "metadata": {
        "id": "hMURU65yCiBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # 官方llm使用OPENAI 接口\n",
        "# from langchain.llms import OpenAI\n",
        "# llm = OpenAI(model_name=\"text-davinci-003\")\n",
        "# prompt = \"你好\"\n",
        "# response = llm(prompt)"
      ],
      "metadata": {
        "id": "RS4PLFqgSXtz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### google-search"
      ],
      "metadata": {
        "id": "dS7ZeLoMv3dY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.agents import load_tools\n",
        "# import os\n",
        "\n",
        "\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = '你的api key'\n",
        "\n",
        "# tools = load_tools([\"serpapi\", \"python_repl\", \"llm-math\"], llm=llm)\n",
        "# agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "# agent.run(\"当前金价是多少一克？\")"
      ],
      "metadata": {
        "id": "eWOG03owv-Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prompt 提示\n",
        "填入内容来引导大模型输出"
      ],
      "metadata": {
        "id": "7SE976V_CuP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "template = \"\"\"什么是{query},还有如何真正做到并细说实现步骤\"\"\"\n",
        "prompt_tem = PromptTemplate(input_variables=[\"query\"], template=template)\n",
        "prompt = prompt_tem.format(query='阶级跳跃')\n",
        "\n",
        "response, history = llm(prompt=prompt, history=[])\n",
        "print(prompt, '-->', response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPt2QqzyCtsB",
        "outputId": "d25150b4-1db3-4645-9b25-54fe71dd61b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "什么是阶级跳跃,还有如何真正做到并细说实现步骤 --> 阶级跳跃是指在某个游戏中，玩家通过积累特定的道具和技能，从普通玩家变成高级玩家的过程。这种过程需要玩家在游戏中不断挑战自己，提高自己的技能和道具的利用效率，从而获得更多的游戏奖励和成就。\n",
            "\n",
            "要实现阶级跳跃，需要玩家掌握以下技能和策略：\n",
            "\n",
            "1. 道具的使用：玩家需要学会如何有效地使用游戏中的各种道具，如技能、装备、金币等，以提高自己的战斗力和属性。\n",
            "\n",
            "2. 技能的学习：玩家需要学习游戏中的各种技能，如战斗技能、操作技能、策略技能等，以便在游戏中更好地发挥自己的能力。\n",
            "\n",
            "3. 挑战和升级：玩家需要不断挑战游戏中的各种难度模式，如普通、专家、大师等，以提高自己的技能和道具的等级。\n",
            "\n",
            "4. 金币的使用：玩家需要学会如何有效地使用游戏中的金币，以便购买更多的道具和技能，提高自己的战斗力和属性。\n",
            "\n",
            "以下是实现阶级跳跃的具体步骤：\n",
            "\n",
            "1. 学习技能：玩家需要学习游戏中的各种技能，并了解每个技能的作用和优点，以便更好地利用这些技能来提高自己的战斗力和属性。\n",
            "\n",
            "2. 挑战难度模式：玩家需要不断挑战游戏中的各种难度模式，如普通、专家、大师等，以提高自己的技能和道具的等级。\n",
            "\n",
            "3. 购买道具和技能：玩家需要购买游戏中的各种道具和技能，并了解它们的作用和优点，以便更好地利用这些道具和技能来提高自己的战斗力和属性。\n",
            "\n",
            "4. 使用金币：玩家需要学会如何有效地使用游戏中的金币，以便购买更多的道具和技能，提高自己的战斗力和属性。\n",
            "\n",
            "5. 持续学习和提高：玩家需要不断学习和提高自己的技能和道具的等级，以便在游戏中更好地发挥自己的能力，实现阶级跳跃。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"你是一个把{input_language}翻译成{output_language}的助手\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "messages = chat_prompt.format_prompt(input_language=\"英语\", output_language=\"汉语\", text=\"I love programming.\")\n",
        "\n",
        "messages.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_m3-jfGB7qs",
        "outputId": "d22752fb-fa61-415a-8f26-7eee0f7da52a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='你是一个把英语翻译成汉语的助手', additional_kwargs={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chains 链\n",
        "链接多个组件处理一个特定的下游任务"
      ],
      "metadata": {
        "id": "4_MqGD66Di6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chains import LLMChain\n",
        "# chain = LLMChain(llm=openAI(), prompt=promptTem)\n",
        "# print(chain.run(\"你好\"))\n",
        "\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "\n",
        "\n",
        "class DemoChain():\n",
        "    def __init__(self, llm, prompt, history) -> None:\n",
        "        self.llm = llm\n",
        "        self.prompt = prompt\n",
        "        self.history = history\n",
        "\n",
        "    def run(self, query, history, context=None) -> Any:\n",
        "        if context is not None:\n",
        "            prompt = self.prompt.format(query=query, context=context)\n",
        "        else:\n",
        "            prompt = self.prompt.format(query=query)\n",
        "\n",
        "        response, history = self.llm(prompt, history)\n",
        "        return response, history\n",
        "\n",
        "chain = DemoChain(llm=llm, prompt=prompt_tem, history=[])\n",
        "response, history = chain.run(query=\"阶级跳跃\", history=[])\n",
        "print(response, history)\n",
        "\n",
        "chain = DemoChain(llm=llm, prompt=prompt_tem, history=[])\n",
        "response, history = chain.run(query=\"阶级跳跃\", history=[])\n",
        "print(response, history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYEVjYq4DlrN",
        "outputId": "93831244-56e3-4c52-bb5d-d3b7f7bb4f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "阶级跳跃是指一个人或组织通过不断努力和学习，从一个阶级跨越到另一个阶级的过程。这个过程通常涉及到对社会结构和游戏规则的深刻理解，以及在适当的时间和场合下采取适当的行动。\n",
            "\n",
            "实现阶级跳跃的步骤如下：\n",
            "\n",
            "1. 学习游戏规则和历史经验：阶级跳跃需要对游戏规则和历史经验有深入的了解。玩家需要学习如何在游戏中获得优势，如何赢得比赛，以及如何与对手竞争。\n",
            "\n",
            "2. 建立强大的关系网络：关系网络可以帮助玩家获得游戏中的优势。玩家需要与其他人建立联系，以获得他们的信任和资源。\n",
            "\n",
            "3. 提高自己的技能：玩家需要不断提高自己的技能，以增加在游戏中的优势。这可能包括学习新技能、获得更好的工作经验或获得更高的学历。\n",
            "\n",
            "4. 选择适当的时间和场合：阶级跳跃需要在适当的时间和场合下进行。玩家需要选择适当的时间进行阶级跳跃，例如在赢得比赛之前进行阶级跳跃，或者在比赛中获得优势之后进行阶级跳跃。\n",
            "\n",
            "5. 实施阶级跳跃：玩家需要采取适当的行动来实施阶级跳跃。这可能包括建立强大的关系网络、提高自己的技能、选择适当的时间和场合，以及实施阶级跳跃。\n",
            "\n",
            "阶级跳跃是一个需要不断学习和改进的过程，也是一个需要耐心和毅力的过程。只有对于那些具有坚定的信念和毅力的人来说，阶级跳跃才能真正实现。 [('什么是阶级跳跃,还有如何真正做到并细说实现步骤', '阶级跳跃是指一个人或组织通过不断努力和学习，从一个阶级跨越到另一个阶级的过程。这个过程通常涉及到对社会结构和游戏规则的深刻理解，以及在适当的时间和场合下采取适当的行动。\\n\\n实现阶级跳跃的步骤如下：\\n\\n1. 学习游戏规则和历史经验：阶级跳跃需要对游戏规则和历史经验有深入的了解。玩家需要学习如何在游戏中获得优势，如何赢得比赛，以及如何与对手竞争。\\n\\n2. 建立强大的关系网络：关系网络可以帮助玩家获得游戏中的优势。玩家需要与其他人建立联系，以获得他们的信任和资源。\\n\\n3. 提高自己的技能：玩家需要不断提高自己的技能，以增加在游戏中的优势。这可能包括学习新技能、获得更好的工作经验或获得更高的学历。\\n\\n4. 选择适当的时间和场合：阶级跳跃需要在适当的时间和场合下进行。玩家需要选择适当的时间进行阶级跳跃，例如在赢得比赛之前进行阶级跳跃，或者在比赛中获得优势之后进行阶级跳跃。\\n\\n5. 实施阶级跳跃：玩家需要采取适当的行动来实施阶级跳跃。这可能包括建立强大的关系网络、提高自己的技能、选择适当的时间和场合，以及实施阶级跳跃。\\n\\n阶级跳跃是一个需要不断学习和改进的过程，也是一个需要耐心和毅力的过程。只有对于那些具有坚定的信念和毅力的人来说，阶级跳跃才能真正实现。')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding\n",
        "外部信息编码成一个高维向量"
      ],
      "metadata": {
        "id": "dKx53ahfSmKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #官方示例代码，用的OpenAI的ada的文本Embedding模型\n",
        "# #1） Embeding model\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings(model_name=\"ada\")\n",
        "# query_result = embeddings.embed_query(\"你好\")\n",
        "\n",
        "# #2) 文本切割\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=100, chunk_overlap=0\n",
        "# )\n",
        "# texts = \"\"\"阶级跳跃是指一个人或一个组织通过提高自己的技能、知识和领导能力，从一个阶级跨越到另一个阶级的过程。要实现阶级跳跃，以下是一些建议：\\n\\n1. 学习新技能：学习新技能可以让人具备新的知识和技能，从而增加自己的竞争力。可以选择学习与目前工作相关的新技能，或者学习与未来工作相关的新技能。\\n\\n2. 提高知识水平：不断学习新知识可以增加自己的知识储备，从而提高自己的竞争力。可以通过阅读书籍、参加培训、参与线上课程等方式来提高自己的知识水平。\\n\\n3. 建立良好的人际关系：建立良好的人际关系可以让人更容易得到新机会，同时也可以获得更多的支持和帮助。可以通过参加社交活动、建立人脉、参加社区组织等方式来建立良好的人际关系。\\n\\n4. 提高自己的领导能力：领导能力可以让人更好地管理自己的时间和资源，从而更好地完成工作。可以通过参加领导力课程、参加团队建设活动、自我反思等方式来提高自己的领导能力。\\n\\n5. 建立自己的品牌：建立自己的品牌可以让人更容易被人记住，从而更容易得到新机会。可以通过写博客、发布视频、制作网站等方式来建立自己的品牌。\\n\\n阶级跳跃需要长期的努力和不断学习，需要对自己的能力和目标有清晰的认识，并制定明确的计划和目标。\"\"\"\n",
        "# texts = text_splitter.create_documents([texts])\n",
        "# print(texts[0].page_content)\n",
        "\n",
        "# # 3)入库检索，官方使用的Pinecone,他提供一个后台管理界面 | 用户需求太大，不好用了已经，一直加载中....\n",
        "# import pinecone\n",
        "# from langchain.vectorstores import Pinecone\n",
        "# pinecone.init(api_key=os.getenv(\"\"), enviroment=os.getenv(\"\"))\n",
        "\n",
        "# index_name = \"demo\"\n",
        "# search = Pinecone.from_documents(texts=texts, embeddings, index_name=index_name)\n",
        "# query = \"What is magical about an autoencoder?\"\n",
        "# result = search.similarity_search(query)\n",
        "\n",
        "### 这里使用chatGLM\n",
        "# 1） Embedding model:  text2vec-large-chinese"
      ],
      "metadata": {
        "id": "odYd5qgFSnIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "class TextSpliter(CharacterTextSplitter):\n",
        "    def __init__(self, separator: str = \"\\n\\n\", **kwargs: Any):\n",
        "        super().__init__(separator, **kwargs)\n",
        "\n",
        "    def split_text(self, text: str) -> List[str]:\n",
        "        texts = text.split(\"\\n\")\n",
        "        texts = [Document(page_content=text, metadata={\"from\": \"filename or book.txt\"}) for text in texts]\n",
        "        return texts\n",
        "\n",
        "texts = response\n",
        "\n",
        "text_splitter = TextSpliter()\n",
        "texts = text_splitter.split_text(texts)\n",
        "texts1 = [text.page_content for text in texts]\n",
        "\n",
        "texts1"
      ],
      "metadata": {
        "id": "KD-lRdx-Ynxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"GanymedeNil/text2vec-large-chinese\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={'device': \"cuda\"})\n",
        "query_result = embeddings.embed_query(\"阶级跳跃\")\n",
        "\n",
        "np.array(query_result).shape"
      ],
      "metadata": {
        "id": "vLkGJuZcbPUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vs_path = \"text_to_emb\"\n",
        "\n",
        "docs = embeddings.embed_documents(texts1)\n",
        "\n",
        "vector_store = FAISS.from_documents(texts, embeddings)\n",
        "vector_store.save_local(vs_path)\n",
        "\n",
        "vector_store = FAISS.load_local(vs_path, embeddings)\n",
        "related_docs_with_score = vector_store.similarity_search_with_score(query=\"阶级跳跃\", k=2)\n",
        "\n",
        "related_docs_with_score"
      ],
      "metadata": {
        "id": "x7vw5wD6bOLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 基于查询到的知识做prompt\n",
        "context = \"\"\n",
        "for pack in related_docs_with_score:\n",
        "    doc, socre = pack\n",
        "    content = doc.page_content\n",
        "    print(\"检索到的知识=%s, from=%s, socre=%.3f\"%(content, doc.metadata.get(\"from\"), socre))\n",
        "    context += content"
      ],
      "metadata": {
        "id": "J8-n7QC2dS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 重新配置一个基于上下文的模板在来调下语言模型\n",
        "template = \"已知{context}, 请给我解释一下{query}的意思?\"\n",
        "promptTem = PromptTemplate(input_variables=[\"context\", \"query\"], template=template)\n",
        "chain = DemoChain(llm=llm, prompt=promptTem, history=[])\n",
        "print(\"-\"*80)\n",
        "print(chain.run(query=\"阶级跳跃\", context=context, history=[]))\n",
        "print(\"-\"*80)"
      ],
      "metadata": {
        "id": "I-wjt8rWf2tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings HuggingFace"
      ],
      "metadata": {
        "id": "5AZA3Af7U4CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "pcpxG5aA_mZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding 持久化"
      ],
      "metadata": {
        "id": "_gMz_XnFVBa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "HBg_cyR1_oC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 对超长文本进行总结"
      ],
      "metadata": {
        "id": "uQ_CFNbDyAZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import IPython\n",
        "import sentence_transformers\n",
        "\n",
        "\n",
        "embedding_model_dict = {\n",
        "    \"ernie-tiny\": \"nghuyong/ernie-3.0-nano-zh\",\n",
        "    \"ernie-base\": \"nghuyong/ernie-3.0-base-zh\",\n",
        "    \"text2vec\": \"GanymedeNil/text2vec-large-chinese\",\n",
        "    \"text2vec2\":\"uer/sbert-base-chinese-nli\",\n",
        "    \"text2vec3\":\"shibing624/text2vec-base-chinese\",\n",
        "}\n",
        "\n",
        "EMBEDDING_MODEL = \"text2vec3\"\n",
        "# 初始化 hugginFace 的 embeddings 对象\n",
        "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_dict[EMBEDDING_MODEL], )\n",
        "embeddings.client = sentence_transformers.SentenceTransformer(\n",
        "        embeddings.model_name, device='mps')\n"
      ],
      "metadata": {
        "id": "o_TcsCeAU6om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# 初始化加载器\n",
        "db = Chroma.from_documents(split_docs, embeddings,persist_directory=\"./news_test\")\n",
        "# 持久化\n",
        "db.persist()\n",
        "\n",
        "db = Chroma(persist_directory=\"./news_test\", embedding_function=embeddings)\n"
      ],
      "metadata": {
        "id": "FO6DX4PHVFGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(split_docs, embeddings)\n",
        "db.save_local(\"./news_test\")\n",
        "\n",
        "db = FAISS.load_local(\"./news_test\",embeddings=embeddings)"
      ],
      "metadata": {
        "id": "FL1nNyQOVQTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "import IPython\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "# 进行问答\n",
        "query = \"2022年腾讯营收多少\"\n",
        "print(qa.run(query))"
      ],
      "metadata": {
        "id": "pwjDxF44V64p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "import re\n",
        "\n",
        "class TfboyLLM(LLM):\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        print(\"问题:\",prompt)\n",
        "        pattern = re.compile(r'^.*(\\d+[*/+-]\\d+).*$')\n",
        "        match = pattern.search(prompt)\n",
        "        if match:\n",
        "            result = eval(match.group(1))\n",
        "        elif \"？\" in prompt:\n",
        "            rep_args = {\"我\":\"你\", \"你\":\"我\", \"吗\":\"\", \"？\":\"！\"}\n",
        "            result = [(rep_args[c] if c in rep_args else c) for c in list(prompt)]\n",
        "            result = ''.join(result)\n",
        "        else:\n",
        "            result = \"很抱歉，请换一种问法。比如：1+1等于几\"\n",
        "        return result\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {}\n",
        "\n"
      ],
      "metadata": {
        "id": "4GQgTpN_7xyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = TfboyLLM()\n",
        "print(\"答案:\",llm(\"我能问你问题吗？\"))"
      ],
      "metadata": {
        "id": "6-ra4sWw8AnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 实例"
      ],
      "metadata": {
        "id": "KfgmWXlx_Pws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 信息抽取"
      ],
      "metadata": {
        "id": "aUWP4xNR_Uuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    def __init__(self, model_name, quantization_bit=4):\n",
        "        print('----1', model_name)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        print('----2', model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().cuda().eval()\n",
        "        # self.model = self.model.quantize(quantization_bit)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        response, history = self.model.chat(self.tokenizer, prompt)\n",
        "        return response, history\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "lZxCyTLZF3ur"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = CustomLLM(model_name=\"THUDM/chatglm-6B-int4\")\n",
        "llm(\"This is a foobar thing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "mXAD14HCGJY9",
        "outputId": "4a15b5bc-98e9-4526-d0fe-4142f64c22b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----1 THUDM/chatglm-6B-int4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eb0caf3d6d81>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"THUDM/chatglm-6B-int4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is a foobar thing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-699754725a19>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, quantization_bit)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'----2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.cpython-310-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__setattr__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \"CustomLLM\" object has no field \"tokenizer\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "\n",
        "text = \"阿尔茨海默病(Alzheimer's disease, AD),俗称老年痴呆症,是一种全身性神经退行性疾病，它是由大脑神经退行性变性引起的，\\\n",
        "主要表现为记忆力减退、思维能力减退、行为变化等。阿尔茨海默病的原因尚不十分清楚，但是研究表明，阿尔茨海默病可能与遗传因素、环境因素、\\\n",
        "营养不良、压力过大、不良生活习惯等有关。根据世界卫生组织的统计数据，全球有超过4700万人患有阿尔茨海默病，其中美国有超过600万人患有阿尔茨海默病，\\\n",
        "欧洲有超过1000万人患有阿尔茨海默病，亚洲有超过2500万人患有阿尔茨海默病，其中中国有超过1000万人患有阿尔茨海默病。阿尔茨海默病的发病率与年龄有关，\\\n",
        "随着年龄的增长而增加，65岁以上的人群为主要受害群体，占比高达80%，其中45-65岁的人群占比为15%，20-45岁的人群占比为5%。65岁以上的人群发病率约为10%，\\\n",
        "75岁以上的人群发病率约为20%，85岁以上的人群发病率约为30%。根据统计，男性患病率高于女性，男性患病比例为1.4：1，即男性患病率比女性高出40%。\\\n",
        "根据统计，阿尔茨海默病在不同的人种中分布情况也有所不同。白人患病率最高，占总患病率的70%，黑人患病率次之，占总患病率的20%，\\\n",
        "其他少数民族患病率最低，占总患病率的10%。阿尔茨海默病在不同的饮食习惯中分布情况也有所不同。维生素B12缺乏的人群患病率更高，\\\n",
        "而均衡膳食的人群患病率较低。阿尔茨海默病不仅会给患者带来记忆力减退、思维能力减退、行为变化等症状，还会给患者的家庭带来巨大的心理负担。\\\n",
        "因此，患者应尽快就医，及时进行治疗。治疗阿尔茨海默病的方法有药物治疗、行为治疗、认知行为治疗等，具体治疗方案要根据患者的具体情况而定。\"\n",
        "\n",
        "#创建模板\n",
        "fact_extraction_prompt = PromptTemplate(\n",
        "    input_variables=[\"text_input\"],\n",
        "    template=\"从下面的本文中提取关键事实。尽量使用文本中的统计数据来说明事实:\\n\\n {text_input}\"\n",
        ")\n",
        "\n",
        "#定义chain\n",
        "fact_extraction_chain = LLMChain(llm=llm, prompt=fact_extraction_prompt)\n",
        "facts = fact_extraction_chain.run(text)\n",
        "print(facts)"
      ],
      "metadata": {
        "id": "Otck1zjw_WXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.chains.base import Chain\n",
        "\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        # Union of the input keys of the two chains.\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}\n",
        "\n",
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good slogan for a company that makes {product}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"colorful socks\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")\n",
        "\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"colorful socks\")\n"
      ],
      "metadata": {
        "id": "LwFPex-UEX31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"阿尔茨海默病(Alzheimer's disease, AD),俗称老年痴呆症,是一种全身性神经退行性疾病，它是由大脑神经退行性变性引起的，\\\n",
        "主要表现为记忆力减退、思维能力减退、行为变化等。阿尔茨海默病的原因尚不十分清楚，但是研究表明，阿尔茨海默病可能与遗传因素、环境因素、\\\n",
        "营养不良、压力过大、不良生活习惯等有关。根据世界卫生组织的统计数据，全球有超过4700万人患有阿尔茨海默病，其中美国有超过600万人患有阿尔茨海默病，\\\n",
        "欧洲有超过1000万人患有阿尔茨海默病，亚洲有超过2500万人患有阿尔茨海默病，其中中国有超过1000万人患有阿尔茨海默病。阿尔茨海默病的发病率与年龄有关，\\\n",
        "随着年龄的增长而增加，65岁以上的人群为主要受害群体，占比高达80%，其中45-65岁的人群占比为15%，20-45岁的人群占比为5%。65岁以上的人群发病率约为10%，\\\n",
        "75岁以上的人群发病率约为20%，85岁以上的人群发病率约为30%。根据统计，男性患病率高于女性，男性患病比例为1.4：1，即男性患病率比女性高出40%。\\\n",
        "根据统计，阿尔茨海默病在不同的人种中分布情况也有所不同。白人患病率最高，占总患病率的70%，黑人患病率次之，占总患病率的20%，\\\n",
        "其他少数民族患病率最低，占总患病率的10%。阿尔茨海默病在不同的饮食习惯中分布情况也有所不同。维生素B12缺乏的人群患病率更高，\\\n",
        "而均衡膳食的人群患病率较低。阿尔茨海默病不仅会给患者带来记忆力减退、思维能力减退、行为变化等症状，还会给患者的家庭带来巨大的心理负担。\\\n",
        "因此，患者应尽快就医，及时进行治疗。治疗阿尔茨海默病的方法有药物治疗、行为治疗、认知行为治疗等，具体治疗方案要根据患者的具体情况而定。\"\n",
        "\n",
        "\n",
        "#创建模板\n",
        "fact_extraction_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"从下面的本文中提取关键事实。尽量使用文本中的统计数据来说明事实:\\n\\n {query}\"\n",
        ")\n",
        "\n",
        "fact_extraction_chain = DemoChain(llm=llm, prompt=fact_extraction_prompt, history=[])\n",
        "response, history = fact_extraction_chain.run(query=text, history=[])\n",
        "print(response, history)"
      ],
      "metadata": {
        "id": "_6Ah9BnWAabw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doctor_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"你是神经内科医生。根据以下阿尔茨海默病的事实统计列表，为您的病人写一个简短的预防阿尔茨海默病的建议。 不要遗漏关键信息：\\n\\n {query}\"\n",
        ")\n",
        "\n",
        "doctor_chain = DemoChain(llm=llm, prompt=doctor_prompt, history=history)\n",
        "response, history = doctor_chain.run(query=response, history=history)\n",
        "print(response, history)"
      ],
      "metadata": {
        "id": "AG0-mJoOBbV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
        "\n",
        "\n",
        "#定义SimpleSequentialChain\n",
        "full_chain = SimpleSequentialChain(chains=[fact_extraction_chain, doctor_chain], verbose=True)\n",
        "response, history = full_chain.run(text)\n",
        "print(response, history)\n"
      ],
      "metadata": {
        "id": "em9SQwT2B19B"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5AZA3Af7U4CR",
        "_gMz_XnFVBa6",
        "uQ_CFNbDyAZM"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/hsJxZLuTFLQlV1RjRiBf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}