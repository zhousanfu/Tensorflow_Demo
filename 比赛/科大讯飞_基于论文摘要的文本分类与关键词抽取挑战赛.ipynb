{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 科大讯飞 基于论文摘要的文本分类与关键词抽取挑战赛[🔗](https://challenge.xfyun.cn/topic/info?type=abstract-of-the-paper&option=ssgy&ch=F5ZbQiB)\n",
    "\n",
    "一、赛事背景\n",
    "\n",
    "医学领域的文献库中蕴含了丰富的疾病诊断和治疗信息，如何高效地从海量文献中提取关键信息，进行疾病诊断和治疗推荐，对于临床医生和研究人员具有重要意义。\n",
    "\n",
    "二、赛事任务\n",
    "\n",
    "本任务分为两个子任务：\n",
    "机器通过对论文摘要等信息的理解，判断该论文是否属于医学领域的文献。\n",
    "提取出该论文关键词。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 传统NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# 加载预训练的BERT模型和分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 定义分类模型\n",
    "class PaperClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PaperClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 2)  # 768是BERT模型的隐藏层大小\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        _, pooled_output = self.bert(text)\n",
    "        output = self.fc(pooled_output)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "# 定义关键词提取模型\n",
    "class KeywordExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeywordExtractor, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.fc = nn.Linear(768, 1)  # 768是BERT模型的隐藏层大小\n",
    "        \n",
    "    def forward(self, text):\n",
    "        _, pooled_output = self.bert(text)\n",
    "        output = self.fc(pooled_output)\n",
    "        return output\n",
    "\n",
    "# 预处理数据\n",
    "def preprocess_data():\n",
    "    # 定义字段\n",
    "    TEXT = Field(sequential=True, tokenize=tokenizer.tokenize, lower=True, include_lengths=True)\n",
    "    LABEL = Field(sequential=False, use_vocab=False)\n",
    "    KEYWORDS = Field(sequential=True, tokenize='spacy', lower=True)\n",
    "    \n",
    "    # 加载数据集\n",
    "    train_data, test_data = TabularDataset.splits(\n",
    "        path='data',\n",
    "        train='train.csv',\n",
    "        test='test.csv',\n",
    "        format='csv',\n",
    "        fields=[('text', TEXT), ('label', LABEL), ('keywords', KEYWORDS)]\n",
    "    )\n",
    "    \n",
    "    # 构建词汇表\n",
    "    TEXT.build_vocab(train_data)\n",
    "    KEYWORDS.build_vocab(train_data)\n",
    "    \n",
    "    # 创建迭代器\n",
    "    train_iterator, test_iterator = BucketIterator.splits(\n",
    "        (train_data, test_data),\n",
    "        batch_size=32,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True\n",
    "    )\n",
    "    \n",
    "    return train_iterator, test_iterator\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_iterator):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    classifier_criterion = nn.CrossEntropyLoss()\n",
    "    extractor_criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        for batch in train_iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            labels = batch.label\n",
    "            keywords = torch.tensor([getattr(batch, 'keywords')]).squeeze().T.float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_output, extract_output = model(text)\n",
    "            classifier_loss = classifier_criterion(class_output, labels)\n",
    "            extractor_loss = extractor_criterion(extract_output, keywords)\n",
    "            loss = classifier_loss + extractor_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# 使用模型进行预测\n",
    "def predict(model, sentence):\n",
    "    model.eval()\n",
    "    \n",
    "    indexed_tokens = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        class_output, extract_output = model(tokens_tensor)\n",
    "        predicted_label = torch.argmax(class_output, dim=1)\n",
    "        predicted_keywords = [token for token, score in zip(tokenizer.tokenize(sentence), extract_output[0]) if score > 0]\n",
    "        \n",
    "    return predicted_label.item(), predicted_keywords\n",
    "\n",
    "# 主函数\n",
    "if __name__ == \"__main__\":\n",
    "    train_iterator, _ = preprocess_data()\n",
    "    \n",
    "    classifier_model = PaperClassifier()\n",
    "    keyword_model = KeywordExtractor()\n",
    "    model = nn.ModuleList([classifier_model, keyword_model])\n",
    "    \n",
    "    train_model(model, train_iterator)\n",
    "    \n",
    "    example_sentence = \"This is a medical paper about cancer detection.\"\n",
    "    predicted_label, predicted_keywords = predict(model, example_sentence)\n",
    "    \n",
    "    if predicted_label == 1:\n",
    "        print(\"1\")\n",
    "    else:\n",
    "        print(\"0\")\n",
    "    \n",
    "    print(\"提取的关键词：\", predicted_keywords)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
