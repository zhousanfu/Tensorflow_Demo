{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='course', help='which dataset to use')\n",
    "parser.add_argument('--aggregator', type=str, default='sum', help='which aggregator to use')\n",
    "parser.add_argument('--n_epochs', type=int, default=10, help='the number of epochs')\n",
    "parser.add_argument('--neighbor_sample_size', type=int, default=16, help='the number of neighbors to be sampled')\n",
    "parser.add_argument('--dim', type=int, default=16, help='dimension of user and entity embeddings')\n",
    "parser.add_argument('--n_iter', type=int, default=1, help='number of iterations when computing entity representation')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "parser.add_argument('--l2_weight', type=float, default=1e-4, help='weight of l2 regularization')\n",
    "parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')\n",
    "parser.add_argument('--ratio', type=float, default=0.9, help='size of training dataset')\n",
    "args = parser.parse_args(['--l2_weight', '1e-4'])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    '''\n",
    "    Data Loader class which makes dataset for training / knowledge graph dictionary\n",
    "    '''\n",
    "    def __init__(self, data):\n",
    "        self.cfg = {\n",
    "            'course': {\n",
    "                'item2id_path': 'data/course/item_index2entity_id.txt',\n",
    "                'kg_path': 'data/course/kg.txt',\n",
    "                'rating_path': 'data/course/user_artists.txt',\n",
    "                'rating_sep': '\\t',\n",
    "                'threshold': 0\n",
    "            },\n",
    "            'music': {\n",
    "                'item2id_path': 'data/music/item_index2entity_id.txt',\n",
    "                'kg_path': 'data/course/kg.txt',\n",
    "                'rating_path': 'data/music/user_artists.dat',\n",
    "                'rating_sep': '\\t',\n",
    "                'threshold': 0\n",
    "            }\n",
    "        }\n",
    "        self.data = data\n",
    "        \n",
    "        df_item2id = pd.read_csv(self.cfg[data]['item2id_path'], sep='\\t', header=None, names=['item','id'])\n",
    "        df_kg = pd.read_csv(self.cfg[data]['kg_path'], sep='\\t', header=None, names=['head','relation','tail'])\n",
    "        df_rating = pd.read_csv(self.cfg[data]['rating_path'], sep=self.cfg[data]['rating_sep'], names=['userID', 'itemID', 'rating'], skiprows=1)\n",
    "        \n",
    "        # df_rating['itemID'] and df_item2id['item'] both represents old entity ID\n",
    "        df_rating = df_rating[df_rating['itemID'].isin(df_item2id['item'])]\n",
    "        df_rating.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        self.df_item2id = df_item2id\n",
    "        self.df_kg = df_kg\n",
    "        self.df_rating = df_rating\n",
    "        \n",
    "        self.user_encoder = LabelEncoder()\n",
    "        self.entity_encoder = LabelEncoder()\n",
    "        self.relation_encoder = LabelEncoder()\n",
    "\n",
    "        self._encoding()\n",
    "    \n",
    "    def _encoding(self):\n",
    "        '''\n",
    "        Fit each label encoder and encode knowledge graph\n",
    "        '''\n",
    "        self.user_encoder.fit(self.df_rating['userID'])\n",
    "        # df_item2id['id'] and df_kg[['head', 'tail']] represents new entity ID\n",
    "        self.entity_encoder.fit(pd.concat([self.df_item2id['id'], self.df_kg['head'], self.df_kg['tail']]))\n",
    "        self.relation_encoder.fit(self.df_kg['relation'])\n",
    "        \n",
    "        # encode df_kg\n",
    "        self.df_kg['head'] = self.entity_encoder.transform(self.df_kg['head'])\n",
    "        self.df_kg['tail'] = self.entity_encoder.transform(self.df_kg['tail'])\n",
    "        self.df_kg['relation'] = self.relation_encoder.transform(self.df_kg['relation'])\n",
    "\n",
    "    def _build_dataset(self):\n",
    "        '''\n",
    "        Build dataset for training (rating data)\n",
    "        It contains negative sampling process\n",
    "        '''\n",
    "        print('Build dataset dataframe ...', end=' ')\n",
    "        # df_rating update\n",
    "        df_dataset = pd.DataFrame()\n",
    "        df_dataset['userID'] = self.user_encoder.transform(self.df_rating['userID'])\n",
    "        \n",
    "        # update to new id\n",
    "        item2id_dict = dict(zip(self.df_item2id['item'], self.df_item2id['id']))\n",
    "        self.df_rating['itemID'] = self.df_rating['itemID'].apply(lambda x: item2id_dict[x])\n",
    "        df_dataset['itemID'] = self.entity_encoder.transform(self.df_rating['itemID'])\n",
    "        if self.data == 'course':\n",
    "            df_dataset['label'] = self.df_rating['rating'].apply(lambda x: 1)\n",
    "        # else:\n",
    "        #     df_dataset['label'] = self.df_rating['rating'].apply(lambda x: 0 if x < self.cfg[self.data]['threshold'] else 1)\n",
    "        \n",
    "        # negative sampling\n",
    "        df_dataset = df_dataset[df_dataset['label']==1]\n",
    "        # df_dataset requires columns to have new entity ID\n",
    "        # full_item_set = set(range(len(self.entity_encoder.classes_)))\n",
    "        # user_list = []\n",
    "        # item_list = []\n",
    "        # label_list = []\n",
    "        # for user, group in df_dataset.groupby(['userID']):\n",
    "        #     item_set = set(group['itemID'])\n",
    "        #     negative_set = full_item_set - item_set\n",
    "        #     negative_sampled = random.sample(list(negative_set), len(item_set))\n",
    "        #     user_list.extend([user] * len(negative_sampled))\n",
    "        #     item_list.extend(negative_sampled)\n",
    "        #     label_list.extend([0] * len(negative_sampled))\n",
    "        # negative = pd.DataFrame({'userID': user_list, 'itemID': item_list, 'label': label_list})\n",
    "        # df_dataset = pd.concat([df_dataset, negative])\n",
    "        \n",
    "        df_dataset = df_dataset.sample(frac=1, replace=False, random_state=999)\n",
    "        df_dataset.reset_index(inplace=True, drop=True)\n",
    "        print('Done')\n",
    "        return df_dataset\n",
    "    \n",
    "    def _construct_kg(self):\n",
    "        '''\n",
    "        Construct knowledge graph\n",
    "        Knowledge graph is dictionary form\n",
    "        'head': [(relation, tail), ...]\n",
    "        '''\n",
    "        print('Construct knowledge graph ...', end=' ')\n",
    "        kg = dict()\n",
    "        for i in range(len(self.df_kg)):\n",
    "            head = self.df_kg.iloc[i]['head']\n",
    "            relation = self.df_kg.iloc[i]['relation']\n",
    "            tail = self.df_kg.iloc[i]['tail']\n",
    "            if head in kg:\n",
    "                kg[head].append((relation, tail))\n",
    "            else:\n",
    "                kg[head] = [(relation, tail)]\n",
    "            if tail in kg:\n",
    "                kg[tail].append((relation, head))\n",
    "            else:\n",
    "                kg[tail] = [(relation, head)]\n",
    "        print('Done')\n",
    "        return kg\n",
    "    \n",
    "    def load_dataset(self):\n",
    "        return self._build_dataset()\n",
    "\n",
    "    def load_kg(self):\n",
    "        return self._construct_kg()\n",
    "    \n",
    "    def get_encoders(self):\n",
    "        return (self.user_encoder, self.entity_encoder, self.relation_encoder)\n",
    "    \n",
    "    def get_num(self):\n",
    "        return (len(self.user_encoder.classes_), len(self.entity_encoder.classes_), len(self.relation_encoder.classes_))\n",
    "\n",
    "class KGCNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        user_id = np.array(self.df.iloc[idx]['userID'])\n",
    "        item_id = np.array(self.df.iloc[idx]['itemID'])\n",
    "        label = np.array(self.df.iloc[idx]['label'], dtype=np.float32)\n",
    "        return user_id, item_id, label\n",
    "\n",
    "class Aggregator(torch.nn.Module):\n",
    "    '''\n",
    "    Aggregator class\n",
    "    Mode in ['sum', 'concat', 'neighbor']\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, batch_size, dim, aggregator):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        if aggregator == 'concat':\n",
    "            self.weights = torch.nn.Linear(2 * dim, dim, bias=True)\n",
    "        else:\n",
    "            self.weights = torch.nn.Linear(dim, dim, bias=True)\n",
    "        self.aggregator = aggregator\n",
    "        \n",
    "    def forward(self, self_vectors, neighbor_vectors, neighbor_relations, user_embeddings, act):\n",
    "        batch_size = user_embeddings.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        neighbors_agg = self._mix_neighbor_vectors(neighbor_vectors, neighbor_relations, user_embeddings)\n",
    "        \n",
    "        if self.aggregator == 'sum':\n",
    "            output = (self_vectors + neighbors_agg).view((-1, self.dim))\n",
    "            \n",
    "        elif self.aggregator == 'concat':\n",
    "            output = torch.cat((self_vectors, neighbors_agg), dim=-1)\n",
    "            output = output.view((-1, 2 * self.dim))\n",
    "            \n",
    "        else:\n",
    "            output = neighbors_agg.view((-1, self.dim))\n",
    "            \n",
    "        output = self.weights(output)\n",
    "        return act(output.view((self.batch_size, -1, self.dim)))\n",
    "        \n",
    "    def _mix_neighbor_vectors(self, neighbor_vectors, neighbor_relations, user_embeddings):\n",
    "        '''\n",
    "        This aims to aggregate neighbor vectors\n",
    "        '''\n",
    "        # [batch_size, 1, dim] -> [batch_size, 1, 1, dim]\n",
    "        user_embeddings = user_embeddings.view((self.batch_size, 1, 1, self.dim))\n",
    "        \n",
    "        # [batch_size, -1, n_neighbor, dim] -> [batch_size, -1, n_neighbor]\n",
    "        user_relation_scores = (user_embeddings * neighbor_relations).sum(dim = -1)\n",
    "        user_relation_scores_normalized = F.softmax(user_relation_scores, dim = -1)\n",
    "        \n",
    "        # [batch_size, -1, n_neighbor] -> [batch_size, -1, n_neighbor, 1]\n",
    "        user_relation_scores_normalized = user_relation_scores_normalized.unsqueeze(dim = -1)\n",
    "        \n",
    "        # [batch_size, -1, n_neighbor, 1] * [batch_size, -1, n_neighbor, dim] -> [batch_size, -1, dim]\n",
    "        neighbors_aggregated = (user_relation_scores_normalized * neighbor_vectors).sum(dim = 2)\n",
    "        \n",
    "        return neighbors_aggregated\n",
    "\n",
    "class KGCN(torch.nn.Module):\n",
    "    def __init__(self, num_user, num_ent, num_rel, kg, args, device):\n",
    "        super(KGCN, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_ent = num_ent\n",
    "        self.num_rel = num_rel\n",
    "        self.n_iter = args.n_iter\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dim = args.dim\n",
    "        self.n_neighbor = args.neighbor_sample_size\n",
    "        self.kg = kg\n",
    "        self.device = device\n",
    "        self.aggregator = Aggregator(self.batch_size, self.dim, args.aggregator)\n",
    "        \n",
    "        self._gen_adj()\n",
    "            \n",
    "        self.usr = torch.nn.Embedding(num_user, args.dim)\n",
    "        self.ent = torch.nn.Embedding(num_ent, args.dim)\n",
    "        self.rel = torch.nn.Embedding(num_rel, args.dim)\n",
    "        \n",
    "    def _gen_adj(self):\n",
    "        '''\n",
    "        Generate adjacency matrix for entities and relations\n",
    "        Only cares about fixed number of samples\n",
    "        '''\n",
    "        self.adj_ent = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        self.adj_rel = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        \n",
    "        for e in self.kg:\n",
    "            if len(self.kg[e]) >= self.n_neighbor:\n",
    "                neighbors = random.sample(self.kg[e], self.n_neighbor)\n",
    "            else:\n",
    "                neighbors = random.choices(self.kg[e], k=self.n_neighbor)\n",
    "                \n",
    "            self.adj_ent[e] = torch.LongTensor([ent for _, ent in neighbors])\n",
    "            self.adj_rel[e] = torch.LongTensor([rel for rel, _ in neighbors])\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        '''\n",
    "        input: u, v are batch sized indices for users and items\n",
    "        u: [batch_size]\n",
    "        v: [batch_size]\n",
    "        '''\n",
    "        batch_size = u.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        # change to [batch_size, 1]\n",
    "        u = u.view((-1, 1))\n",
    "        v = v.view((-1, 1))\n",
    "        \n",
    "        # [batch_size, dim]\n",
    "        user_embeddings = self.usr(u).squeeze(dim = 1)\n",
    "        \n",
    "        entities, relations = self._get_neighbors(v)\n",
    "        \n",
    "        item_embeddings = self._aggregate(user_embeddings, entities, relations)\n",
    "        \n",
    "        scores = (user_embeddings * item_embeddings).sum(dim = 1)\n",
    "            \n",
    "        return torch.sigmoid(scores)\n",
    "    \n",
    "    def _get_neighbors(self, v):\n",
    "        '''\n",
    "        v is batch sized indices for items\n",
    "        v: [batch_size, 1]\n",
    "        '''\n",
    "        entities = [v]\n",
    "        relations = []\n",
    "        \n",
    "        for h in range(self.n_iter):\n",
    "            neighbor_entities = torch.LongTensor(self.adj_ent[entities[h]]).view((self.batch_size, -1)).to(self.device)\n",
    "            neighbor_relations = torch.LongTensor(self.adj_rel[entities[h]]).view((self.batch_size, -1)).to(self.device)\n",
    "            entities.append(neighbor_entities)\n",
    "            relations.append(neighbor_relations)\n",
    "            \n",
    "        return entities, relations\n",
    "    \n",
    "    def _aggregate(self, user_embeddings, entities, relations):\n",
    "        '''\n",
    "        Make item embeddings by aggregating neighbor vectors\n",
    "        '''\n",
    "        entity_vectors = [self.ent(entity) for entity in entities]\n",
    "        relation_vectors = [self.rel(relation) for relation in relations]\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            if i == self.n_iter - 1:\n",
    "                act = torch.tanh\n",
    "            else:\n",
    "                act = torch.sigmoid\n",
    "            \n",
    "            entity_vectors_next_iter = []\n",
    "            for hop in range(self.n_iter - i):\n",
    "                vector = self.aggregator(\n",
    "                    self_vectors=entity_vectors[hop],\n",
    "                    neighbor_vectors=entity_vectors[hop + 1].view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    neighbor_relations=relation_vectors[hop].view((self.batch_size, -1, self.n_neighbor, self.dim)),\n",
    "                    user_embeddings=user_embeddings,\n",
    "                    act=act)\n",
    "                entity_vectors_next_iter.append(vector)\n",
    "            entity_vectors = entity_vectors_next_iter\n",
    "        \n",
    "        return entity_vectors[0].view((self.batch_size, self.dim))\n",
    "\n",
    "class LightKGCN(torch.nn.Module):\n",
    "    def __init__(self, num_user, num_ent, num_rel, kg, args, device):\n",
    "        super(LightKGCN, self).__init__()\n",
    "        self.num_user = num_user\n",
    "        self.num_ent = num_ent\n",
    "        self.num_rel = num_rel\n",
    "        self.n_iter = args.n_iter\n",
    "        self.batch_size = args.batch_size\n",
    "        self.dim = args.dim\n",
    "        self.n_neighbor = args.neighbor_sample_size\n",
    "        self.kg = kg\n",
    "        self.device = device\n",
    "        \n",
    "        self._gen_adj()\n",
    "            \n",
    "        self.usr = torch.nn.Embedding(num_user, args.dim)\n",
    "        self.ent = torch.nn.Embedding(num_ent, args.dim)\n",
    "        self.rel = torch.nn.Embedding(num_rel, args.dim)\n",
    "        \n",
    "    def _gen_adj(self):\n",
    "        self.adj_ent = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        self.adj_rel = torch.empty(self.num_ent, self.n_neighbor, dtype=torch.long)\n",
    "        \n",
    "        for e in self.kg:\n",
    "            if len(self.kg[e]) >= self.n_neighbor:\n",
    "                neighbors = random.sample(self.kg[e], self.n_neighbor)\n",
    "            else:\n",
    "                neighbors = random.choices(self.kg[e], k=self.n_neighbor)\n",
    "                \n",
    "            self.adj_ent[e] = torch.LongTensor([ent for _, ent in neighbors])\n",
    "            self.adj_rel[e] = torch.LongTensor([rel for rel, _ in neighbors])\n",
    "        \n",
    "    def forward(self, u, v):\n",
    "        batch_size = u.size(0)\n",
    "        if batch_size != self.batch_size:\n",
    "            self.batch_size = batch_size\n",
    "        u = u.view((-1, 1))\n",
    "        v = v.view((-1, 1))\n",
    "        \n",
    "        user_embeddings = self.usr(u).squeeze(dim=1)\n",
    "        \n",
    "        entities, relations = self._get_neighbors(v)\n",
    "        \n",
    "        item_embeddings = self._lightgcn(user_embeddings, entities, relations)\n",
    "        \n",
    "        scores = (user_embeddings * item_embeddings).sum(dim=1)\n",
    "        \n",
    "        return torch.sigmoid(scores)\n",
    "    \n",
    "    def _get_neighbors(self, v):\n",
    "        entities = [v]\n",
    "        relations = []\n",
    "        \n",
    "        for h in range(self.n_iter):\n",
    "            neighbor_entities = torch.LongTensor(self.adj_ent[entities[h]]).view((self.batch_size, -1)).to(self.device)\n",
    "            neighbor_relations = torch.LongTensor(self.adj_rel[entities[h]]).view((self.batch_size, -1)).to(self.device)\n",
    "            entities.append(neighbor_entities)\n",
    "            relations.append(neighbor_relations)\n",
    "        \n",
    "        return entities, relations\n",
    "    \n",
    "    def _lightgcn(self, user_embeddings, entities, relations):\n",
    "        entity_vectors = [self.ent(entity) for entity in entities]\n",
    "        relation_vectors = [self.rel(relation) for relation in relations]\n",
    "        \n",
    "        print('----->', entity_vectors, relation_vectors)\n",
    "        for i in range(self.n_iter):\n",
    "            entity_vectors = [entity_vectors[i] + relation_vectors[i] for i in range(self.n_iter)]\n",
    "\n",
    "        print(entity_vectors)\n",
    "        \n",
    "        return entity_vectors[0].view((self.batch_size, self.dim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(args.dataset)\n",
    "kg = data_loader.load_kg()\n",
    "df_dataset = data_loader.load_dataset()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_dataset, df_dataset['label'], test_size=1 - args.ratio, shuffle=False, random_state=999)\n",
    "train_dataset = KGCNDataset(x_train)\n",
    "test_dataset = KGCNDataset(x_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user, num_entity, num_relation = data_loader.get_num()\n",
    "user_encoder, entity_encoder, relation_encoder = data_loader.get_encoders()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = KGCN(num_user, num_entity, num_relation, kg, args, device).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2_weight)\n",
    "print(num_user, num_entity, num_relation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []\n",
    "test_loss_list = []\n",
    "auc_score_list = []\n",
    "for epoch in range(args.n_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (user_ids, item_ids, labels) in enumerate(train_loader):\n",
    "        user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(user_ids, item_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('[Epoch {}]train_loss: '.format(epoch+1), running_loss / len(train_loader))\n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "\n",
    "    torch.save(model, 'course.bin')\n",
    "    # model = torch.load('course.bin')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss = 0\n",
    "        total_roc = 0\n",
    "        for user_ids, item_ids, labels in test_loader:\n",
    "            user_ids, item_ids, labels = user_ids.to(device), item_ids.to(device), labels.to(device)\n",
    "            outputs = model(user_ids, item_ids)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            # total_roc += roc_auc_score(labels.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "        print('[Epoch {}]test_loss: '.format(epoch+1), test_loss / len(test_loader))\n",
    "        test_loss_list.append(test_loss / len(test_loader))\n",
    "        # auc_score_list.append(total_roc / len(test_loader))\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))  # 1 row, 2 columns\n",
    "ax1.plot(loss_list)\n",
    "ax1.plot(test_loss_list)\n",
    "# ax2.plot(auc_score_list)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/course/course_id.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "course_id = [int(k) for k, v in data.items()]\n",
    "\n",
    "model = torch.load('course.bin')\n",
    "v = torch.tensor([2], dtype=torch.long)\n",
    "\n",
    "for i in course_id:\n",
    "    course_id = torch.tensor(i, dtype=torch.long)\n",
    "    output = model(v, course_id)\n",
    "    print(output[0], course_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_df = pd.read_csv('data/course/train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('data/course/test.csv', encoding='utf-8')\n",
    "kg_df = pd.read_csv('data/course/rel_user_course.csv', encoding='utf-8')\n",
    "concat_df = pd.concat([train_df, test_df, kg_df])\n",
    "print(train_df.shape, test_df.shape, kg_df.shape, concat_df.shape)\n",
    "\n",
    "le = LabelEncoder()\n",
    "concat_df['user_id'] = le.fit_transform(concat_df['user'])\n",
    "user_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "concat_df['course_id'] = le.fit_transform(concat_df['course'])\n",
    "course_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "train_df['user'] = train_df['user'].map(user_dict)\n",
    "train_df['course'] = train_df['course'].map(course_dict)\n",
    "test_df['user'] = test_df['user'].map(user_dict)\n",
    "test_df['course'] = test_df['course'].map(course_dict)\n",
    "kg_df['user'] = kg_df['user'].map(user_dict)\n",
    "kg_df['course'] = kg_df['course'].map(course_dict)\n",
    "\n",
    "fw = open('data/course/raw/user_list.txt', 'w', encoding='utf-8')\n",
    "fw.write('org_id remap_id'+'\\n')\n",
    "for k,v in user_dict.items():\n",
    "    fw.write(k + ' ' + str(v) + '\\n')\n",
    "fw.close()\n",
    "\n",
    "fw = open('data/course/raw/item_list.txt', 'w', encoding='utf-8')\n",
    "fw.write('org_id remap_id'+'\\n')\n",
    "for k,v in course_dict.items():\n",
    "    fw.write(k + ' ' + str(v) + '\\n')\n",
    "fw.close()\n",
    "\n",
    "grouped_df = train_df.groupby('user')['course'].apply(list)\n",
    "with open('data/course/raw/train.txt', 'w') as f:\n",
    "    for index, row in grouped_df.iteritems():\n",
    "        print(index, type(index), row, type(row))\n",
    "        row_str = \" \".join(map(str, row))\n",
    "        f.write(f\"{index} {row_str}\\n\")\n",
    "\n",
    "grouped_df = test_df.groupby('user')['course'].apply(list)\n",
    "with open('data/course/raw/test.txt', 'w') as f:\n",
    "    for index, row in grouped_df.iteritems():\n",
    "        print(index, type(index), row, type(row))\n",
    "        row_str = \" \".join(map(str, row))\n",
    "        f.write(f\"{index} {row_str}\\n\")\n",
    "\n",
    "\n",
    "# fw_kg = open('data/course/kg.txt', 'w', encoding='utf-8')\n",
    "# for index, row in kg_df.iterrows():\n",
    "#     fw_kg.write(str(row['user']) + '\\t' + 'rel' + '\\t' + str(row['course']) + '\\n')\n",
    "# fw_kg.close()\n",
    "\n",
    "# df = pd.DataFrame()\n",
    "# df['entity_id'] = pd.Series([int(v) for k, v in user_dict.items()]).sort_values()\n",
    "# df['item_index'] = pd.Series([int(v) for k, v in course_dict.items()]).sort_values()\n",
    "# df.to_csv('data/course/item_index2entity_id.txt', index=False, sep='\\t')\n",
    "\n",
    "# fw_uc = open('data/course/user_artists.txt', 'w', encoding='utf-8')\n",
    "# fw_uc.write('userID\tartistID\tweight\\n')\n",
    "# for index, row in train_df.iterrows():\n",
    "#     fw_uc.write(str(row['user']) + '\\t' + str(row['course']) + '\\t' + str(4) + '\\n')\n",
    "# fw_uc.close()\n",
    "\n",
    "# user_id_dict = {}\n",
    "# course_id_dict = {}\n",
    "# for k,v in user_dict.items():\n",
    "#     user_id_dict[int(user_dict[k])] = k\n",
    "# for k,v in course_dict.items():\n",
    "#     course_id_dict[int(course_dict[k])] = k\n",
    "# with open('data/course/user_id.json', \"w\") as json_file:\n",
    "#     json.dump(user_id_dict, json_file)\n",
    "# with open('data/course/course_id.json', \"w\") as json_file:\n",
    "#     json.dump(course_id_dict, json_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
