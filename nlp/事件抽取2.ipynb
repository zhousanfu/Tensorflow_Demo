{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQIWgEItkZO3"
   },
   "source": [
    "DuEE v1.0数据集：https://aistudio.baidu.com/aistudio/competition/detail/46/0/datasets\n",
    "文章介绍：https://kexue.fm/archives/8926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGn3x9-Hgpt8"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!mkdir roberta\n",
    "!gdown --id '1W3WgPJWGVKlU9wpUYsdZuurAIFKvrl_Y' --output roberta/chinese_roberta_wwm_ext_L-12_H-768_A-12.zip\n",
    "# !cp /content/drive/MyDrive/Data/NLP/预训练模型/chinese_roberta_wwm_ext_L-12_H-768_A-12.zip /content/roberta/chinese_roberta_wwm_ext_L-12_H-768_A-12.zip\n",
    "# !unzip -d /content/roberta /content/roberta/chinese_roberta_wwm_ext_L-12_H-768_A-12.zip\n",
    "# !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=13oki4nUEp3z_6Sl4NovhldB90HZ1w1c-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=13oki4nUEp3z_6Sl4NovhldB90HZ1w1c-\" -O Linemod_and_Occlusion.zip && rm -rf /tmp/cookies.txt\n",
    "#!pip install transformers\n",
    "#from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "#model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrJAiF06a3jV"
   },
   "outputs": [],
   "source": [
    "# https://aistudio.baidu.com/aistudio/competition/detail/46/0/datasets\n",
    "# 关系抽取\n",
    "!rm -rf data\n",
    "!mkdir data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duie_sample.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duie_schema.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duie_train.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duie_dev.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duie_test2.json.zip -P /content/data\n",
    "\n",
    "# 句子级事件抽取\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_sample.json.zip -P data\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_schema.zip -P data\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_train.json.zip -P data\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_dev.json.zip -P data\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_test2.json.zip -P data\n",
    "\n",
    "# 篇章级事件抽取\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_fin_sample.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_fin_schema.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_fin_train.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_fin_dev.json.zip -P /content/data\n",
    "# !wget https://dataset-bj.cdn.bcebos.com/qianyan/duee_fin_test2.json.zip -P /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgsZPeJBFFml"
   },
   "outputs": [],
   "source": [
    "# !unzip -d /content/data /content/data/duee_fin_sample.json.zip\n",
    "# !unzip -d /content/data /content/data/duee_fin_schema.zip\n",
    "# !unzip -d /content/data /content/data/duee_fin_train.json.zip\n",
    "# !unzip -d /content/data /content/data/duee_fin_dev.json.zip\n",
    "# !unzip -d /content/data /content/data/duee_fin_test2.json.zip\n",
    "!unzip -d data data/duee_sample.json.zip\n",
    "!unzip -d data data/duee_schema.zip\n",
    "!unzip -d data data/duee_train.json.zip\n",
    "!unzip -d data data/duee_dev.json.zip\n",
    "!unzip -d data data/duee_test2.json.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2VZ9oDLkffU"
   },
   "outputs": [],
   "source": [
    "# python3.7\n",
    "!pip install git+https://www.github.com/bojone/bert4keras.git\n",
    "!pip install keras-bert==0.81.1\n",
    "!pip install keras-transformer==0.33.0\n",
    "!pip install tensorboard==1.13.1\n",
    "!pip install tensorflow==1.13.1\n",
    "!pip install tensorflow-gpu==1.13.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "#os.chidr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UaKf188ihjE_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from bert4keras.backend import keras, K\n",
    "from bert4keras.backend import sparse_multilabel_categorical_crossentropy\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.layers import EfficientGlobalPointer as GlobalPointer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, to_array\n",
    "from tqdm import tqdm\n",
    "\n",
    "maxlen = 128\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "config_path = 'roberta/bert_config.json'\n",
    "checkpoint_path = 'roberta/bert_model.ckpt'\n",
    "dict_path = 'roberta/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "IiVaEEKInCaQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('财经/交易-出售/收购', '触发词'),\n",
       " ('财经/交易-出售/收购', '时间'),\n",
       " ('财经/交易-出售/收购', '出售方'),\n",
       " ('财经/交易-出售/收购', '交易物'),\n",
       " ('财经/交易-出售/收购', '出售价格'),\n",
       " ('财经/交易-出售/收购', '收购方'),\n",
       " ('财经/交易-跌停', '触发词'),\n",
       " ('财经/交易-跌停', '时间'),\n",
       " ('财经/交易-跌停', '跌停股票'),\n",
       " ('财经/交易-加息', '触发词'),\n",
       " ('财经/交易-加息', '时间'),\n",
       " ('财经/交易-加息', '加息幅度'),\n",
       " ('财经/交易-加息', '加息机构'),\n",
       " ('财经/交易-降价', '触发词'),\n",
       " ('财经/交易-降价', '时间'),\n",
       " ('财经/交易-降价', '降价方'),\n",
       " ('财经/交易-降价', '降价物'),\n",
       " ('财经/交易-降价', '降价幅度'),\n",
       " ('财经/交易-降息', '触发词'),\n",
       " ('财经/交易-降息', '时间'),\n",
       " ('财经/交易-降息', '降息幅度'),\n",
       " ('财经/交易-降息', '降息机构'),\n",
       " ('财经/交易-融资', '触发词'),\n",
       " ('财经/交易-融资', '时间'),\n",
       " ('财经/交易-融资', '跟投方'),\n",
       " ('财经/交易-融资', '领投方'),\n",
       " ('财经/交易-融资', '融资轮次'),\n",
       " ('财经/交易-融资', '融资金额'),\n",
       " ('财经/交易-融资', '融资方'),\n",
       " ('财经/交易-上市', '触发词'),\n",
       " ('财经/交易-上市', '时间'),\n",
       " ('财经/交易-上市', '地点'),\n",
       " ('财经/交易-上市', '上市企业'),\n",
       " ('财经/交易-上市', '融资金额'),\n",
       " ('财经/交易-涨价', '触发词'),\n",
       " ('财经/交易-涨价', '时间'),\n",
       " ('财经/交易-涨价', '涨价幅度'),\n",
       " ('财经/交易-涨价', '涨价物'),\n",
       " ('财经/交易-涨价', '涨价方'),\n",
       " ('财经/交易-涨停', '触发词'),\n",
       " ('财经/交易-涨停', '时间'),\n",
       " ('财经/交易-涨停', '涨停股票'),\n",
       " ('产品行为-发布', '触发词'),\n",
       " ('产品行为-发布', '时间'),\n",
       " ('产品行为-发布', '发布产品'),\n",
       " ('产品行为-发布', '发布方'),\n",
       " ('产品行为-获奖', '触发词'),\n",
       " ('产品行为-获奖', '时间'),\n",
       " ('产品行为-获奖', '获奖人'),\n",
       " ('产品行为-获奖', '奖项'),\n",
       " ('产品行为-获奖', '颁奖机构'),\n",
       " ('产品行为-上映', '触发词'),\n",
       " ('产品行为-上映', '时间'),\n",
       " ('产品行为-上映', '上映方'),\n",
       " ('产品行为-上映', '上映影视'),\n",
       " ('产品行为-下架', '触发词'),\n",
       " ('产品行为-下架', '时间'),\n",
       " ('产品行为-下架', '下架产品'),\n",
       " ('产品行为-下架', '被下架方'),\n",
       " ('产品行为-下架', '下架方'),\n",
       " ('产品行为-召回', '触发词'),\n",
       " ('产品行为-召回', '时间'),\n",
       " ('产品行为-召回', '召回内容'),\n",
       " ('产品行为-召回', '召回方'),\n",
       " ('交往-道歉', '触发词'),\n",
       " ('交往-道歉', '时间'),\n",
       " ('交往-道歉', '道歉对象'),\n",
       " ('交往-道歉', '道歉者'),\n",
       " ('交往-点赞', '触发词'),\n",
       " ('交往-点赞', '时间'),\n",
       " ('交往-点赞', '点赞方'),\n",
       " ('交往-点赞', '点赞对象'),\n",
       " ('交往-感谢', '触发词'),\n",
       " ('交往-感谢', '时间'),\n",
       " ('交往-感谢', '致谢人'),\n",
       " ('交往-感谢', '被感谢人'),\n",
       " ('交往-会见', '触发词'),\n",
       " ('交往-会见', '时间'),\n",
       " ('交往-会见', '地点'),\n",
       " ('交往-会见', '会见主体'),\n",
       " ('交往-会见', '会见对象'),\n",
       " ('交往-探班', '触发词'),\n",
       " ('交往-探班', '时间'),\n",
       " ('交往-探班', '探班主体'),\n",
       " ('交往-探班', '探班对象'),\n",
       " ('竞赛行为-夺冠', '触发词'),\n",
       " ('竞赛行为-夺冠', '时间'),\n",
       " ('竞赛行为-夺冠', '冠军'),\n",
       " ('竞赛行为-夺冠', '夺冠赛事'),\n",
       " ('竞赛行为-晋级', '触发词'),\n",
       " ('竞赛行为-晋级', '时间'),\n",
       " ('竞赛行为-晋级', '晋级方'),\n",
       " ('竞赛行为-晋级', '晋级赛事'),\n",
       " ('竞赛行为-禁赛', '触发词'),\n",
       " ('竞赛行为-禁赛', '时间'),\n",
       " ('竞赛行为-禁赛', '禁赛时长'),\n",
       " ('竞赛行为-禁赛', '被禁赛人员'),\n",
       " ('竞赛行为-禁赛', '禁赛机构'),\n",
       " ('竞赛行为-胜负', '触发词'),\n",
       " ('竞赛行为-胜负', '时间'),\n",
       " ('竞赛行为-胜负', '败者'),\n",
       " ('竞赛行为-胜负', '胜者'),\n",
       " ('竞赛行为-胜负', '赛事名称'),\n",
       " ('竞赛行为-退赛', '触发词'),\n",
       " ('竞赛行为-退赛', '时间'),\n",
       " ('竞赛行为-退赛', '退赛赛事'),\n",
       " ('竞赛行为-退赛', '退赛方'),\n",
       " ('竞赛行为-退役', '触发词'),\n",
       " ('竞赛行为-退役', '时间'),\n",
       " ('竞赛行为-退役', '退役者'),\n",
       " ('人生-产子/女', '触发词'),\n",
       " ('人生-产子/女', '时间'),\n",
       " ('人生-产子/女', '产子者'),\n",
       " ('人生-产子/女', '出生者'),\n",
       " ('人生-出轨', '触发词'),\n",
       " ('人生-出轨', '时间'),\n",
       " ('人生-出轨', '出轨方'),\n",
       " ('人生-出轨', '出轨对象'),\n",
       " ('人生-订婚', '触发词'),\n",
       " ('人生-订婚', '时间'),\n",
       " ('人生-订婚', '订婚主体'),\n",
       " ('人生-分手', '触发词'),\n",
       " ('人生-分手', '时间'),\n",
       " ('人生-分手', '分手双方'),\n",
       " ('人生-怀孕', '触发词'),\n",
       " ('人生-怀孕', '时间'),\n",
       " ('人生-怀孕', '怀孕者'),\n",
       " ('人生-婚礼', '触发词'),\n",
       " ('人生-婚礼', '时间'),\n",
       " ('人生-婚礼', '地点'),\n",
       " ('人生-婚礼', '参礼人员'),\n",
       " ('人生-婚礼', '结婚双方'),\n",
       " ('人生-结婚', '触发词'),\n",
       " ('人生-结婚', '时间'),\n",
       " ('人生-结婚', '结婚双方'),\n",
       " ('人生-离婚', '触发词'),\n",
       " ('人生-离婚', '时间'),\n",
       " ('人生-离婚', '离婚双方'),\n",
       " ('人生-庆生', '触发词'),\n",
       " ('人生-庆生', '时间'),\n",
       " ('人生-庆生', '生日方'),\n",
       " ('人生-庆生', '生日方年龄'),\n",
       " ('人生-庆生', '庆祝方'),\n",
       " ('人生-求婚', '触发词'),\n",
       " ('人生-求婚', '时间'),\n",
       " ('人生-求婚', '求婚者'),\n",
       " ('人生-求婚', '求婚对象'),\n",
       " ('人生-失联', '触发词'),\n",
       " ('人生-失联', '时间'),\n",
       " ('人生-失联', '地点'),\n",
       " ('人生-失联', '失联者'),\n",
       " ('人生-死亡', '触发词'),\n",
       " ('人生-死亡', '时间'),\n",
       " ('人生-死亡', '地点'),\n",
       " ('人生-死亡', '死者年龄'),\n",
       " ('人生-死亡', '死者'),\n",
       " ('司法行为-罚款', '触发词'),\n",
       " ('司法行为-罚款', '时间'),\n",
       " ('司法行为-罚款', '罚款对象'),\n",
       " ('司法行为-罚款', '执法机构'),\n",
       " ('司法行为-罚款', '罚款金额'),\n",
       " ('司法行为-拘捕', '触发词'),\n",
       " ('司法行为-拘捕', '时间'),\n",
       " ('司法行为-拘捕', '拘捕者'),\n",
       " ('司法行为-拘捕', '被拘捕者'),\n",
       " ('司法行为-举报', '触发词'),\n",
       " ('司法行为-举报', '时间'),\n",
       " ('司法行为-举报', '举报发起方'),\n",
       " ('司法行为-举报', '举报对象'),\n",
       " ('司法行为-开庭', '触发词'),\n",
       " ('司法行为-开庭', '时间'),\n",
       " ('司法行为-开庭', '开庭法院'),\n",
       " ('司法行为-开庭', '开庭案件'),\n",
       " ('司法行为-立案', '触发词'),\n",
       " ('司法行为-立案', '时间'),\n",
       " ('司法行为-立案', '立案机构'),\n",
       " ('司法行为-立案', '立案对象'),\n",
       " ('司法行为-起诉', '触发词'),\n",
       " ('司法行为-起诉', '时间'),\n",
       " ('司法行为-起诉', '被告'),\n",
       " ('司法行为-起诉', '原告'),\n",
       " ('司法行为-入狱', '触发词'),\n",
       " ('司法行为-入狱', '时间'),\n",
       " ('司法行为-入狱', '入狱者'),\n",
       " ('司法行为-入狱', '刑期'),\n",
       " ('司法行为-约谈', '触发词'),\n",
       " ('司法行为-约谈', '时间'),\n",
       " ('司法行为-约谈', '约谈对象'),\n",
       " ('司法行为-约谈', '约谈发起方'),\n",
       " ('灾害/意外-爆炸', '触发词'),\n",
       " ('灾害/意外-爆炸', '时间'),\n",
       " ('灾害/意外-爆炸', '地点'),\n",
       " ('灾害/意外-爆炸', '死亡人数'),\n",
       " ('灾害/意外-爆炸', '受伤人数'),\n",
       " ('灾害/意外-车祸', '触发词'),\n",
       " ('灾害/意外-车祸', '时间'),\n",
       " ('灾害/意外-车祸', '地点'),\n",
       " ('灾害/意外-车祸', '死亡人数'),\n",
       " ('灾害/意外-车祸', '受伤人数'),\n",
       " ('灾害/意外-地震', '触发词'),\n",
       " ('灾害/意外-地震', '时间'),\n",
       " ('灾害/意外-地震', '死亡人数'),\n",
       " ('灾害/意外-地震', '震级'),\n",
       " ('灾害/意外-地震', '震源深度'),\n",
       " ('灾害/意外-地震', '震中'),\n",
       " ('灾害/意外-地震', '受伤人数'),\n",
       " ('灾害/意外-洪灾', '触发词'),\n",
       " ('灾害/意外-洪灾', '时间'),\n",
       " ('灾害/意外-洪灾', '地点'),\n",
       " ('灾害/意外-洪灾', '死亡人数'),\n",
       " ('灾害/意外-洪灾', '受伤人数'),\n",
       " ('灾害/意外-起火', '触发词'),\n",
       " ('灾害/意外-起火', '时间'),\n",
       " ('灾害/意外-起火', '地点'),\n",
       " ('灾害/意外-起火', '死亡人数'),\n",
       " ('灾害/意外-起火', '受伤人数'),\n",
       " ('灾害/意外-坍/垮塌', '触发词'),\n",
       " ('灾害/意外-坍/垮塌', '时间'),\n",
       " ('灾害/意外-坍/垮塌', '坍塌主体'),\n",
       " ('灾害/意外-坍/垮塌', '死亡人数'),\n",
       " ('灾害/意外-坍/垮塌', '受伤人数'),\n",
       " ('灾害/意外-袭击', '触发词'),\n",
       " ('灾害/意外-袭击', '时间'),\n",
       " ('灾害/意外-袭击', '地点'),\n",
       " ('灾害/意外-袭击', '袭击对象'),\n",
       " ('灾害/意外-袭击', '死亡人数'),\n",
       " ('灾害/意外-袭击', '袭击者'),\n",
       " ('灾害/意外-袭击', '受伤人数'),\n",
       " ('灾害/意外-坠机', '触发词'),\n",
       " ('灾害/意外-坠机', '时间'),\n",
       " ('灾害/意外-坠机', '地点'),\n",
       " ('灾害/意外-坠机', '死亡人数'),\n",
       " ('灾害/意外-坠机', '受伤人数'),\n",
       " ('组织关系-裁员', '触发词'),\n",
       " ('组织关系-裁员', '时间'),\n",
       " ('组织关系-裁员', '裁员方'),\n",
       " ('组织关系-裁员', '裁员人数'),\n",
       " ('组织关系-辞/离职', '触发词'),\n",
       " ('组织关系-辞/离职', '时间'),\n",
       " ('组织关系-辞/离职', '离职者'),\n",
       " ('组织关系-辞/离职', '原所属组织'),\n",
       " ('组织关系-加盟', '触发词'),\n",
       " ('组织关系-加盟', '时间'),\n",
       " ('组织关系-加盟', '加盟者'),\n",
       " ('组织关系-加盟', '所加盟组织'),\n",
       " ('组织关系-解雇', '触发词'),\n",
       " ('组织关系-解雇', '时间'),\n",
       " ('组织关系-解雇', '解雇方'),\n",
       " ('组织关系-解雇', '被解雇人员'),\n",
       " ('组织关系-解散', '触发词'),\n",
       " ('组织关系-解散', '时间'),\n",
       " ('组织关系-解散', '解散方'),\n",
       " ('组织关系-解约', '触发词'),\n",
       " ('组织关系-解约', '时间'),\n",
       " ('组织关系-解约', '被解约方'),\n",
       " ('组织关系-解约', '解约方'),\n",
       " ('组织关系-停职', '触发词'),\n",
       " ('组织关系-停职', '时间'),\n",
       " ('组织关系-停职', '所属组织'),\n",
       " ('组织关系-停职', '停职人员'),\n",
       " ('组织关系-退出', '触发词'),\n",
       " ('组织关系-退出', '时间'),\n",
       " ('组织关系-退出', '退出方'),\n",
       " ('组织关系-退出', '原所属组织'),\n",
       " ('组织行为-罢工', '触发词'),\n",
       " ('组织行为-罢工', '时间'),\n",
       " ('组织行为-罢工', '所属组织'),\n",
       " ('组织行为-罢工', '罢工人数'),\n",
       " ('组织行为-罢工', '罢工人员'),\n",
       " ('组织行为-闭幕', '触发词'),\n",
       " ('组织行为-闭幕', '时间'),\n",
       " ('组织行为-闭幕', '地点'),\n",
       " ('组织行为-闭幕', '活动名称'),\n",
       " ('组织行为-开幕', '触发词'),\n",
       " ('组织行为-开幕', '时间'),\n",
       " ('组织行为-开幕', '地点'),\n",
       " ('组织行为-开幕', '活动名称'),\n",
       " ('组织行为-游行', '触发词'),\n",
       " ('组织行为-游行', '时间'),\n",
       " ('组织行为-游行', '地点'),\n",
       " ('组织行为-游行', '游行组织'),\n",
       " ('组织行为-游行', '游行人数')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取schema\n",
    "labels = []\n",
    "with open('data/duee_event_schema.json') as f:\n",
    "    for l in f:\n",
    "        l = json.loads(l)\n",
    "        t = l['event_type']\n",
    "        for r in [u'触发词'] + [s['role'] for s in l['role_list']]:\n",
    "            labels.append((t, r))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text ---> 雀巢裁员4000人：时代抛弃你时，连招呼都不会打！\n",
      "id ---> 409389c96efe78d6af1c86e0450fd2d7\n",
      "{'event_type': '组织关系-裁员', 'trigger': '裁员', 'trigger_start_index': 2, 'arguments': [{'argument_start_index': 0, 'role': '裁员方', 'argument': '雀巢', 'alias': []}, {'argument_start_index': 4, 'role': '裁员人数', 'argument': '4000人', 'alias': []}], 'class': '组织关系'}\n"
     ]
    }
   ],
   "source": [
    "with open('data/duee_train.json/duee_train.json', encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            l = json.loads(l)\n",
    "            break\n",
    "for k,v in l.items():\n",
    "    if k == 'event_list':\n",
    "        print(v[0])\n",
    "    else:\n",
    "        print(k,'--->', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Github/kg/data/天池/CMeEE/CMeEE_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "f1X34AcwnIiL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '8月20日消息，据腾讯新闻《一线》报道，知情人士表示，为了控制成本支出，蔚来计划将美国分公司的人员规模除自动驾驶业务相关人员外，减少至200人左右。截至美国时间8月16日，蔚来位于美国硅谷的分公司已裁减100名员工。',\n",
       " 'events': [[('组织关系-裁员', '触发词', '裁减', 99),\n",
       "   ('组织关系-裁员', '时间', '8月16日', 80),\n",
       "   ('组织关系-裁员', '裁员方', '蔚来', 86),\n",
       "   ('组织关系-裁员', '裁员人数', '100名员工', 101)]]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：{'text': text, 'events': [[(type, role, argument, start_index)]]}\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            l = json.loads(l)\n",
    "            d = {'text': l['text'], 'events': []}\n",
    "            for e in l['event_list']:\n",
    "                d['events'].append([(\n",
    "                    e['event_type'], u'触发词', e['trigger'],\n",
    "                    e['trigger_start_index']\n",
    "                )])\n",
    "                for a in e['arguments']:\n",
    "                    d['events'][-1].append((\n",
    "                        e['event_type'], a['role'], a['argument'],\n",
    "                        a['argument_start_index']\n",
    "                    ))\n",
    "            D.append(d)\n",
    "    return D\n",
    "\n",
    "# 加载数据集\n",
    "train_data = load_data('data/duee_train.json/duee_train.json')\n",
    "valid_data = load_data('data/duee_dev.json/duee_dev.json')\n",
    "train_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cy9i_OnCxgaQ"
   },
   "outputs": [],
   "source": [
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N3Hx4DJkgcG"
   },
   "outputs": [],
   "source": [
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        batch_argu_labels, batch_head_labels, batch_tail_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            tokens = tokenizer.tokenize(d['text'], maxlen=maxlen)\n",
    "            mapping = tokenizer.rematch(d['text'], tokens)\n",
    "            start_mapping = {j[0]: i for i, j in enumerate(mapping) if j}\n",
    "            end_mapping = {j[-1]: i for i, j in enumerate(mapping) if j}\n",
    "            token_ids = tokenizer.tokens_to_ids(tokens)\n",
    "            segment_ids = [0] * len(token_ids)\n",
    "            # 整理事件\n",
    "            events = []\n",
    "            for e in d['events']:\n",
    "                events.append([])\n",
    "                for t, r, a, i in e:\n",
    "                    label = labels.index((t, r))\n",
    "                    start, end = i, i + len(a) - 1\n",
    "                    if start in start_mapping and end in end_mapping:\n",
    "                        start, end = start_mapping[start], end_mapping[end]\n",
    "                        events[-1].append((label, start, end))\n",
    "            # 构建标签\n",
    "            argu_labels = [set() for _ in range(len(labels))]\n",
    "            head_labels, tail_labels = set(), set()\n",
    "            for e in events:\n",
    "                for l, h, t in e:\n",
    "                    argu_labels[l].add((h, t))\n",
    "                for i1, (_, h1, t1) in enumerate(e):\n",
    "                    for i2, (_, h2, t2) in enumerate(e):\n",
    "                        if i2 > i1:\n",
    "                            head_labels.add((min(h1, h2), max(h1, h2)))\n",
    "                            tail_labels.add((min(t1, t2), max(t1, t2)))\n",
    "            for label in argu_labels + [head_labels, tail_labels]:\n",
    "                if not label:  # 至少要有一个标签\n",
    "                    label.add((0, 0))  # 如果没有则用0填充\n",
    "            argu_labels = sequence_padding([list(l) for l in argu_labels])\n",
    "            head_labels = sequence_padding([list(head_labels)])\n",
    "            tail_labels = sequence_padding([list(tail_labels)])\n",
    "            # 构建batch\n",
    "            batch_token_ids.append(token_ids)\n",
    "            batch_segment_ids.append(segment_ids)\n",
    "            batch_argu_labels.append(argu_labels)\n",
    "            batch_head_labels.append(head_labels)\n",
    "            batch_tail_labels.append(tail_labels)\n",
    "            if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                batch_argu_labels = sequence_padding(\n",
    "                    batch_argu_labels, seq_dims=2\n",
    "                )\n",
    "                batch_head_labels = sequence_padding(\n",
    "                    batch_head_labels, seq_dims=2\n",
    "                )\n",
    "                batch_tail_labels = sequence_padding(\n",
    "                    batch_tail_labels, seq_dims=2\n",
    "                )\n",
    "                yield [batch_token_ids, batch_segment_ids], [\n",
    "                    batch_argu_labels, batch_head_labels, batch_tail_labels\n",
    "                ]\n",
    "                batch_token_ids, batch_segment_ids = [], []\n",
    "                batch_argu_labels, batch_head_labels, batch_tail_labels = [], [], []\n",
    "\n",
    "\n",
    "def globalpointer_crossentropy(y_true, y_pred):\n",
    "    \"\"\"给GlobalPointer设计的交叉熵\n",
    "    \"\"\"\n",
    "    shape = K.shape(y_pred)\n",
    "    y_true = y_true[..., 0] * K.cast(shape[2], K.floatx()) + y_true[..., 1]\n",
    "    y_pred = K.reshape(y_pred, (shape[0], -1, K.prod(shape[2:])))\n",
    "    loss = sparse_multilabel_categorical_crossentropy(y_true, y_pred, True)\n",
    "    return K.mean(K.sum(loss, axis=1))\n",
    "\n",
    "\n",
    "class DedupList(list):\n",
    "    \"\"\"定义去重的list\n",
    "    \"\"\"\n",
    "    def append(self, x):\n",
    "        if x not in self:\n",
    "            super(DedupList, self).append(x)\n",
    "\n",
    "\n",
    "def neighbors(host, argus, links):\n",
    "    \"\"\"构建邻集（host节点与其所有邻居的集合）\n",
    "    \"\"\"\n",
    "    results = [host]\n",
    "    for argu in argus:\n",
    "        if host[2:] + argu[2:] in links:\n",
    "            results.append(argu)\n",
    "    return list(sorted(results))\n",
    "\n",
    "\n",
    "def clique_search(argus, links):\n",
    "    \"\"\"搜索每个节点所属的完全子图作为独立事件\n",
    "    搜索思路：找出不相邻的节点，然后分别构建它们的邻集，递归处理。\n",
    "    \"\"\"\n",
    "    Argus = DedupList()\n",
    "    for i1, (_, _, h1, t1) in enumerate(argus):\n",
    "        for i2, (_, _, h2, t2) in enumerate(argus):\n",
    "            if i2 > i1:\n",
    "                if (h1, t1, h2, t2) not in links:\n",
    "                    Argus.append(neighbors(argus[i1], argus, links))\n",
    "                    Argus.append(neighbors(argus[i2], argus, links))\n",
    "    if Argus:\n",
    "        results = DedupList()\n",
    "        for A in Argus:\n",
    "            for a in clique_search(A, links):\n",
    "                results.append(a)\n",
    "        return results\n",
    "    else:\n",
    "        return [list(sorted(argus))]\n",
    "\n",
    "\n",
    "def extract_events(text, threshold=0, trigger=True):\n",
    "    \"\"\"抽取输入text所包含的所有事件\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "    outputs = model.predict([token_ids, segment_ids])\n",
    "    outputs = [o[0] for o in outputs]\n",
    "    # 抽取论元\n",
    "    argus = set()\n",
    "    outputs[0][:, [0, -1]] -= np.inf\n",
    "    outputs[0][:, :, [0, -1]] -= np.inf\n",
    "    for l, h, t in zip(*np.where(outputs[0] > threshold)):\n",
    "        argus.add(labels[l] + (h, t))\n",
    "    # 构建链接\n",
    "    links = set()\n",
    "    for i1, (_, _, h1, t1) in enumerate(argus):\n",
    "        for i2, (_, _, h2, t2) in enumerate(argus):\n",
    "            if i2 > i1:\n",
    "                if outputs[1][0, min(h1, h2), max(h1, h2)] > threshold:\n",
    "                    if outputs[2][0, min(t1, t2), max(t1, t2)] > threshold:\n",
    "                        links.add((h1, t1, h2, t2))\n",
    "                        links.add((h2, t2, h1, t1))\n",
    "    # 析出事件\n",
    "    events = []\n",
    "    for _, sub_argus in groupby(sorted(argus), key=lambda s: s[0]):\n",
    "        for event in clique_search(list(sub_argus), links):\n",
    "            events.append([])\n",
    "            for argu in event:\n",
    "                start, end = mapping[argu[2]][0], mapping[argu[3]][-1] + 1\n",
    "                events[-1].append(argu[:2] + (text[start:end], start))\n",
    "            if trigger and all([argu[1] != u'触发词' for argu in event]):\n",
    "                events.pop()\n",
    "    return events\n",
    "\n",
    "\n",
    "def evaluate(data, threshold=0):\n",
    "    \"\"\"评估函数，计算f1、precision、recall\n",
    "    \"\"\"\n",
    "    ex, ey, ez = 1e-10, 1e-10, 1e-10  # 事件级别\n",
    "    ax, ay, az = 1e-10, 1e-10, 1e-10  # 论元级别\n",
    "    for d in tqdm(data, ncols=0):\n",
    "        pred_events = extract_events(d['text'], threshold, False)\n",
    "        # 事件级别\n",
    "        R, T = DedupList(), DedupList()\n",
    "        for event in pred_events:\n",
    "            if any([argu[1] == u'触发词' for argu in event]):\n",
    "                R.append(list(sorted(event)))\n",
    "        for event in d['events']:\n",
    "            T.append(list(sorted(event)))\n",
    "        for event in R:\n",
    "            if event in T:\n",
    "                ex += 1\n",
    "        ey += len(R)\n",
    "        ez += len(T)\n",
    "        # 论元级别\n",
    "        R, T = DedupList(), DedupList()\n",
    "        for event in pred_events:\n",
    "            for argu in event:\n",
    "                if argu[1] != u'触发词':\n",
    "                    R.append(argu)\n",
    "        for event in d['events']:\n",
    "            for argu in event:\n",
    "                if argu[1] != u'触发词':\n",
    "                    T.append(argu)\n",
    "        for argu in R:\n",
    "            if argu in T:\n",
    "                ax += 1\n",
    "        ay += len(R)\n",
    "        az += len(T)\n",
    "    e_f1, e_pr, e_rc = 2 * ex / (ey + ez), ex / ey, ex / ez\n",
    "    a_f1, a_pr, a_rc = 2 * ax / (ay + az), ax / ay, ax / az\n",
    "    return e_f1, e_pr, e_rc, a_f1, a_pr, a_rc\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_e_f1 = 0.\n",
    "        self.best_val_a_f1 = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        e_f1, e_pr, e_rc, a_f1, a_pr, a_rc = evaluate(valid_data)\n",
    "        if e_f1 >= self.best_val_e_f1:\n",
    "            self.best_val_e_f1 = e_f1\n",
    "            model.save_weights('best_model.e.weights')\n",
    "        if a_f1 >= self.best_val_a_f1:\n",
    "            self.best_val_a_f1 = a_f1\n",
    "            model.save_weights('best_model.a.weights')\n",
    "        print(\n",
    "            '[event level] f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f'\n",
    "            % (e_f1, e_pr, e_rc, self.best_val_e_f1)\n",
    "        )\n",
    "        print(\n",
    "            '[argument level] f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n'\n",
    "            % (a_f1, a_pr, a_rc, self.best_val_a_f1)\n",
    "        )\n",
    "\n",
    "\n",
    "def isin(event_a, event_b):\n",
    "    \"\"\"判断event_a是否event_b的一个子集\n",
    "    \"\"\"\n",
    "    if event_a['event_type'] != event_b['event_type']:\n",
    "        return False\n",
    "    for argu in event_a['arguments']:\n",
    "        if argu not in event_b['arguments']:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def predict_to_file(in_file, out_file):\n",
    "    \"\"\"预测结果到文件，方便提交\n",
    "    \"\"\"\n",
    "    fw = open(out_file, 'w', encoding='utf-8')\n",
    "    with open(in_file) as fr:\n",
    "        for l in tqdm(fr):\n",
    "            l = json.loads(l)\n",
    "            event_list = DedupList()\n",
    "            for event in extract_events(l['text']):\n",
    "                final_event = {\n",
    "                    'event_type': event[0][0],\n",
    "                    'arguments': DedupList()\n",
    "                }\n",
    "                for argu in event:\n",
    "                    if argu[1] != u'触发词':\n",
    "                        final_event['arguments'].append({\n",
    "                            'role': argu[1],\n",
    "                            'argument': argu[2]\n",
    "                        })\n",
    "                event_list = [\n",
    "                    event for event in event_list\n",
    "                    if not isin(event, final_event)\n",
    "                ]\n",
    "                if not any([isin(final_event, event) for event in event_list]):\n",
    "                    event_list.append(final_event)\n",
    "            l['event_list'] = event_list\n",
    "            l = json.dumps(l, ensure_ascii=False)\n",
    "            fw.write(l + '\\n')\n",
    "    fw.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6-fuqdtpZSM"
   },
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "base = build_transformer_model(\n",
    "    config_path = config_path,\n",
    "    checkpoint_path = checkpoint_path,\n",
    "    return_keras_model = False\n",
    ")\n",
    "output = base.model.output\n",
    "\n",
    "# 预测结果\n",
    "argu_output = GlobalPointer(heads=len(labels), head_size=64)(output)\n",
    "head_output = GlobalPointer(heads=1, head_size=64, RoPE=False)(output)\n",
    "tail_output = GlobalPointer(heads=1, head_size=64, RoPE=False)(output)\n",
    "outputs = [argu_output, head_output, tail_output]\n",
    "\n",
    "# 构建模型\n",
    "model = keras.models.Model(base.model.inputs, outputs)\n",
    "model.compile(loss=globalpointer_crossentropy, optimizer=Adam(2e-5))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49VJx2Mlp05x"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch = len(train_generator),\n",
    "        epochs = epochs,\n",
    "        callbacks = [evaluator]\n",
    "    )\n",
    "else:\n",
    "    model.load_weights('best_model.e.weights')\n",
    "    # predict_to_file('../datasets/duee_test2.json', 'duee.json')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "事件抽取2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
