{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b78418-a951-461d-8ab4-142bebb17ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "python 3.8.1\n",
    "pytorch==1.8.1\n",
    "transformer==4.9.2\n",
    "configparser\n",
    "https://drive.google.com/file/d/1yK_P8VhWZtdgzaG0gJ3zUGOKWODitKXZ/view\n",
    "https://github.com/xhw205/GPLinker_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2085a8f-3a50-4336-9899-c0c514582fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.optimizer import required\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import logging\n",
    "import abc\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e796b-e416-467e-b245-1fba3d63253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_multilabel_categorical_crossentropy(y_true=None, y_pred=None, mask_zero=False):\n",
    "    '''\n",
    "    稀疏多标签交叉熵损失的torch实现\n",
    "    '''\n",
    "    shape = y_pred.shape\n",
    "    y_true = y_true[..., 0] * shape[2] + y_true[..., 1]\n",
    "    y_pred = y_pred.reshape(shape[0], -1, np.prod(shape[2:]))\n",
    "    zeros = torch.zeros_like(y_pred[...,:1])\n",
    "    y_pred = torch.cat([y_pred, zeros], dim=-1)\n",
    "    if mask_zero:\n",
    "        infs = zeros + 1e12\n",
    "        y_pred = torch.cat([infs, y_pred[..., 1:]], dim=-1)\n",
    "    y_pos_2 = torch.gather(y_pred, index=y_true, dim=-1)\n",
    "    y_pos_1 = torch.cat([y_pos_2, zeros], dim=-1)\n",
    "    if mask_zero:\n",
    "        y_pred = torch.cat([-infs, y_pred[..., 1:]], dim=-1)\n",
    "        y_pos_2 = torch.gather(y_pred, index=y_true, dim=-1)\n",
    "    pos_loss = torch.logsumexp(-y_pos_1, dim=-1)\n",
    "    all_loss = torch.logsumexp(y_pred, dim=-1)\n",
    "    aux_loss = torch.logsumexp(y_pos_2, dim=-1) - all_loss\n",
    "    aux_loss = torch.clip(1 - torch.exp(aux_loss), 1e-10, 1)\n",
    "    neg_loss = all_loss + torch.log(aux_loss)\n",
    "    loss = torch.mean(torch.sum(pos_loss + neg_loss))\n",
    "    return loss\n",
    "\n",
    "class RawGlobalPointer(nn.Module):\n",
    "    def __init__(self, hiddensize, ent_type_size, inner_dim, RoPE=True, tril_mask=True):\n",
    "        '''\n",
    "        :param encoder: BERT\n",
    "        :param ent_type_size: 实体数目\n",
    "        :param inner_dim: 64\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.ent_type_size = ent_type_size\n",
    "        self.inner_dim = inner_dim\n",
    "        self.hidden_size = hiddensize\n",
    "        self.dense = nn.Linear(self.hidden_size, self.ent_type_size * self.inner_dim * 2)\n",
    "\n",
    "        self.RoPE = RoPE\n",
    "        self.trail_mask = tril_mask\n",
    "\n",
    "    def sinusoidal_position_embedding(self, batch_size, seq_len, output_dim):\n",
    "        position_ids = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(-1)\n",
    "\n",
    "        indices = torch.arange(0, output_dim // 2, dtype=torch.float)\n",
    "        indices = torch.pow(10000, -2 * indices / output_dim)\n",
    "        embeddings = position_ids * indices\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.repeat((batch_size, *([1] * len(embeddings.shape))))\n",
    "        embeddings = torch.reshape(embeddings, (batch_size, seq_len, output_dim))\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, context_outputs,  attention_mask):\n",
    "        self.device = attention_mask.device\n",
    "        last_hidden_state = context_outputs[0]\n",
    "        batch_size = last_hidden_state.size()[0]\n",
    "        seq_len = last_hidden_state.size()[1]\n",
    "        outputs = self.dense(last_hidden_state)\n",
    "        outputs = torch.split(outputs, self.inner_dim * 2, dim=-1)\n",
    "        outputs = torch.stack(outputs, dim=-2)\n",
    "        qw, kw = outputs[..., :self.inner_dim], outputs[..., self.inner_dim:]\n",
    "        if self.RoPE:\n",
    "            # pos_emb:(batch_size, seq_len, inner_dim)\n",
    "            pos_emb = self.sinusoidal_position_embedding(batch_size, seq_len, self.inner_dim)\n",
    "            cos_pos = pos_emb[..., None, 1::2].repeat_interleave(2, dim=-1)\n",
    "            sin_pos = pos_emb[..., None, ::2].repeat_interleave(2, dim=-1)\n",
    "            qw2 = torch.stack([-qw[..., 1::2], qw[..., ::2]], -1)\n",
    "            qw2 = qw2.reshape(qw.shape)\n",
    "            qw = qw * cos_pos + qw2 * sin_pos\n",
    "            kw2 = torch.stack([-kw[..., 1::2], kw[..., ::2]], -1)\n",
    "            kw2 = kw2.reshape(kw.shape)\n",
    "            kw = kw * cos_pos + kw2 * sin_pos\n",
    "        # logits:(batch_size, ent_type_size, seq_len, seq_len)\n",
    "        logits = torch.einsum('bmhd,bnhd->bhmn', qw, kw)\n",
    "        # padding mask\n",
    "        pad_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, self.ent_type_size, seq_len, seq_len)\n",
    "        logits = logits * pad_mask - (1 - pad_mask) * 1e12\n",
    "        # 排除下三角\n",
    "        if self.trail_mask:\n",
    "            mask = torch.tril(torch.ones_like(logits), -1)\n",
    "            logits = logits - mask * 1e12\n",
    "\n",
    "        return logits / self.inner_dim ** 0.5\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "if sys.version_info >= (3, 4):\n",
    "    ABC = abc.ABC\n",
    "else:\n",
    "    ABC = abc.ABCMeta('ABC', (), {})\n",
    "\n",
    "\n",
    "class _LRSchedule(ABC):\n",
    "    \"\"\" Parent of all LRSchedules here. \"\"\"\n",
    "    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n",
    "    def __init__(self, warmup=0.002, t_total=-1, **kw):\n",
    "        \"\"\"\n",
    "        :param warmup:  what fraction of t_total steps will be used for linear warmup\n",
    "        :param t_total: how many training steps (updates) are planned\n",
    "        :param kw:\n",
    "        \"\"\"\n",
    "        super(_LRSchedule, self).__init__(**kw)\n",
    "        if t_total < 0:\n",
    "            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n",
    "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
    "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
    "        warmup = max(warmup, 0.)\n",
    "        self.warmup, self.t_total = float(warmup), float(t_total)\n",
    "        self.warned_for_t_total_at_progress = -1\n",
    "\n",
    "    def get_lr(self, step, nowarn=False):\n",
    "        \"\"\"\n",
    "        :param step:    which of t_total steps we're on\n",
    "        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n",
    "        :return:        learning rate multiplier for current update\n",
    "        \"\"\"\n",
    "        if self.t_total < 0:\n",
    "            return 1.\n",
    "        progress = float(step) / self.t_total\n",
    "        ret = self.get_lr_(progress)\n",
    "        # warning for exceeding t_total (only active with warmup_linear\n",
    "        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n",
    "            logger.warning(\n",
    "                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n",
    "                    .format(ret, self.__class__.__name__))\n",
    "            self.warned_for_t_total_at_progress = progress\n",
    "        # end warning\n",
    "        return ret\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_lr_(self, progress):\n",
    "        \"\"\"\n",
    "        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n",
    "        :return:            learning rate multiplier for current update\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class ConstantLR(_LRSchedule):\n",
    "    def get_lr_(self, progress):\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class WarmupCosineSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n",
    "    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    warn_t_total = True\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n",
    "        \"\"\"\n",
    "        :param warmup:      see LRSchedule\n",
    "        :param t_total:     see LRSchedule\n",
    "        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n",
    "        :param kw:\n",
    "        \"\"\"\n",
    "        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n",
    "        self.cycles = cycles\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n",
    "            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n",
    "\n",
    "\n",
    "class WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
    "    learning rate (with hard restarts).\n",
    "    \"\"\"\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
    "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
    "        assert(cycles >= 1.)\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
    "            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n",
    "            return ret\n",
    "\n",
    "\n",
    "class WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n",
    "    \"\"\"\n",
    "    All training progress is divided in `cycles` (default=1.) parts of equal length.\n",
    "    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n",
    "    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n",
    "    \"\"\"\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
    "        assert(warmup * cycles < 1.)\n",
    "        warmup = warmup * cycles if warmup >= 0 else warmup\n",
    "        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        progress = progress * self.cycles % 1.\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
    "            ret = 0.5 * (1. + math.cos(math.pi * progress))\n",
    "            return ret\n",
    "\n",
    "\n",
    "class WarmupConstantSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Keeps learning rate equal to 1. after warmup.\n",
    "    \"\"\"\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class WarmupLinearSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n",
    "    \"\"\"\n",
    "    warn_t_total = True\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        return max((progress - 1.) / (self.warmup - 1.), 0.)\n",
    "\n",
    "\n",
    "SCHEDULES = {\n",
    "    None:       ConstantLR,\n",
    "    \"none\":     ConstantLR,\n",
    "    \"warmup_cosine\": WarmupCosineSchedule,\n",
    "    \"warmup_constant\": WarmupConstantSchedule,\n",
    "    \"warmup_linear\": WarmupLinearSchedule\n",
    "}\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, model, decay):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "class BertAdam(Optimizer):\n",
    "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n",
    "    Params:\n",
    "        lr: learning rate\n",
    "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
    "        t_total: total number of training steps for the learning\n",
    "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
    "        schedule: schedule to use for the warmup (see above).\n",
    "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
    "            If `None` or `'none'`, learning rate is always kept constant.\n",
    "            Default : `'warmup_linear'`\n",
    "        b1: Adams b1. Default: 0.9\n",
    "        b2: Adams b2. Default: 0.999\n",
    "        e: Adams epsilon. Default: 1e-6\n",
    "        weight_decay: Weight decay. Default: 0.01\n",
    "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
    "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
    "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
    "        if not e >= 0.0:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
    "        # initialize schedule object\n",
    "        if not isinstance(schedule, _LRSchedule):\n",
    "            schedule_type = SCHEDULES[schedule]\n",
    "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
    "        else:\n",
    "            if warmup != -1 or t_total != -1:\n",
    "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
    "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
    "        defaults = dict(lr=lr, schedule=schedule,\n",
    "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm)\n",
    "        super(BertAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    return [0]\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "                lr.append(lr_scheduled)\n",
    "        return lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['next_m'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['next_v'] = torch.zeros_like(p.data)\n",
    "\n",
    "                next_m, next_v = state['next_m'], state['next_v']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
    "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                update = next_m / (next_v.sqrt() + group['e'])\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    update += group['weight_decay'] * p.data\n",
    "\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "\n",
    "                update_with_lr = lr_scheduled * update\n",
    "                p.data.add_(-update_with_lr)\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "                # No bias correction\n",
    "                # bias_correction1 = 1 - beta1 ** state['step']\n",
    "                # bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "        return loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
