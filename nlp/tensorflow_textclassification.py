# -*- coding: utf-8 -*-
"""tensorflow_textclassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aaxo4bI-lzdJsBspgS95HbO53tHvin_M

## 需要模块
"""

# !pip install -q tensorflow-datasets
# !pip install tensorflow
# import tensorflow_datasets as tfds
import numpy as np
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import pandas as pd
import io, time
# from sklearn.datasets.samples_generator import make_blobs
# from sklearn.preprocessing import MinMaxScaler

"""## 1.1 数据预处理"""

# pd_all = pd.read_csv('tmp/imo_all.csv', sep='\t', encoding='utf-8')
pd_all = pd.read_csv('文本审核_20210416sub360.csv', sep='\t', encoding='utf-8')
pd_all = shuffle(pd_all)
print(len(pd_all))
review_data, label_data = pd_all.content.replace(r'\n\t\r', '', regex=True), pd_all.label
training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(review_data, label_data, test_size=0.1, shuffle=shuffle)
print(training_sentences.shape, training_labels.shape, testing_sentences.shape, testing_labels.shape)

training_sentences = [' '.join(str(x).replace('\n', '').replace('\r', '').replace(';', ' ').split()) for x in training_sentences.tolist()]
testing_sentences = [' '.join(str(x).replace('\n', '').replace('\r', '').replace(';', ' ').split()) for x in testing_sentences.tolist()]
training_labels = training_labels.tolist()
testing_labels = testing_labels.tolist()
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)


# train_and_label = tf.data.Dataset.from_tensor_slices((np.array(training_sentences), training_labels_final))
# test_and_label = tf.data.Dataset.from_tensor_slices((np.array(testing_sentences), testing_labels_final))
# # <PrefetchDataset shapes: ((None,), (None,)), types: (tf.string, tf.int64)>
# print(train_and_label.shard)
# print(test_and_label.shard)

"""## 2.1 超参数设置"""

MAX_NB_WORDS = 500000      # 设置最频繁使用的XXX个词
MAX_SEQUENCE_LENGTH = 528 # 256   # 文本最大的长度
EMBEDDING_DIM = 16       # 设置Embeddingceng层的维度
TRUNC_TYPE = 'post'
OOV_TOK = '<oov>'

"""### 2.1.1 文字向量"""

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, oov_token=OOV_TOK)
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index

training_sequences = tokenizer.texts_to_sequences(training_sentences)
training_padded = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH, truncating=TRUNC_TYPE)
testing_sentences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sentences, maxlen=MAX_SEQUENCE_LENGTH)

"""## 2.2 定义模型
网络-损失函数和优化器

### 2.2.1 基本
"""

model = tf.keras.Sequential([
  tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM),
  tf.keras.layers.GlobalAveragePooling1D(),
  tf.keras.layers.Dense(16, activation='relu'),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

num_epochs = 1

history = model.fit(
  training_padded,
  training_labels_final,
  epochs=num_epochs,
  batch_size=MAX_SEQUENCE_LENGTH,
  validation_data=(testing_padded, testing_labels_final),
  verbose=1
  )




"""## 4.1 模型预测

### 4.1.1 predict
"""

# df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/imo_discover_comment_data_20210101-20210330.csv', sep='\t')
df = pd.read_csv('资料.csv')
df_data = df.values.tolist()
data = []
data_line = []
my_test_sentences = df['a.contents'].values.tolist()
text_sentences = tokenizer.texts_to_sequences([str(s) for s in my_test_sentences])
my_text_padded = pad_sequences(text_sentences, maxlen=MAX_SEQUENCE_LENGTH)

predictions = model.predict(my_text_padded)
print('my_test_sentences len=', len(my_test_sentences), my_test_sentences[0])
print('my_text_padded len=', len(my_text_padded), my_text_padded[0])
if predictions.shape[-1] > 1:
  predictions_classes = predictions.argmax(axis=-1)
else:
  predictions_classes = (predictions > 0.5).astype('int32')

for i in range(len(predictions)):
  if int(predictions_classes[i][0]) >= 1:
    data_line.append(i)

for i in data_line:
  data.append([df_data[i][0],df_data[i][1],predictions[i][0],predictions_classes[i][0]])


print('data_lin len=', len(data_line), data_line[0])
print('data_len len=', len(data), data[0])
# df = pd.DataFrame(data, columns=['day', 'opt_type', 'message', 'resource_id', 'comment_id', '得分', '标签(1=违规)'])
df = pd.DataFrame(data, columns=['clo_1', '次数', '得分', '标签(1=违规)'])
df.to_excel("all_预测_imo_资料.xlsx", index=False, sheet_name='Sheet1', encoding='utf-8')