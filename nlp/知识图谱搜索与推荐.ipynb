{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "understanding-payment",
   "metadata": {},
   "source": [
    "## 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "trained-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import configparser\n",
    "\n",
    "def get_mysql_data(sql, mysql_config_name=None):\n",
    "    '''\n",
    "     description: 获取mysql数据\n",
    "     param {*}\n",
    "     return {*}\n",
    "    '''\n",
    "    con = configparser.RawConfigParser()\n",
    "    con.read('../config/config.ini', encoding='utf-8')\n",
    "    sections = con.sections()\n",
    "    if mysql_config_name == None:\n",
    "        try:\n",
    "            mysql = dict(con.items('mysql_nlp_tagging'))\n",
    "            connection = pymysql.connect(host=mysql['host'], port=int(mysql['port']), user=mysql['user'],password=mysql['password'], db=mysql['database'], charset='utf8mb4')\n",
    "        except:\n",
    "            connection = pymysql.connect(host='192.168.100.50', port=3306, user='root',password='Aid@pro888888', db='nlp_tagging', charset='utf8mb4')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-berkeley",
   "metadata": {},
   "source": [
    "# neo4j数据转化与接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re, os\n",
    "import json \n",
    "import requests\n",
    "from py2neo import Node, Relationship, Graph, NodeMatcher, RelationshipMatcher\n",
    "from py2neo import NodeMatcher, RelationshipMatcher\n",
    "print(os.getcwd())\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jToJson(object):\n",
    "    \"\"\"知识图谱数据接口\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化数据\"\"\"\n",
    "        # 与neo4j服务器建立连接\n",
    "        self.graph = Graph(\"http://192.168.200.155:7474\", auth=(\"neo4j\", \"aid_neo4j\"))\n",
    "        self.links = []\n",
    "        self.nodes = []\n",
    "\n",
    "    def post(self, select_name=None):\n",
    "        \"\"\"与前端交互\"\"\"\n",
    "        # 前端传过来的数据\n",
    "        if select_name == None:\n",
    "            select_name = '布地奈德'\n",
    "        # 取出所有节点数据\n",
    "        nodes_data_all = self.graph.run(\"MATCH (n) RETURN n\").data()\n",
    "        # node名存储\n",
    "        nodes_list = []\n",
    "        for node in nodes_data_all:\n",
    "            nodes_list.append(node['n']['name'])\n",
    "        # 根据前端的数据，判断搜索的关键字是否在nodes_list中存在，如果存在返回相应数据，否则返回全部数据\n",
    "        if select_name in nodes_list:\n",
    "            # 获取知识图谱中相关节点数据\n",
    "            links_data = self.graph.run(\"MATCH (n)-[r]-(b) where n.name=~'(?i).*{}.*' return r\".format(select_name)).data()\n",
    "            nodes_data = self.graph.run(\"MATCH (n)--(b) where n.name=~'(?i).*{}.*' return n,b\".format(select_name)).data()\n",
    "            self.get_select_nodes(nodes_data)\n",
    "        else:\n",
    "            # 获取知识图谱中所有节点数据\n",
    "            links_data = self.graph.run(\"MATCH ()-[r]->() RETURN r\").data()\n",
    "            nodes_data = self.graph.run(\"MATCH (n) RETURN n\").data()\n",
    "            self.get_all_nodes(nodes_data)\n",
    "\n",
    "        self.get_links(links_data)\n",
    "\n",
    "        # 数据格式转换\n",
    "        neo4j_data = {'links': self.links, 'nodes': self.nodes}\n",
    "        neo4j_data_json = json.dumps(neo4j_data, ensure_ascii=False).replace(u'\\xa0', u'')\n",
    "        return neo4j_data_json\n",
    "\n",
    "    def get_links(self, links_data):\n",
    "        \"\"\"知识图谱关系数据获取\"\"\"\n",
    "        for link in links_data:\n",
    "            links_str = re.sub(\"[\\!\\%\\[\\]\\,\\。\\{\\}\\-\\:\\'\\(\\)\\>]\", \" \", str(link['r'])).split(' ')\n",
    "            links_str = [i for i in links_str if len(i)>1]\n",
    "            if len(links_str) >= 3:\n",
    "                self.links.append({'source':links_str[0], 'name':links_str[1], 'target':links_str[2]})\n",
    "        return self.links\n",
    "\n",
    "    def get_select_nodes(self, nodes_data):\n",
    "        \"\"\"获取知识图谱中所选择的节点数据\"\"\"\n",
    "        for node in nodes_data:\n",
    "            node_str = re.sub(\"[\\!\\%\\[\\]\\,\\。\\{\\}\\-\\:\\'\\(\\)\\>]\", \" \", str(node)).split(' ')\n",
    "            node_str = [i for i in node_str if len(i)>1]\n",
    "            if len(node_str) >= 8:\n",
    "                if node_str[1] != 'Node' or node_str[5] != 'Node':\n",
    "                    self.nodes.append({'name':node_str[3], 'tag':node_str[1]})\n",
    "                    self.nodes.append({'name':node_str[7], 'tag':node_str[5]})\n",
    "        return self.nodes\n",
    "\n",
    "    def get_all_nodes(self, nodes_data):\n",
    "        \"\"\"获取知识图谱中所有节点数据\"\"\"\n",
    "        dict_node = {}\n",
    "        for node in nodes_data:\n",
    "            name = node['n']['name']\n",
    "            tag = node['n']['tag']\n",
    "            dict_node['name'] = name\n",
    "            dict_node['tag'] = tag\n",
    "            self.nodes.append(dict_node)\n",
    "            dict_node = {}\n",
    "        return self.nodes\n",
    "\n",
    "def api_entity_recognize(texts):\n",
    "    url = 'http://192.168.200.155:7712/api/medical_entity_recognize' #实体识别v1.0\n",
    "    data = {'text': texts}\n",
    "    r = requests.post(url, data=data, timeout=60)\n",
    "    return r\n",
    "\n",
    "rules = api_entity_recognize(texts='布地奈德雾化吸⼊治疗⼩⼉哮喘临床疗效观察')\n",
    "print(rules)\n",
    "print(rules.json())\n",
    "\n",
    "# data_neo4j = Neo4jToJson()\n",
    "# data_neo4j.post()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(\"http://192.168.200.155:7474\", auth=(\"neo4j\", \"aid_neo4j\"))\n",
    "nodematcher= NodeMatcher(graph)\n",
    "relamatcher = RelationshipMatcher(graph)\n",
    "\n",
    "match_str = \"MATCH (na:drug)-[*1..10]->(nb) WHERE na.name=~'(?i).*布地奈德.*' RETURN (na)-[*1..10]->(nb)\"\n",
    "match_str = '''MATCH p=(n1:drug)-[r1]-(n2)-[r2]-(n3:disease {name:'哮喘'})\n",
    "WHERE not (n1)-[]-(n3)\n",
    "WITH n1.name as Drug, n3.name as disease, n2.name as NEighborName, 1/log(size((n2)-[]-())) as AdamicAdarScore\n",
    "RETURN Drug, disease, sum(AdamicAdarScore) as TotalAdamicAdarScore\n",
    "ORDER BY TotalAdamicAdarScore DESC'''\n",
    "results = graph.run(match_str)\n",
    "\n",
    "# nodes = [] \n",
    "# for i in results:\n",
    "#     node_str = re.sub(\"Path|Node|name=\", \"\", str(i))\n",
    "#     node_str = re.sub(\"[\\!\\%\\[\\]\\,\\。\\{\\}\\-\\:\\'\\(\\)\\>]\", \" \", node_str).split(' ')\n",
    "#     node_str = [i for i in node_str if len(i)>1]\n",
    "#     if len(node_str) == 7:\n",
    "#         nodes.append(node_str[1])\n",
    "#         nodes.append(node_str[6])\n",
    "\n",
    "# nodes = list(set(nodes))\n",
    "# print(nodes)\n",
    "\n",
    "for i in results:\n",
    "    if '布地奈德' in i[0]:\n",
    "        print(i) \n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-gnome",
   "metadata": {},
   "source": [
    "# 知识图谱嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-modification",
   "metadata": {},
   "source": [
    "### KGE\n",
    "KGE就是将实体和关系嵌入到低维向量空间中，同时保留KG的结构和语义信息\n",
    "\n",
    "现有的KGE方法可以划分为三类：\n",
    "\n",
    "    1.基于翻译距离的(translational distance based)\n",
    "\n",
    "    2.基于语义匹配的(semantic matching based)\n",
    "\n",
    "    3.基于神经网络的(neural network based)\n",
    "\n",
    "测试数据 FB15K知识库[https://everest.hds.utc.fr/lib/exe/fetch.php?media=en:fb15k.tgz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, data_type):\n",
    "    with open(\"%s%s.txt\"%(data_dir, data_type), \"r\") as f:\n",
    "        data = f.read().strip().split(\"\\n\")\n",
    "        data = [i.split('\\t') for i in data]\n",
    "        print(len(data),data_type)\n",
    "        return data\n",
    "\n",
    "class KGE(nn.Module):\n",
    "    def __init__(self,model_name,ent_vec_dim,num_entities,num_relations):\n",
    "        '''\n",
    "        num_entities是所有实体的数量\n",
    "        num_relations是所有关系的数量\n",
    "        ent_vec_dim是每一个实体向量的维度\n",
    "        如果model_name是RESCAL，那么每一个关系用一个矩阵matrix表示，shape==(ent_vec_dim,ent_vec_dim)\n",
    "        '''\n",
    "        super(KGE,self).__init__()\n",
    "        self.E=nn.Embedding(num_embeddings=num_entities,embedding_dim=ent_vec_dim,padding_idx=0)\n",
    "        self.model_name=model_name\n",
    "        self.ent_vec_dim=ent_vec_dim\n",
    "        self.num_entities=num_entities\n",
    "        if self.model_name=='RESCAL':\n",
    "            self.R=nn.Embedding(num_embeddings=num_relations,embedding_dim=ent_vec_dim*ent_vec_dim,padding_idx=0)\n",
    "            self.scoreFun=self.RESCAL\n",
    "        else:\n",
    "            self.R=nn.Embedding(num_embeddings=num_relations,embedding_dim=ent_vec_dim,padding_idx=0)\n",
    "            self.scoreFun=self.DistMult\n",
    "        \n",
    "    def RESCAL(self,head_embed,rel_embed):\n",
    "        '''\n",
    "        RESCAL模型将每一个关系用一个matrix表示。\n",
    "        输入：\n",
    "            head_embed.size()==(batch_size,self.ent_vec_dim)\n",
    "            rel_embed.size()==(batch_size,self.ent_vec_dim*2)\n",
    "        输出：\n",
    "            score.size()==(batch_size,self.num_entities)\n",
    "        '''\n",
    "        batch_size=head_embed.size(0)\n",
    "        head_embed=head_embed.view(batch_size,1,self.ent_vec_dim)\n",
    "        rel_embed=rel_embed.view(batch_size,self.ent_vec_dim,self.ent_vec_dim)\n",
    "        score=torch.mm(torch.squeeze(torch.bmm(head_embed,rel_embed),dim=1),self.E.weight.transpose(1,0))\n",
    "        return score\n",
    "    \n",
    "    def DistMult(self,head_embed,rel_embed):\n",
    "        '''\n",
    "        DistMult是RESCAL的简化版，将每一个关系用一个vector表示。\n",
    "        输入：\n",
    "            head_embed.size()==(batch_size,self.ent_vec_dim)\n",
    "            rel_embed.size()==(batch_size,self.ent_vec_dim)\n",
    "        输出：\n",
    "            score.size()==(batch_size,self.num_entities)        \n",
    "        '''\n",
    "        score=torch.mm(head_embed*rel_embed,self.E.weight.transpose(1,0))\n",
    "        return score\n",
    "    \n",
    "    def forward(self,head_idx,rel_idx):\n",
    "        '''\n",
    "        输入：\n",
    "            head_idx.size()==rel_idx.size()==(batch_size,)\n",
    "        输出：\n",
    "            probabilities.size()==(batch_size,self.num_entities)     \n",
    "            即：预测每一个实体可以作为尾实体的概率\n",
    "        '''\n",
    "        batch_size=head_idx.size(0)\n",
    "        score=self.scoreFun(head_embed=self.E(head_idx),rel_embed=self.R(rel_idx))\n",
    "        assert score.size()==(batch_size,self.num_entities)\n",
    "        probabilities=torch.sigmoid(score)\n",
    "        return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "# train_data = load_data(data_dir='../data/FB15k/',data_type='freebase_mtr100_mte100-train')\n",
    "# valid_data = load_data(data_dir='../data/FB15k/',data_type='freebase_mtr100_mte100-valid')\n",
    "# test_data = load_data(data_dir='../data/FB15k/',data_type='freebase_mtr100_mte100-test')\n",
    "data = get_mysql_data('SELECT entity,relation,object FROM triple LIMIT 1000;')\n",
    "train_data = data[:int(len(data)*0.7)]\n",
    "valid_data = data[int(len(data)*0.7):int(len(data)*0.9)]\n",
    "test_data = data[int(len(data)*0.9):]\n",
    "data = train_data+valid_data+test_data\n",
    "print(len(data), data[0])\n",
    "\n",
    "# 统计所有的头实体、尾实体以及关系\n",
    "entities = sorted(list(set([d[0] for d in data]+[d[2] for d in data])))\n",
    "print(len(entities), entities[0])\n",
    "relations = sorted(list(set([d[1] for d in data])))\n",
    "print(len(relations), relations[0])\n",
    "\n",
    "# 构造entity2id和relation2id的字典映射\n",
    "entity_idxs = {entities[i]:i for i in range(len(entities))}\n",
    "relation_idxs = {relations[i]:i for i in range(len(relations))}\n",
    "\n",
    "# 生成训练数据\n",
    "train_data_idxs = [[entity_idxs[triplet[0]],relation_idxs[triplet[1]],entity_idxs[triplet[2]]] for triplet in train_data]\n",
    "\n",
    "# 生成批次的数据输入\n",
    "er_vocab = defaultdict(list)\n",
    "for triplet in train_data_idxs:\n",
    "    er_vocab[(triplet[0],triplet[1])].append(triplet[2])\n",
    "er_vocab_pairs = list(er_vocab.keys())\n",
    "batch_inputs = er_vocab_pairs[:4]\n",
    "batch_targets = torch.zeros([len(batch_inputs),len(entity_idxs)],dtype=torch.float32)\n",
    "for i,pair in enumerate(batch_inputs):\n",
    "    batch_targets[i,er_vocab[pair]] = 1\n",
    "batch_inputs = np.array(batch_inputs)\n",
    "\n",
    "print(batch_inputs)\n",
    "print(batch_targets.size())\n",
    "print(er_vocab[(3920,791)])\n",
    "print(sum(batch_targets[0]))\n",
    "\n",
    "# 前向传播\n",
    "head_idx = torch.LongTensor(batch_inputs[:,0])\n",
    "rel_idx = torch.LongTensor(batch_inputs[:,1])\n",
    "\n",
    "RESCAL = KGE(model_name='RESCAL',ent_vec_dim=200,num_entities=len(entity_idxs),num_relations=len(relation_idxs))\n",
    "DistMult = KGE(model_name='DistMult',ent_vec_dim=200,num_entities=len(entity_idxs),num_relations=len(relation_idxs))\n",
    "probabilities1 = RESCAL(head_idx,rel_idx)\n",
    "probabilities2 = DistMult(head_idx,rel_idx)\n",
    "\n",
    "# 计算BCE损失\n",
    "loss = torch.nn.BCELoss()(probabilities1,batch_targets)\n",
    "print(loss.item())\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "test_data_idxs = [[entity_idxs[triplet[0]],relation_idxs[triplet[1]],entity_idxs[triplet[2]]] for triplet in test_data]\n",
    "test_er_vocab = defaultdict(list)\n",
    "for triplet in test_data_idxs:\n",
    "    test_er_vocab[(triplet[0],triplet[1])].append(triplet[2])\n",
    "test_batch_inputs = test_data_idxs\n",
    "test_batch_inputs = np.array(test_batch_inputs)\n",
    "\n",
    "# 前向传播获取预测分数(tail_idx是标签,probabilities是预测的分数)\n",
    "head_idx = torch.tensor(test_batch_inputs[:,0])\n",
    "rel_idx = torch.tensor(test_batch_inputs[:,1])\n",
    "tail_idx = torch.tensor(test_batch_inputs[:,2])\n",
    "probabilities = RESCAL(head_idx, rel_idx)\n",
    "\n",
    "# 解析结果\n",
    "entity_ids = {}\n",
    "relation_ids = {}\n",
    "for k,v in entity_idxs.items():\n",
    "    entity_ids[v] = k\n",
    "for k,v in relation_idxs.items():\n",
    "    relation_ids[v] = k\n",
    "\n",
    "pre_data = []\n",
    "for i in range(len(test_batch_inputs)):\n",
    "    head, rel, tail = head_idx[i].item(), rel_idx[i].item(), tail_idx[i].item() # (head,rel,tail)是当前的三元组\n",
    "    all_fact_tails = test_er_vocab[(head,rel)]                                  #给定当前头实体和关系下，测试集中所有符合的尾实体\n",
    "    predict_score = probabilities[i][tail].item()                               #首先取出模型预测当前三元组尾实体的分数\n",
    "    probabilities[i][all_fact_tails] = 0.0                                      #将测试集中所有与(head,rel,?)满足事实三元组的尾实体置空\n",
    "    probabilities[i][tail] = int(predict_score * 100)                                      #恢复模型预测当前三元组尾实体的分数\n",
    "    for tail in all_fact_tails:\n",
    "        pre_data.append([entity_ids[int(head)], relation_ids[int(rel)], entity_ids[int(tail)], predict_score])\n",
    "df = pd.DataFrame(data=pre_data, columns=['head', 'rel', 'tail', 'score']).sort_values('score')\n",
    "# df.to_csv('KCE_test.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-folder",
   "metadata": {},
   "source": [
    "KGE-MeanRank\n",
    "1、probabilities的长度是num_entities，我们将probabilities降序排列，即分数高的排在前面。\n",
    "2、mean_rank中的rank指的就是：模型对于当前三元组尾实体在所有实体中的分数排名。所以这个数值越低越好，因为越低，表明排名越靠前。\n",
    "3、Hit@1,3,10: @就是英文的at。hit at 1指的就是将尾实体排在第一位的次数/测试集合大小，hit at 3和git at 10同理。显然这个数值越高越好。越高说明每一个三元组的尾实体都被模型排名的非常靠前。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_scores,sort_idxs = torch.sort(probabilities,dim=1,descending=True)\n",
    "ranks = []\n",
    "hits = [[] for _ in range(10)]\n",
    "for i in range(len(test_batch_inputs)):\n",
    "    rank = np.where(sort_idxs[i].numpy()==tail_idx[i].item())[0][0]+1\n",
    "    ranks.append(rank)\n",
    "    for hits_level in range(1,11):\n",
    "        if rank <= hits_level:\n",
    "            hits[hits_level-1].append(1.0)\n",
    "        else:\n",
    "            hits[hits_level-1].append(0.0)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-microphone",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitat10 = np.mean(hits[9])\n",
    "hitat3 = np.mean(hits[2])\n",
    "hitat1 = np.mean(hits[0])\n",
    "mean_rank = np.mean(ranks)\n",
    "mrr = np.mean(1./np.array(mean_rank))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-luxembourg",
   "metadata": {},
   "source": [
    "## TransE 转移距离模型\n",
    "转移距离模型（Translational Distance Model）的主要思想是将衡量向量化后的知识图谱中三元组的合理性问题，转化成衡量头实体和尾实体的距离问题。这一方法的重点是如何设计得分函数，得分函数常常被设计成利用关系把头实体转移到尾实体的合理性的函数。 受词向量的启发，由词与词在向量空间的语义层面关系，可以拓展到知识图谱中头实体和尾实体在向量空间的关系。也就是说，同样可以 考虑把知识图谱中的头实体和尾实体映射到向量空间中，且它们之间的 联系也可以考虑成三元组中的关系。\n",
    "\n",
    "TransE便是受到了词向量中平移不变性的启发，在 TransE 中，把实体和关系都表示为向量，对于某一 个具体的关系（head, relation, tail），把关系的向量表示解释成头实体的向量到尾实体的向量的转移向量（Translation vector）。也就是说， 如果在一个知识图谱中，某一个三元组成立，则它的实体和关系需要满 足关系head+relation≈tail。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "entity2id = {}\n",
    "relation2id = {}\n",
    "\n",
    "def data_loader(file):\n",
    "    file1 = file + \"train.txt\"\n",
    "    file2 = file + \"entity2id.txt\"\n",
    "    file3 = file + \"relation2id.txt\"\n",
    "\n",
    "    with open(file2, 'r') as f1, open(file3, 'r') as f2:\n",
    "        lines1 = f1.readlines()\n",
    "        lines2 = f2.readlines()\n",
    "        for line in lines1:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            entity2id[line[0]] = line[1]\n",
    "\n",
    "        for line in lines2:\n",
    "            line = line.strip().split('\\t')\n",
    "            if len(line) != 2:\n",
    "                continue\n",
    "            relation2id[line[0]] = line[1]\n",
    "\n",
    "    entity_set = set()\n",
    "    relation_set = set()\n",
    "    triple_list = []\n",
    "\n",
    "    with codecs.open(file1, 'r') as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            triple = line.strip().split(\"\\t\")\n",
    "            if len(triple) != 3:\n",
    "                continue\n",
    "\n",
    "            h_ = entity2id[triple[0]]\n",
    "            t_ = entity2id[triple[1]]\n",
    "            r_ = relation2id[triple[2]]\n",
    "\n",
    "            triple_list.append([h_,t_,r_])\n",
    "\n",
    "            entity_set.add(h_)\n",
    "            entity_set.add(t_)\n",
    "\n",
    "            relation_set.add(r_)\n",
    "\n",
    "    return entity_set, relation_set, triple_list\n",
    "\n",
    "def distanceL2(h,r,t):\n",
    "    #为方便求梯度，去掉sqrt\n",
    "    return np.sum(np.square(h + r - t))\n",
    "\n",
    "def distanceL1(h,r,t):\n",
    "    return np.sum(np.fabs(h+r-t))\n",
    "\n",
    "class TransE:\n",
    "    def __init__(self, entity_set, relation_set, triple_list, embedding_dim=100, learning_rate=0.01, margin=1,L1=True):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.margin = margin\n",
    "        self.entity = entity_set\n",
    "        self.relation = relation_set\n",
    "        self.triple_list = triple_list\n",
    "        self.L1=L1\n",
    "\n",
    "        self.loss = 0\n",
    "\n",
    "    def emb_initialize(self):\n",
    "        relation_dict = {}\n",
    "        entity_dict = {}\n",
    "\n",
    "        for relation in self.relation:\n",
    "            r_emb_temp = np.random.uniform(-6/math.sqrt(self.embedding_dim), 6/math.sqrt(self.embedding_dim), self.embedding_dim)\n",
    "            relation_dict[relation] = r_emb_temp/np.linalg.norm(r_emb_temp,ord=2)\n",
    "\n",
    "        for entity in self.entity:\n",
    "            e_emb_temp = np.random.uniform(-6/math.sqrt(self.embedding_dim), 6/math.sqrt(self.embedding_dim), self.embedding_dim)\n",
    "            entity_dict[entity] = e_emb_temp/np.linalg.norm(e_emb_temp,ord=2)\n",
    "\n",
    "        self.relation = relation_dict\n",
    "        self.entity = entity_dict\n",
    "\n",
    "    def train(self, epochs):\n",
    "        nbatches = 400\n",
    "        batch_size = len(self.triple_list) // nbatches\n",
    "        print(\"batch size: \", batch_size)\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            self.loss = 0\n",
    "\n",
    "            for k in range(nbatches):\n",
    "                # Sbatch:list\n",
    "                Sbatch = random.sample(self.triple_list, batch_size)\n",
    "                Tbatch = []\n",
    "\n",
    "                for triple in Sbatch:\n",
    "                    # 每个triple选3个负样例\n",
    "                    # for i in range(3):\n",
    "                    corrupted_triple = self.Corrupt(triple)\n",
    "                    if (triple, corrupted_triple) not in Tbatch:\n",
    "                        Tbatch.append((triple, corrupted_triple))\n",
    "                self.update_embeddings(Tbatch)\n",
    "\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"epoch: \", epoch , \"cost time: %s\"%(round((end - start),3)))\n",
    "            print(\"loss: \", self.loss)\n",
    "\n",
    "            #保存临时结果\n",
    "            if epoch % 20 == 0:\n",
    "                with codecs.open(\"entity_temp\", \"w\") as f_e:\n",
    "                    for e in self.entity.keys():\n",
    "                        f_e.write(e + \"\\t\")\n",
    "                        f_e.write(str(list(self.entity[e])))\n",
    "                        f_e.write(\"\\n\")\n",
    "                with codecs.open(\"relation_temp\", \"w\") as f_r:\n",
    "                    for r in self.relation.keys():\n",
    "                        f_r.write(r + \"\\t\")\n",
    "                        f_r.write(str(list(self.relation[r])))\n",
    "                        f_r.write(\"\\n\")\n",
    "\n",
    "        print(\"写入文件...\")\n",
    "        with codecs.open(\"entity_50dim_batch400\", \"w\") as f1:\n",
    "            for e in self.entity.keys():\n",
    "                f1.write(e + \"\\t\")\n",
    "                f1.write(str(list(self.entity[e])))\n",
    "                f1.write(\"\\n\")\n",
    "\n",
    "        with codecs.open(\"relation50dim_batch400\", \"w\") as f2:\n",
    "            for r in self.relation.keys():\n",
    "                f2.write(r + \"\\t\")\n",
    "                f2.write(str(list(self.relation[r])))\n",
    "                f2.write(\"\\n\")\n",
    "        print(\"写入完成\")\n",
    "\n",
    "\n",
    "    def Corrupt(self,triple):\n",
    "        corrupted_triple = copy.deepcopy(triple)\n",
    "        seed = random.random()\n",
    "        if seed > 0.5:\n",
    "            # 替换head\n",
    "            rand_head = triple[0]\n",
    "            while rand_head == triple[0]:\n",
    "                rand_head = random.sample(self.entity.keys(),1)[0]\n",
    "            corrupted_triple[0]=rand_head\n",
    "\n",
    "        else:\n",
    "            # 替换tail\n",
    "            rand_tail = triple[1]\n",
    "            while rand_tail == triple[1]:\n",
    "                rand_tail = random.sample(self.entity.keys(), 1)[0]\n",
    "            corrupted_triple[1] = rand_tail\n",
    "        return corrupted_triple\n",
    "\n",
    "    def update_embeddings(self, Tbatch):\n",
    "        copy_entity = copy.deepcopy(self.entity)\n",
    "        copy_relation = copy.deepcopy(self.relation)\n",
    "\n",
    "        for triple, corrupted_triple in Tbatch:\n",
    "            # 取copy里的vector累积更新\n",
    "            h_correct_update = copy_entity[triple[0]]\n",
    "            t_correct_update = copy_entity[triple[1]]\n",
    "            relation_update = copy_relation[triple[2]]\n",
    "\n",
    "            h_corrupt_update = copy_entity[corrupted_triple[0]]\n",
    "            t_corrupt_update = copy_entity[corrupted_triple[1]]\n",
    "\n",
    "            # 取原始的vector计算梯度\n",
    "            h_correct = self.entity[triple[0]]\n",
    "            t_correct = self.entity[triple[1]]\n",
    "            relation = self.relation[triple[2]]\n",
    "\n",
    "            h_corrupt = self.entity[corrupted_triple[0]]\n",
    "            t_corrupt = self.entity[corrupted_triple[1]]\n",
    "\n",
    "            if self.L1:\n",
    "                dist_correct = distanceL1(h_correct, relation, t_correct)\n",
    "                dist_corrupt = distanceL1(h_corrupt, relation, t_corrupt)\n",
    "            else:\n",
    "                dist_correct = distanceL2(h_correct, relation, t_correct)\n",
    "                dist_corrupt = distanceL2(h_corrupt, relation, t_corrupt)\n",
    "\n",
    "            err = self.hinge_loss(dist_correct, dist_corrupt)\n",
    "\n",
    "            if err > 0:\n",
    "                self.loss += err\n",
    "\n",
    "                grad_pos = 2 * (h_correct + relation - t_correct)\n",
    "                grad_neg = 2 * (h_corrupt + relation - t_corrupt)\n",
    "\n",
    "                if self.L1:\n",
    "                    for i in range(len(grad_pos)):\n",
    "                        if (grad_pos[i] > 0):\n",
    "                            grad_pos[i] = 1\n",
    "                        else:\n",
    "                            grad_pos[i] = -1\n",
    "\n",
    "                    for i in range(len(grad_neg)):\n",
    "                        if (grad_neg[i] > 0):\n",
    "                            grad_neg[i] = 1\n",
    "                        else:\n",
    "                            grad_neg[i] = -1\n",
    "\n",
    "                # head系数为正，减梯度；tail系数为负，加梯度\n",
    "                h_correct_update -= self.learning_rate * grad_pos\n",
    "                t_correct_update -= (-1) * self.learning_rate * grad_pos\n",
    "\n",
    "                # corrupt项整体为负，因此符号与correct相反\n",
    "                if triple[0] == corrupted_triple[0]:  # 若替换的是尾实体，则头实体更新两次\n",
    "                    h_correct_update -= (-1) * self.learning_rate * grad_neg\n",
    "                    t_corrupt_update -= self.learning_rate * grad_neg\n",
    "\n",
    "                elif triple[1] == corrupted_triple[1]:  # 若替换的是头实体，则尾实体更新两次\n",
    "                    h_corrupt_update -= (-1) * self.learning_rate * grad_neg\n",
    "                    t_correct_update -= self.learning_rate * grad_neg\n",
    "\n",
    "                #relation更新两次\n",
    "                relation_update -= self.learning_rate*grad_pos\n",
    "                relation_update -= (-1)*self.learning_rate*grad_neg\n",
    "\n",
    "\n",
    "        #batch norm\n",
    "        for i in copy_entity.keys():\n",
    "            copy_entity[i] /= np.linalg.norm(copy_entity[i])\n",
    "        for i in copy_relation.keys():\n",
    "            copy_relation[i] /= np.linalg.norm(copy_relation[i])\n",
    "\n",
    "        # 达到批量更新的目的\n",
    "        self.entity = copy_entity\n",
    "        self.relation = copy_relation\n",
    "\n",
    "    def hinge_loss(self,dist_correct,dist_corrupt):\n",
    "        return max(0,dist_correct-dist_corrupt+self.margin)\n",
    "\n",
    "data = open('../data/FB15k/freebase_mtr100_mte100-train.txt', 'r').read().strip().split(\"\\n\")\n",
    "data = [i.split('\\t') for i in data]\n",
    "entities = sorted(list(set([d[0] for d in data]+[d[2] for d in data])))\n",
    "relations = sorted(list(set([d[1] for d in data])))\n",
    "entity_idxs = {entities[i]:i for i in range(len(entities))}\n",
    "relation_idxs = {relations[i]:i for i in range(len(relations))}\n",
    "triplet_list = [[entity_idxs[triplet[0]], relation_idxs[triplet[1]], entity_idxs[triplet[2]]] for triplet in data]\n",
    "print(len(triplet_list), triplet_list[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-canvas",
   "metadata": {},
   "source": [
    "## Ampligraph\n",
    "pip install ampligraph tensorboard git+https://github.com/Phlya/adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "freelance-gather",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pymysql\n",
    "import configparser\n",
    "\n",
    "import ampligraph\n",
    "from ampligraph.latent_features import TransE, ComplEx\n",
    "from ampligraph.utils import save_model, restore_model, create_tensorboard_visualizations\n",
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score, evaluate_performance, train_test_split_no_unseen\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "print(ampligraph.__version__)\n",
    "\n",
    "def display_aggregate_metrics(ranks):\n",
    "    print('Mean Rank:', mr_score(ranks))\n",
    "    print('Mean Reciprocal Rank:', mrr_score(ranks))\n",
    "    print('Hits@1:', hits_at_n_score(ranks, 1))\n",
    "    print('Hits@3:', hits_at_n_score(ranks, 3))\n",
    "    print('Hits@10:', hits_at_n_score(ranks, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug');\")\n",
    "dataset = pd.DataFrame(data)\n",
    "dataset.columns = ['subject', 'predicate', 'object']\n",
    "print(dataset.shape)\n",
    "\n",
    "test_train, X_valid = train_test_split_no_unseen(dataset.values, 5000, seed=0)\n",
    "X_train, X_test = train_test_split_no_unseen(test_train, 10000, seed=0)\n",
    "print('Total triples:', dataset.shape)\n",
    "print('Size of train:', X_train.shape)\n",
    "print('Size of valid:', X_valid.shape)\n",
    "print('Size of test:', X_test.shape)\n",
    "\n",
    "# create a model as earlier\n",
    "model = TransE(\n",
    "    k=400,\n",
    "    epochs=80,\n",
    "    eta=30,\n",
    "    loss='multiclass_nll',\n",
    "    # initializer='xavier',\n",
    "    regularizer='LP',\n",
    "    optimizer='adam',\n",
    "    initializer_params={'uniform':False},\n",
    "    regularizer_params= {'lambda':0.0001, 'p':2},\n",
    "    optimizer_params={'lr':0.001},\n",
    "    embedding_model_params={'norm':1},\n",
    "    seed=0,\n",
    "    batches_count=64,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "# call model.fit by passing early stopping params\n",
    "model.fit(\n",
    "    X_train,                                      # training set\n",
    "    early_stopping=True,                          # set early stopping to true\n",
    "    early_stopping_params={\n",
    "        'x_valid': X_valid,   # Validation set on which early stopping will be performed\n",
    "        'criteria': 'mrr',    # metric to watch during early stopping\n",
    "        'burn_in': 150,       # Burn in time, i.e. early stopping checks will not be performed till 150 epochs\n",
    "        'check_interval': 50, # After burn in time, early stopping checks will be performed at every 50th epochs (i.e. 150, 200, 250, ...)\n",
    "        'stop_interval': 2,   # If the monitored criteria degrades for these many epochs, the training stops. \n",
    "        'corrupt_side': 's,o'#'s,o'  # Which sides to corrupt furing early stopping evaluation (default both subject and obj as described earlier)\n",
    "        }   # pass the early stopping params\n",
    "    )\n",
    "\n",
    "# evaluate the model with filter\n",
    "X_filter = np.concatenate([X_train, X_valid, X_test], 0)\n",
    "ranks = evaluate_performance(X_test, model=model, filter_triples=X_filter)\n",
    "display_aggregate_metrics(ranks)\n",
    "save_model(model, '/data/nlp_models/TransE-small.pkl')\n",
    "\n",
    "\n",
    "data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug') LIMIT 5;\")\n",
    "hypothesis = np.array(data)\n",
    "# stack it horizontally to create s, p, o\n",
    "# p_list = [i[1] for i in data]\n",
    "# o_list = [i[2] for i in data]\n",
    "# hypothesis = np.column_stack(['布地奈德', p_list, o_list])\n",
    "\n",
    "triple_scores = model.predict(hypothesis)\n",
    "probs = expit(triple_scores)\n",
    "ranks = evaluate_performance(\n",
    "    hypothesis, \n",
    "    model=model, \n",
    "    filter_triples=X_filter,\n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=True,\n",
    "    verbose=True\n",
    "    )\n",
    "print(len(ranks))\n",
    "df = pd.DataFrame(\n",
    "    list(zip([','.join(x) for x in hypothesis], ranks, np.squeeze(triple_scores), np.squeeze(probs))),\n",
    "    columns=['triple', 'rank', 'score', 'prob']\n",
    "    ).sort_values('prob')\n",
    "df\n",
    "\n",
    "# epochs:100\n",
    "# Mean Rank: 10991.15365\n",
    "# Mean Reciprocal Rank: 0.029947334564126286\n",
    "# Hits@1: 0.0202\n",
    "# Hits@3: 0.03065\n",
    "# Hits@10: 0.04755\n",
    "\n",
    "# epochs:150\n",
    "# Mean Rank: 11571.2768\n",
    "# Mean Reciprocal Rank: 0.024059398719972723\n",
    "# Hits@1: 0.01705\n",
    "# Hits@3: 0.0241\n",
    "# Hits@10: 0.03665\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "younger-hardware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([['哮喘', 'Symptom_Disease', '脱症'],\n",
      "       ['哮喘', 'Symptom_Disease', '肾小球基底膜蛾噬现象'],\n",
      "       ['哮喘', 'Symptom_Disease', '帕里诺眼-腺综合征'],\n",
      "       ['哮喘', 'Symptom_Disease', '腹部“气串样”肿块'],\n",
      "       ['哮喘', 'Symptom_Disease', '剧烈运动后呕吐'],\n",
      "       ['哮喘', 'Symptom_Disease', '脓疱性痤疮'],\n",
      "       ['哮喘', 'Symptom_Disease', '菌尿'],\n",
      "       ['哮喘', 'Symptom_Disease', '小儿双下肢蜷曲'],\n",
      "       ['哮喘', 'Symptom_Disease', '额头长痘'],\n",
      "       ['哮喘', 'Symptom_Disease', '牙龈深红或暗红色'],\n",
      "       ['哮喘', 'Symptom_Disease', '扁桃体小或缺如'],\n",
      "       ['哮喘', 'Symptom_Disease', '饮水呛咳'],\n",
      "       ['哮喘', 'Symptom_Disease', '食管恶性病变'],\n",
      "       ['哮喘', 'Symptom_Disease', '鼻衂'],\n",
      "       ['哮喘', 'Symptom_Disease', '新生儿皮肤薄'],\n",
      "       ['哮喘', 'Symptom_Disease', '尾骨压痛及异常活动'],\n",
      "       ['哮喘', 'Symptom_Disease', '反复肺不张'],\n",
      "       ['哮喘', 'Symptom_Disease', '上半身代偿性多汗症'],\n",
      "       ['哮喘', 'Symptom_Disease', '新生儿心力衰竭'],\n",
      "       ['哮喘', 'Symptom_Disease', '震荡性神经受累'],\n",
      "       ['哮喘', 'Symptom_Disease', '真皮侵袭性生长'],\n",
      "       ['哮喘', 'Symptom_Disease', '心悸伴高血压'],\n",
      "       ['哮喘', 'Symptom_Disease', '难治性贫血'],\n",
      "       ['哮喘', 'Symptom_Disease', '淋巴结退化'],\n",
      "       ['哮喘', 'Symptom_Disease', '突肤挺胸'],\n",
      "       ['哮喘', 'Symptom_Disease', '外周围血淋巴细...'],\n",
      "       ['哮喘', 'Symptom_Disease', '妊娠斑'],\n",
      "       ['哮喘', 'Symptom_Disease', '大动脉错位'],\n",
      "       ['哮喘', 'Symptom_Disease', '去质状态'],\n",
      "       ['哮喘', 'Symptom_Disease', '婴儿湿疹结痂'],\n",
      "       ['哮喘', 'Symptom_Disease', '耳孔处肿物'],\n",
      "       ['哮喘', 'Symptom_Disease', '“红光满面”'],\n",
      "       ['哮喘', 'Symptom_Disease', '脐带感染'],\n",
      "       ['哮喘', 'Symptom_Disease', '凝血因子功能的障碍'],\n",
      "       ['哮喘', 'Symptom_Disease', '乳白色尿'],\n",
      "       ['哮喘', 'Symptom_Disease', '胎儿宫内发育迟缓'],\n",
      "       ['哮喘', 'Symptom_Disease', '抬肩以助呼吸'],\n",
      "       ['哮喘', 'Symptom_Disease', '食指不明原因疼痛'],\n",
      "       ['哮喘', 'Symptom_Disease', '骨髓浆细胞增多'],\n",
      "       ['哮喘', 'Symptom_Disease', '食管腔梗阻'],\n",
      "       ['哮喘', 'Symptom_Disease', '儿童骨缺损'],\n",
      "       ['哮喘', 'Symptom_Disease', '肺泡出血'],\n",
      "       ['哮喘', 'Symptom_Disease', '弥漫性头痛'],\n",
      "       ['哮喘', 'Symptom_Disease', '胃寒疼痛'],\n",
      "       ['哮喘', 'Symptom_Disease', '块状痰'],\n",
      "       ['哮喘', 'Symptom_Disease', '肺动脉瓣狭窄杂音'],\n",
      "       ['哮喘', 'Symptom_Disease', '梗噎'],\n",
      "       ['哮喘', 'Symptom_Disease', '逐渐加重的右侧...'],\n",
      "       ['哮喘', 'Symptom_Disease', '顽固性心功能不全'],\n",
      "       ['哮喘', 'Symptom_Disease', '排出结石']], dtype='<U73'), array([-31.77799 , -32.534676, -32.845963, -32.93617 , -33.057823,\n",
      "       -33.22902 , -33.321846, -33.32568 , -33.34592 , -33.360252,\n",
      "       -33.47181 , -33.591568, -33.599396, -33.664192, -33.695126,\n",
      "       -33.701817, -33.735943, -33.737556, -33.741535, -33.793583,\n",
      "       -33.808144, -33.82427 , -33.827606, -33.834564, -33.835056,\n",
      "       -33.837067, -33.842476, -33.845043, -33.857327, -33.86006 ,\n",
      "       -33.864838, -33.874767, -33.884274, -33.894978, -33.89703 ,\n",
      "       -33.904823, -33.923317, -33.92627 , -33.931107, -33.936207,\n",
      "       -33.938957, -33.944073, -33.94909 , -33.958244, -33.960854,\n",
      "       -33.96163 , -33.966835, -33.973145, -33.984222, -33.985065],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.discovery import query_topn\n",
    "\n",
    "model = restore_model('/data/nlp_models/TransE-small.pkl')\n",
    "\n",
    "# data = get_mysql_data(\"SELECT entity,relation FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug') LIMIT 100;\")\n",
    "# data_dt = {}\n",
    "# for i in data:\n",
    "#     data_dt[i[0]] = i[1]\n",
    "\n",
    "# for k,v in data_dt.items():\n",
    "#     print(query_topn(model, top_n=10, head=k, relation=v, tail=None))\n",
    "\n",
    "\n",
    "print(query_topn(model, top_n=50, head='哮喘', relation='Symptom_Disease', tail=None))\n",
    "\n",
    "#query_topn(model, top_n=20, head='哮喘', relation='Symptom_Disease', tail=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-apparatus",
   "metadata": {},
   "source": [
    "ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE relation IN ('English_Name','Symptom_Disease','Food_Good','Food_Bad');\")\n",
    "X = np.array(data)\n",
    "\n",
    "entities = np.unique(np.concatenate([X[:,0], X[:,2]]))\n",
    "relations = np.unique((X[:,1]))\n",
    "\n",
    "data = {}\n",
    "num_test = int(len(X) * (20/100))\n",
    "data['train'], data['test'] = train_test_split_no_unseen(X, test_size=num_test, seed=0, allow_duplication=False)\n",
    "print('train set size:', data['train'].shape)\n",
    "print('test set size:', data['test'].shape)\n",
    "\n",
    "model = ComplEx(\n",
    "    batches_count=100,\n",
    "    seed=0,\n",
    "    epochs=5,\n",
    "    k=150,\n",
    "    eta=5,\n",
    "    optimizer='adam',\n",
    "    optimizer_params={'lr':1e-3},\n",
    "    loss='multiclass_nll',\n",
    "    regularizer='LP',\n",
    "    regularizer_params={'p':3, 'lambda':1e-5},\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "model.fit(data['train'], early_stopping=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE relation IN ('English_Name','Symptom_Disease','Food_Good','Food_Bad') LIMIT 10000;\")\n",
    "X_unseen = data['test'] #np.array(data)\n",
    "positives_filter = X\n",
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack(((positives_filter, X_unseen)))}))\n",
    "\n",
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen,\n",
    "    model=model,\n",
    "    filter_triples=unseen_filter,\n",
    "    corrupt_side='s+o',\n",
    "    use_default_protocol=False,\n",
    "    verbose=True\n",
    ")\n",
    "display_aggregate_metrics(ranks_unseen)\n",
    "\n",
    "scores = model.predict(X_unseen)\n",
    "probs = expit(scores)\n",
    "df = pd.DataFrame(\n",
    "    list(zip([', '.join(x) for x in X_unseen], ranks_unseen, np.squeeze(scores), np.squeeze(probs))),\n",
    "    columns=['triple', 'rank', 'score', 'prob']\n",
    "    ).sort_values('score')\n",
    "# create_tensorboard_visualizations(model, 'GoT_embeddings')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-saturn",
   "metadata": {},
   "source": [
    "## lightKG\n",
    "\n",
    "测试环境 Python 3.6.8 Pytorch 1.4.0\n",
    "所需依赖\n",
    "torchtext>=0.4.0\n",
    "tqdm>=4.28.1\n",
    "torch>=1.0.0\n",
    "pytorch_crf>=0.7.0\n",
    "scikit_learn>=0.20.2\n",
    "networkx>=2.2\n",
    "revtok\n",
    "jieba\n",
    "regex\n",
    "运行前需要按如下方式安装lightKG库 \n",
    "!pip install -i https://pypi.douban.com/simple/ lightKG。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from lightkg.krl import KRL\n",
    "from lightkg.krl.config import DEFAULT_CONFIG\n",
    "\n",
    "DEFAULT_CONFIG['epoch'] = 10 #修改epoch，默认1000\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug')\")\n",
    "train_data = pd.DataFrame(data)\n",
    "test_data = pd.DataFrame(data[int(len(data)*0.8):])\n",
    "train_data.to_csv('/home/aid/Github/kg/data/lightkg_data/train.sample.csv', sep=',', encoding='utf-8')\n",
    "test_data.to_csv('/home/aid/Github/kg/data/lightkg_data/test.sample.csv', sep=',', encoding='utf-8')\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "krl = KRL()\n",
    "krl.train('/home/aid/Github/kg/data/lightkg_data/train.sample.csv',\n",
    "    model_type='TransE',\n",
    "    dev_path='/home/aid/Github/kg/data/lightkg_data/test.sample.csv',\n",
    "    save_path='/home/aid/Github/kg/data/lightkg_data/LP_{}'.format('TransE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "krl = KRL()\n",
    "krl.load(save_path='/home/aid/Github/kg/data/lightkg_data/LP_{}'.format('TransE'), model_type='TransE')\n",
    "# krl.test('/home/aid/Github/kg/data/lightkg_data/test.sample.csv')\n",
    "\n",
    "data = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug') LIMIT 100;\")\n",
    "for i in data:\n",
    "    head, rel, tail = i[0], i[1], i[2]\n",
    "    predict_score = krl.predict(head=head, rel=rel, tail=tail)\n",
    "    predict_head = krl.predict_head(rel=rel, tail=tail, topk=3)\n",
    "    # predict_rel = krl.predict_rel(head=head, tail=tail, topk=3)\n",
    "    # predict_tail = krl.predict_tail(head=head, rel=rel, topk=3)\n",
    "    # print('---->预测三元组其概率:', predict_score)\n",
    "    print('---->预测三元组其头实体:', predict_head)\n",
    "    # print('---->预测三元组其关系:', predict_rel)\n",
    "    # print('---->预测三元组其尾实体:', predict_tail)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-punishment",
   "metadata": {},
   "source": [
    "## Pykeen\n",
    "pip install git+https://github.com/pykeen/pykeen.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import pykeen\n",
    "from pykeen.datasets import Nations\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.hpo import hpo_pipeline\n",
    "from pykeen.models.predict import predict_triples_df\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'svg'\n",
    "import pymysql\n",
    "import configparser\n",
    "\n",
    "def get_mysql_data(sql, mysql_config_name=None):\n",
    "    '''\n",
    "     description: 获取mysql数据\n",
    "     param {*}\n",
    "     return {*}\n",
    "    '''\n",
    "    con = configparser.RawConfigParser()\n",
    "    con.read('../config/config.ini', encoding='utf-8')\n",
    "    sections = con.sections()\n",
    "    if mysql_config_name == None:\n",
    "        try:\n",
    "            mysql = dict(con.items('mysql_nlp_tagging'))\n",
    "            connection = pymysql.connect(host=mysql['host'], port=int(mysql['port']), user=mysql['user'],password=mysql['password'], db=mysql['database'], charset='utf8mb4')\n",
    "        except:\n",
    "            connection = pymysql.connect(host='192.168.100.50', port=3306, user='root',password='Aid@pro888888', db='nlp_tagging', charset='utf8mb4')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "pykeen.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_data  = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug')\")\n",
    "triples_data = np.array(triples_data)\n",
    "train_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data))\n",
    "test_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data[int(len(triples_data)*0.7):int(len(triples_data)*0.9)]))\n",
    "vali_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data[int(len(triples_data)*0.9):]))\n",
    "print('data train:', train_triples)\n",
    "print('data test:', test_triples)\n",
    "print('data vali:', vali_triples)\n",
    "\n",
    "pipeline_result = pipeline(\n",
    "    training=train_triples,\n",
    "    testing=test_triples,\n",
    "    validation=vali_triples,\n",
    "    model='TransE',#'TransE'TuckER,\n",
    "    model_kwargs=dict(embedding_dim=128),\n",
    "    training_kwargs=dict(num_epochs=10, use_tqdm_batch=False),\n",
    "    device='cpu',\n",
    "    )\n",
    "# pipeline_result.save_to_directory('/data/nlp_models/TuckER') #('../models/TransE')\n",
    "\n",
    "realistic_mean_rank = pipeline_result.get_metric('meanreciprocalrank')\n",
    "mean_rank = pipeline_result.get_metric('mean_rank')\n",
    "hits_at_1 = pipeline_result.get_metric('hits@10')\n",
    "hits_at_3 = pipeline_result.get_metric('hits@10')\n",
    "hits_at_10 = pipeline_result.get_metric('hits@10')\n",
    "print('realistic_mean_rank:',realistic_mean_rank, '\\nmean_rank:',mean_rank, '\\nhits_at_1:',hits_at_1, '\\nhits_at_3:',hits_at_3, '\\nhits_at_10:',hits_at_10)\n",
    "# pipeline_result.plot(er_kwargs=dict(plot_relations=True))\n",
    "# plt.savefig('../rules/toy_3.png', dpi=1000)\n",
    "\n",
    "# result = pipeline(\n",
    "#     dataset='Nations',\n",
    "#     model='PairRE',\n",
    "#     # Training configuration\n",
    "#     training_kwargs=dict(\n",
    "#         num_epochs=200,\n",
    "#         use_tqdm_batch=False,\n",
    "#     ),  \n",
    "#     # Runtime configuration\n",
    "#     random_seed=1235,\n",
    "#     device='cpu',\n",
    "# )\n",
    "# 0.003185485312360561 1225.2230224609375\n",
    "# 0.004759519038076153 0.004759519038076153 0.004759519038076153"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_result.save_to_directory('/data/nlp_models/TuckER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_result.model:\n",
    "    model = pipeline_result.model\n",
    "else:\n",
    "    pass\n",
    "    # model = torch.load('/data/nlp_models/TuckER/trained_model.pkl')#('../models/TransE/trained_model.pkl')\n",
    "\n",
    "import pymysql\n",
    "import configparser\n",
    "def get_mysql_data(sql, mysql_config_name=None):\n",
    "    '''\n",
    "     description: 获取mysql数据\n",
    "     param {*}\n",
    "     return {*}\n",
    "    '''\n",
    "    con = configparser.RawConfigParser()\n",
    "    con.read('../config/config.ini', encoding='utf-8')\n",
    "    sections = con.sections()\n",
    "    if mysql_config_name == None:\n",
    "        try:\n",
    "            mysql = dict(con.items('mysql_nlp_tagging'))\n",
    "            connection = pymysql.connect(host=mysql['host'], port=int(mysql['port']), user=mysql['user'],password=mysql['password'], db=mysql['database'], charset='utf8mb4')\n",
    "        except:\n",
    "            connection = pymysql.connect(host='192.168.100.50', port=3306, user='root',password='Aid@pro888888', db='nlp_tagging', charset='utf8mb4')\n",
    "\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(sql)\n",
    "    data = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "triples_data  = get_mysql_data(\"SELECT entity,relation,object FROM triple WHERE  o_type IN ('disease','symptom','check', 'drug')\")\n",
    "triples_data = np.array(triples_data)\n",
    "train_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data[:int(len(triples_data)*0.7)]))\n",
    "test_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data[int(len(triples_data)*0.7):int(len(triples_data)*0.9)]))\n",
    "val_triples = TriplesFactory.from_labeled_triples(triples=np.array(triples_data[int(len(triples_data)*0.9):]))\n",
    "\n",
    "\n",
    "predict_data = []\n",
    "head, rel, tail = '哮喘', 'Symptom_Disease', ''\n",
    "head, rel, tail = '哮喘', 'Complication', ''\n",
    "# predict_head_df = model.get_head_prediction_df(relation_label=rel, tail_label=tail, triples_factory=train_triples)\n",
    "# predict_rel_df = model.get_relation_prediction_df(head_label=head, tail_label=tail, triples_factory=train_triples)\n",
    "predict_tail_df = model.get_tail_prediction_df(head_label=head, relation_label=rel, triples_factory=train_triples)\n",
    "for x in predict_tail_df.values.tolist():\n",
    "    predict_data.append([head, x[0], x[1], x[2], x[3]])\n",
    "predict_df = pd.DataFrame(predict_data, columns=['head', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "# predict_df = pd.DataFrame(predict_data, columns=['head', 'rel', 'tail', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "predict_df.sort_values('pre_score')\n",
    "predict_df.to_csv('test2.csv')\n",
    "predict_df.head(50)\n",
    "\n",
    "# predict_data = []\n",
    "# for i in triples_data:\n",
    "#     head, rel, tail = i[0], i[1], i[2]\n",
    "#     # predict_head_df = model.get_head_prediction_df(relation_label=rel, tail_label=tail, triples_factory=train_triples)\n",
    "#     # predict_rel_df = model.get_relation_prediction_df(head_label=head, tail_label=tail, triples_factory=train_triples)\n",
    "#     predict_tail_df = model.get_tail_prediction_df(head_label=head, relation_label=rel, triples_factory=train_triples)\n",
    "#     for x in predict_tail_df.values.tolist():\n",
    "#         if x[2] > 0:\n",
    "#             if '哮喘' == head:\n",
    "#                 predict_data.append([head, x[0], x[1], x[2], x[3]])\n",
    "#                 # predict_data.append([head, rel, tail, x[0], x[1], x[2], x[3]])\n",
    "#                 break\n",
    "\n",
    "# predict_df = pd.DataFrame(predict_data, columns=['head', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "# # predict_df = pd.DataFrame(predict_data, columns=['head', 'rel', 'tail', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "# predict_df.sort_values('pre_score')\n",
    "# predict_df.to_csv('test.csv')\n",
    "# predict_df.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = []\n",
    "head, rel, tail = '哮喘', 'Symptom_Disease', ''\n",
    "# predict_head_df = model.get_head_prediction_df(relation_label=rel, tail_label=tail, triples_factory=train_triples)\n",
    "# predict_rel_df = model.get_relation_prediction_df(head_label=head, tail_label=tail, triples_factory=train_triples)\n",
    "predict_tail_df = model.get_tail_prediction_df(head_label=head, relation_label=rel, triples_factory=train_triples)\n",
    "for x in predict_tail_df.values.tolist():\n",
    "    predict_data.append([head, x[0], x[1], x[2], x[3]])\n",
    "predict_df = pd.DataFrame(predict_data, columns=['head', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "# predict_df = pd.DataFrame(predict_data, columns=['head', 'rel', 'tail', 'pre_tail_id', 'pre_tail', 'pre_score', 'is_train'])\n",
    "predict_df.sort_values('pre_score')\n",
    "predict_df.to_csv('test.csv')\n",
    "predict_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e408aa74879271478431ea8f0b2a5a84dea208066a8768fe9f525a348f27606f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
