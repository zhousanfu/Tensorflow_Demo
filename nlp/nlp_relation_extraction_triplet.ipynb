{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "279ffa0a",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 版本依赖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450348a7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "bert4keras==0.7.8\n",
    "Keras==2.2.4\n",
    "numpy==1.16.4\n",
    "tensorflow-gpu==1.14.0\n",
    "tqdm==4.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a6e0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T08:15:10.482844Z",
     "start_time": "2021-10-26T08:15:10.476942Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\n",
    "from bert4keras.snippets import open\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.layers import LayerNormalization\n",
    "from bert4keras.backend import K, batch_gather\n",
    "from bert4keras.snippets import DataGenerator, sequence_padding\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "rootPath = os.path.dirname(os.getcwd())\n",
    "modelsPath = rootPath + '/models'\n",
    "dataPath = rootPath + '/data'\n",
    "print(rootPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0863b6",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b96ea5",
   "metadata": {
    "code_folding": [
     1,
     32,
     37,
     44,
     54,
     116,
     119,
     174
    ],
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReextractBertTrainHandler():\n",
    "    def __init__(self, params, Train=False):\n",
    "        self.bert_config_path = modelsPath + \"/chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "        self.bert_checkpoint_path = modelsPath + \"/chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "        self.bert_vocab_path = modelsPath + \"/chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.tokenizer = Tokenizer(self.bert_vocab_path, do_lower_case=True)\n",
    "        self.model_path = modelsPath + \"/best_model.weights\"\n",
    "        self.params_path = modelsPath + '/params.json'\n",
    "        gpu_id = params.get(\"gpu_id\", None)\n",
    "        print(\"-----> 选择的gpu_id===\", gpu_id)\n",
    "        self._set_gpu_id(gpu_id)  # 设置训练的GPU_ID\n",
    "        self.memory_fraction = params.get('memory_fraction')\n",
    "        if Train:\n",
    "            self.train_data_file_path = params.get('train_data_path')\n",
    "            self.valid_data_file_path = params.get('valid_data_path')\n",
    "            self.maxlen = params.get('maxlen', 128)\n",
    "            self.batch_size = params.get('batch_size', 32)\n",
    "            self.epoch = params.get('epoch')\n",
    "            self.data_process()\n",
    "        else:\n",
    "            load_params = json.load(open(self.params_path, encoding='utf-8'))\n",
    "            self.maxlen = load_params.get('maxlen')\n",
    "            self.num_classes = load_params.get('num_classes')\n",
    "            self.p2s_dict = load_params.get('p2s_dict')\n",
    "            self.i2p_dict = load_params.get('i2p_dict')\n",
    "            self.p2o_dict = load_params.get('p2o_dict')\n",
    "        self.build_model()\n",
    "        if not Train:\n",
    "            self.load_model()\n",
    "\n",
    "    def _set_gpu_id(self, gpu_id):\n",
    "        if gpu_id:\n",
    "            print(\"---> gpu_id:\", gpu_id, os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "    def data_process(self):\n",
    "        self.train_data, self.valid_data, self.p2s_dict, self.p2o_dict, self.i2p_dict, self.p2i_dict = data_process(\n",
    "            self.train_data_file_path, self.valid_data_file_path, self.maxlen, self.params_path)\n",
    "        self.num_classes = len(self.i2p_dict)\n",
    "        self.train_generator = Data_Generator(self.train_data, self.batch_size, self.tokenizer, self.p2i_dict,\n",
    "                                              self.maxlen)\n",
    "\n",
    "    def extrac_subject(self, inputs):\n",
    "        \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "        \"\"\"\n",
    "        output, subject_ids = inputs\n",
    "        subject_ids = K.cast(subject_ids, 'int32')\n",
    "        start = batch_gather(output, subject_ids[:, :1])\n",
    "        end = batch_gather(output, subject_ids[:, 1:])\n",
    "        subject = K.concatenate([start, end], 2)\n",
    "        return subject[:, 0]\n",
    "\n",
    "    def build_model(self):\n",
    "        import tensorflow as tf\n",
    "        from keras.backend.tensorflow_backend import set_session\n",
    "        config = tf.ConfigProto()\n",
    "        # A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        if self.memory_fraction:\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = self.memory_fraction\n",
    "            config.gpu_options.allow_growth = False\n",
    "        else:\n",
    "            print('------> no memory_fraction')\n",
    "            # 重点：设置动态分配GPU\n",
    "            config.gpu_options.allow_growth = True\n",
    "            # 设置最大占有GPU不超过显存的80%\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "        set_session(tf.Session(config=config))\n",
    "\n",
    "        # 补充输入\n",
    "        subject_labels = Input(shape=(None, 2), name='Subject-Labels')\n",
    "        subject_ids = Input(shape=(2,), name='Subject-Ids')\n",
    "        object_labels = Input(\n",
    "            shape=(None, self.num_classes, 2), name='Object-Labels')\n",
    "        # 加载预训练模型\n",
    "        bert = build_transformer_model(\n",
    "            config_path=self.bert_config_path,\n",
    "            checkpoint_path=self.bert_checkpoint_path,\n",
    "            return_keras_model=False,\n",
    "        )\n",
    "        # 预测subject\n",
    "        output = Dense(units=2,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=bert.initializer)(bert.model.output)\n",
    "        subject_preds = Lambda(lambda x: x ** 2)(output)\n",
    "        self.subject_model = Model(bert.model.inputs, subject_preds)\n",
    "        # 传入subject，预测object\n",
    "        # 通过Conditional Layer Normalization将subject融入到object的预测中\n",
    "        output = bert.model.layers[-2].get_output_at(-1)\n",
    "        subject = Lambda(self.extrac_subject)([output, subject_ids])\n",
    "        output = LayerNormalization(conditional=True)([output, subject])\n",
    "        output = Dense(units=self.num_classes * 2,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=bert.initializer)(output)\n",
    "        output = Lambda(lambda x: x ** 4)(output)\n",
    "        object_preds = Reshape((-1, self.num_classes, 2))(output)\n",
    "        self.object_model = Model(\n",
    "            bert.model.inputs + [subject_ids], object_preds)\n",
    "        # 训练模型\n",
    "        self.train_model = Model(bert.model.inputs + [subject_labels, subject_ids, object_labels],\n",
    "                                 [subject_preds, object_preds])\n",
    "        mask = bert.model.get_layer('Embedding-Token').output_mask\n",
    "        mask = K.cast(mask, K.floatx())\n",
    "        subject_loss = K.binary_crossentropy(subject_labels, subject_preds)\n",
    "        subject_loss = K.mean(subject_loss, 2)\n",
    "        subject_loss = K.sum(subject_loss * mask) / K.sum(mask)\n",
    "        object_loss = K.binary_crossentropy(object_labels, object_preds)\n",
    "        object_loss = K.sum(K.mean(object_loss, 3), 2)\n",
    "        object_loss = K.sum(object_loss * mask) / K.sum(mask)\n",
    "        self.train_model.add_loss(subject_loss + object_loss)\n",
    "        AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
    "        self.optimizer = AdamEMA(lr=1e-4)\n",
    "        self.train_model.compile(optimizer=self.optimizer)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.train_model.load_weights(self.model_path)\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        抽取输入text所包含的三元组\n",
    "        text：str(<离开>是由张宇谱曲，演唱)\n",
    "        \"\"\"\n",
    "        # print('--->', text)\n",
    "        tokens = self.tokenizer.tokenize(text, max_length=self.maxlen)\n",
    "        token_ids, segment_ids = self.tokenizer.encode(\n",
    "            text, max_length=self.maxlen)\n",
    "        # 抽取subject\n",
    "        subject_preds = self.subject_model.predict(\n",
    "            [[token_ids], [segment_ids]])\n",
    "        # print('---->', subject_preds)\n",
    "        start = np.where(subject_preds[0, :, 0] > 0.6)[0]\n",
    "        end = np.where(subject_preds[0, :, 1] > 0.5)[0]\n",
    "        subjects = []\n",
    "        for i in start:\n",
    "            j = end[end >= i]\n",
    "            if len(j) > 0:\n",
    "                j = j[0]\n",
    "                subjects.append((i, j))\n",
    "        if subjects:\n",
    "            spoes = []\n",
    "            token_ids = np.repeat([token_ids], len(subjects), 0)\n",
    "            segment_ids = np.repeat([segment_ids], len(subjects), 0)\n",
    "            subjects = np.array(subjects)\n",
    "            # 传入subject，抽取object和predicate\n",
    "            object_preds = self.object_model.predict(\n",
    "                [token_ids, segment_ids, subjects])\n",
    "            for subject, object_pred in zip(subjects, object_preds):\n",
    "                start = np.where(object_pred[:, :, 0] > 0.6)\n",
    "                end = np.where(object_pred[:, :, 1] > 0.5)\n",
    "                for _start, predicate1 in zip(*start):\n",
    "                    for _end, predicate2 in zip(*end):\n",
    "                        if _start <= _end and predicate1 == predicate2:\n",
    "                            spoes.append((subject, predicate1, (_start, _end)))\n",
    "                            break\n",
    "            i2p_values = []\n",
    "            for k, v in self.i2p_dict.items():\n",
    "                i2p_values.append(v)\n",
    "\n",
    "            return [\n",
    "                (\n",
    "                    [self.tokenizer.decode(token_ids[0, s[0]:s[1] + 1], tokens[s[0]:s[1] + 1]),\n",
    "                     self.p2s_dict[i2p_values[p]]],\n",
    "                    i2p_values[p],\n",
    "                    [self.tokenizer.decode(token_ids[0, o[0]:o[1] + 1], tokens[o[0]:o[1] + 1]),\n",
    "                     self.p2o_dict[i2p_values[p]]],\n",
    "                    (s[0], s[1] + 1),\n",
    "                    (o[0], o[1] + 1)\n",
    "                ) for s, p, o in spoes\n",
    "            ]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def train(self):\n",
    "        evaluator = Evaluator(self.train_model, self.model_path, self.tokenizer, self.predict, self.optimizer,\n",
    "                              self.valid_data)\n",
    "\n",
    "        self.train_model.fit_generator(self.train_generator.forfit(),\n",
    "                                       steps_per_epoch=len(\n",
    "                                           self.train_generator),\n",
    "                                       epochs=self.epoch,\n",
    "                                       callbacks=[evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbacc9d",
   "metadata": {
    "code_folding": [],
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598e1b4",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data_Generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size, tokenizer, p2i_dict, maxlen):\n",
    "        super().__init__(data, batch_size=batch_size)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p2i_dict = p2i_dict\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def sample(self, random=False):\n",
    "        \"\"\"采样函数，每个样本同时返回一个is_end标记\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "                        # elif len(caches) == self.buffer_size:\n",
    "                        #     isfull = True\n",
    "                    while caches:\n",
    "                        i = np.random.randint(len(caches))\n",
    "                        yield caches.pop(i)\n",
    "\n",
    "            else:\n",
    "\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "\n",
    "        d_current = next(data)\n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "\n",
    "        yield True, d_current\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            token_ids, segment_ids = self.tokenizer.encode(first_text=d['text'], max_length=self.maxlen)\n",
    "            # 整理三元组 {s: [(o_start,0_end, p)]}/{s_token_ids:[]}\n",
    "            spoes = {}\n",
    "            for spo in d['new_spo_list']:\n",
    "                s = spo['s']\n",
    "                p = spo['p']\n",
    "                o = spo['o']\n",
    "                s_token = self.tokenizer.encode(s['entity'])[0][1:-1]\n",
    "                p = self.p2i_dict[p['entity']]\n",
    "                o_token = self.tokenizer.encode(o['entity'])[0][1:-1]\n",
    "                s_idx = search(s_token, token_ids)  # s_idx s起始位置\n",
    "                o_idx = search(o_token, token_ids)  # o_idx o起始位置\n",
    "                if s_idx != -1 and o_idx != -1:\n",
    "                    s = (s_idx, s_idx + len(s_token) - 1)  # s s起始结束位置，s的类别\n",
    "                    o = (o_idx, o_idx + len(o_token) - 1, p)  # o o起始结束位置及p的id,o的类别\n",
    "                    if s not in spoes:\n",
    "                        spoes[s] = []\n",
    "                    spoes[s].append(o)\n",
    "            if spoes:\n",
    "                # subject标签，采用二维向量分别标记subject的起始位置和结束位置\n",
    "                subject_labels = np.zeros((len(token_ids), 2))\n",
    "                for s in spoes:\n",
    "                    subject_labels[s[0], 0] = 1\n",
    "                    subject_labels[s[1], 1] = 1\n",
    "                # 随机选一个subject\n",
    "                start, end = np.array(list(spoes.keys())).T\n",
    "                start = np.random.choice(start)\n",
    "                end = np.random.choice(end[end >= start])\n",
    "                subject_ids = (start, end)\n",
    "                # 对应的object标签\n",
    "                object_labels = np.zeros((len(token_ids), len(self.p2i_dict), 2))\n",
    "                for o in spoes.get(subject_ids, []):\n",
    "                    object_labels[o[0], o[2], 0] = 1\n",
    "                    object_labels[o[1], o[2], 1] = 1\n",
    "                # 构建batch\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_subject_labels.append(subject_labels)\n",
    "                batch_subject_ids.append(subject_ids)\n",
    "                batch_object_labels.append(object_labels)\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                    batch_subject_labels = sequence_padding(batch_subject_labels, padding=np.zeros(2))\n",
    "                    batch_subject_ids = np.array(batch_subject_ids)\n",
    "                    batch_object_labels = sequence_padding(batch_object_labels,\n",
    "                                                           padding=np.zeros((3, 2)))\n",
    "                    yield [\n",
    "                              batch_token_ids, batch_segment_ids,\n",
    "                              batch_subject_labels, batch_subject_ids, batch_object_labels\n",
    "\n",
    "                          ], None\n",
    "                    batch_token_ids, batch_segment_ids = [], []\n",
    "                    batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2ce34",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 评估和保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed2249",
   "metadata": {
    "code_folding": [
     0,
     23,
     33,
     90
    ],
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估和保存模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, model_path, tokenizer,predict,optimizer,valid_data):\n",
    "        self.EMAer = optimizer\n",
    "        self.best_val_f1 = 0.\n",
    "        self.model = model\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.predict = predict\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.EMAer.apply_ema_weights()\n",
    "        f1, precision, recall = evaluate(self.tokenizer,self.valid_data,self.predict)\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            self.model.save_weights(self.model_path)\n",
    "        self.EMAer.reset_old_weights()\n",
    "        print('f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f/n' %\n",
    "              (f1, precision, recall, self.best_val_f1))\n",
    "\n",
    "def search(pattern, sequence):\n",
    "    \"\"\"从sequence中寻找子串pattern\n",
    "    如果找到，返回第一个下标；否则返回-1。\n",
    "    \"\"\"\n",
    "    n = len(pattern)\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i + n] == pattern:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def evaluate(tokenizer,data,predict):\n",
    "    \"\"\"评估函数，计算f1、precision、recall\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    f = open('dev_pred.json', 'w', encoding='utf-8')\n",
    "    pbar = tqdm()\n",
    "\n",
    "    class SPO(tuple):\n",
    "        \"\"\"用来存三元组的类\n",
    "        表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，\n",
    "        使得在判断两个三元组是否等价时容错性更好。\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, spo):\n",
    "            self.spox = (\n",
    "                tuple(spo[0]),\n",
    "                spo[1],\n",
    "                tuple(spo[2]),\n",
    "            )\n",
    "\n",
    "        def __hash__(self):\n",
    "            return self.spox.__hash__()\n",
    "\n",
    "        def __eq__(self, spo):\n",
    "            return self.spox == spo.spox\n",
    "\n",
    "    for d in data:\n",
    "        R = set([SPO(spo) for spo in\n",
    "                 [[tokenizer.tokenize(spo_str[0][0]), spo_str[1], tokenizer.tokenize(spo_str[2][0])] for\n",
    "                  spo_str\n",
    "                  in predict(d['text'])]])\n",
    "        T = set([SPO(spo) for spo in\n",
    "                 [[tokenizer.tokenize(spo_str['s']['entity']), spo_str['p']['entity'],\n",
    "                   tokenizer.tokenize(spo_str['o']['entity'])] for spo_str\n",
    "                  in d['new_spo_list']]])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        pbar.update()\n",
    "        pbar.set_description('f1: %.5f, precision: %.5f, recall: %.5f' %\n",
    "                             (f1, precision, recall))\n",
    "        s = json.dumps(\n",
    "            {\n",
    "                'text': d['text'],\n",
    "                'spo_list': list(T),\n",
    "                'spo_list_pred': list(R),\n",
    "                'new': list(R - T),\n",
    "                'lack': list(T - R),\n",
    "            },\n",
    "            ensure_ascii=False,\n",
    "            indent=4)\n",
    "        f.write(s + '/n')\n",
    "    pbar.close()\n",
    "    f.close()\n",
    "    return f1, precision, recall\n",
    "\n",
    "def data_process(train_data_file_path, valid_data_file_path, max_len, params_path):\n",
    "    train_data = json.load(open(train_data_file_path, encoding='utf-8'))\n",
    "\n",
    "    if valid_data_file_path:\n",
    "        train_data_ret = train_data\n",
    "        valid_data_ret = json.load(open(valid_data_file_path, encoding='utf-8'))\n",
    "    else:\n",
    "        split = int(len(train_data) * 0.8)\n",
    "        train_data_ret, valid_data_ret = train_data[:split], train_data[split:]\n",
    "    p2s_dict = {}\n",
    "    p2o_dict = {}\n",
    "    predicate = []\n",
    "\n",
    "    for content in train_data:\n",
    "        for spo in content.get('new_spo_list'):\n",
    "            s_type = spo.get('s').get('type')\n",
    "            p_key = spo.get('p').get('entity')\n",
    "            o_type = spo.get('o').get('type')\n",
    "            if p_key not in p2s_dict:\n",
    "                p2s_dict[p_key] = s_type\n",
    "            if p_key not in p2o_dict:\n",
    "                p2o_dict[p_key] = o_type\n",
    "            if p_key not in predicate:\n",
    "                predicate.append(p_key)\n",
    "    i2p_dict = {i: key for i, key in enumerate(predicate)}\n",
    "    p2i_dict = {key: i for i, key in enumerate(predicate)}\n",
    "    save_params = {}\n",
    "    save_params['p2s_dict'] = p2s_dict\n",
    "    save_params['i2p_dict'] = i2p_dict\n",
    "    save_params['p2o_dict'] = p2o_dict\n",
    "    save_params['maxlen'] = max_len\n",
    "    save_params['num_classes'] = len(i2p_dict)\n",
    "    # 数据保存\n",
    "    json.dump(save_params,\n",
    "              open(params_path, 'w', encoding='utf-8'),\n",
    "              ensure_ascii=False, indent=4)\n",
    "    return train_data_ret, valid_data_ret, p2s_dict, p2o_dict, i2p_dict, p2i_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6d4ef",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "### 训练预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62ba9",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "params = {\n",
    "    \"gpu_id\": 0,\n",
    "    \"maxlen\": 256,\n",
    "    \"batch_size\": 40,\n",
    "    \"epoch\": 5,\n",
    "    \"train_data_path\": dataPath + \"/train_data.json\",\n",
    "    # \"valid_data_path\": dataPath + \"/valid_test.json\",\n",
    "    \"dev_data_path\": dataPath + \"/valid_data.json\",\n",
    "}\n",
    "model = ReextractBertTrainHandler(params, Train=True)\n",
    "model.train()\n",
    "text = \"胃壁、肠管壁未见明确增厚及肿块影，肠腔未见异常扩张\"\n",
    "print('-->结果：', model.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42422a-53fe-4c2f-9ebd-40967a0ed095",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"gpu_id\": 0,\n",
    "    \"maxlen\": 128,\n",
    "    \"batch_size\": 32,\n",
    "    \"epoch\": 2,\n",
    "    \"train_data_path\": dataPath + \"/train_data.json\",\n",
    "    # \"valid_data_path\": dataPath + \"/valid_test.json\",\n",
    "    \"dev_data_path\": dataPath + \"/valid_data.json\",\n",
    "}\n",
    "model = ReextractBertTrainHandler(params, Train=True)\n",
    "text = \"胃壁、肠管壁未见明确增厚及肿块影，肠腔未见异常扩张\"\n",
    "print('-->结果：', model.predict(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac88aac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 外部数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a3e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-26T08:46:39.319284Z",
     "start_time": "2021-10-26T08:46:39.131100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ann_read(file_dir):\n",
    "    rules = []\n",
    "    lines = \"\"\n",
    "    with open(file_dir, 'r', encoding='utf-8')as f:\n",
    "        lines += f.read()\n",
    "    f.close()\n",
    "\n",
    "    lines_l = lines.split('\\n')\n",
    "    for i in lines_l:\n",
    "        line = i.split('\\t')\n",
    "        try:\n",
    "            rules.append(line[2].replace('\\d', '') + ',' + line[1].split(' ')[0])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return rules\n",
    "\n",
    "\n",
    "ann_rules = []\n",
    "for dirpath, dirnames, filenames in os.walk(rootPath):\n",
    "    for file in filenames:\n",
    "        if '.ann' in file:\n",
    "            rules = ann_read(dirpath + '/' + file)\n",
    "            ann_rules.append(rules)\n",
    "            \n",
    "with open(rootPath+'/data/ann_rules.txt', 'w') as fw:\n",
    "    for x in ann_rules:\n",
    "        for y in x:\n",
    "            fw.write(y + '\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f282b4b-3bab-4b17-a4cf-65b2dd81ef63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spo_to_triplet(spo_txt):\n",
    "    triplet_dict = {}\n",
    "    triplet_dict['new_spo_list'] = []\n",
    "    for k,v in json.loads(spo_txt).items():\n",
    "        if k == 'text':\n",
    "            triplet_dict['text'] = v\n",
    "        elif k == 'spo_list':\n",
    "            for spo_list in v:\n",
    "                new_spo_ld = {}\n",
    "                new_s = {}\n",
    "                new_p = {}\n",
    "                new_o = {}\n",
    "                for k_s,v_s in spo_list.items():\n",
    "                    if k_s == 'subject':\n",
    "                        new_s['entity'] = v_s\n",
    "                    elif k_s == 'subject_type':\n",
    "                        new_s['type'] = v_s\n",
    "                    elif k_s == 'predicate':\n",
    "                        new_p['entity'] = v_s\n",
    "                        new_p['type'] = '_rel'\n",
    "                    elif k_s == 'object':\n",
    "                        new_o['entity'] = v_s['@value']\n",
    "                    elif k_s == 'object_type':\n",
    "                        new_o['type'] = v_s['@value']\n",
    "                new_spo_ld['s'] = new_s\n",
    "                new_spo_ld['p'] = new_p\n",
    "                new_spo_ld['o'] = new_o\n",
    "                triplet_dict['new_spo_list'] = [new_spo_ld]\n",
    "    triplet_dict = json.dumps(triplet_dict, ensure_ascii=False)\n",
    "    return triplet_dict\n",
    "\n",
    "sop_line = []\n",
    "with open(rootPath+'/data/CMeIE/CMeIE_dev.json', 'r')as f_sop:\n",
    "    sop_line = f_sop.read().split('\\n')\n",
    "    \n",
    "triplet_dict = spo_to_triplet(sop_line[:10])\n",
    "print('-->', type(triplet_dict), triplet_dict)\n",
    "\n",
    "with open(rootPath+'/data/valid_data.json', 'w') as fw:\n",
    "    for i in sop_line:\n",
    "        try:\n",
    "            fw.write(spo_to_triplet(i) + ',')\n",
    "        except:\n",
    "            pass\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3d14a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sop_line = []\n",
    "with open(rootPath+'/data/nlp_relation_extraction_spo三元组/train_data.json', 'r')as f_sop:\n",
    "    sop_line = f_sop.read().split('\\n')\n",
    "print(sop_line[0][:1000], '\\n')\n",
    "sop_line = []\n",
    "with open(rootPath+'/data/nlp_relation_extraction_spo三元组/valid_data.json', 'r')as f_sop:\n",
    "    sop_line = f_sop.read().split('\\n')\n",
    "print(sop_line[0][:1000], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3bca79-f400-4e36-8460-52748b48b2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
