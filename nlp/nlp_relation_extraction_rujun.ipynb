{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2f09b1-6905-4fcc-b45a-ceb4727523fd",
   "metadata": {
    "id": "2a2f09b1-6905-4fcc-b45a-ceb4727523fd"
   },
   "source": [
    "# 一、pil关系抽取\n",
    "实体 - 关系\n",
    "ruijin_competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8682f70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8682f70",
    "outputId": "1547099d-0b45-4f6e-cb84-58cee989739e"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/astronstar/ruijin_competition.git\n",
    "import os,sys\n",
    "\n",
    "base_path = os.path.dirname(os.getcwd()) #os.getcwd()\n",
    "data_path = base_path + '/data'\n",
    "train_data_dir = data_path + '/ruijin_competition/phase2/Demo/DataSets/ruijin_round2_train/ruijin_round2_train'\n",
    "test_a_data_dir = data_path + '/ruijin_competition/phase2/Demo/DataSets/ruijin_round2_test_a/ruijin_round2_test_a'\n",
    "test_b_data_dir = data_path + '/ruijin_competition/phase2/Demo/DataSets/ruijin_round2_test_b/ruijin_round2_test_b'\n",
    "# train_data_dir = data_path + '/my_rujin_train'\n",
    "train_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4e705-e830-4f04-9157-45777ae1ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d0e54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "id": "4d0d0e54",
    "outputId": "867bb191-da3b-49ba-de42-4e3080bbde19"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall keras tensorflow\n",
    "# !pip install keras==2.2.4 tensorflow==1.13.1 h5py==1.5.2 sklearn\n",
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import permutations, chain\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from itertools import groupby\n",
    "from spacy import displacy\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Concatenate, Dense\n",
    "from keras.layers import Conv1D, MaxPool1D, Flatten\n",
    "from keras.layers import Input, LSTM, Embedding, Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.layers.crf import CRF, crf_loss, crf_viterbi_accuracy\n",
    "from keras_contrib.utils import save_load_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bgNE3pfai01o",
   "metadata": {
    "id": "bgNE3pfai01o"
   },
   "outputs": [],
   "source": [
    "ENTITIES = [\n",
    "    \"Amount\", \"Anatomy\", \"Disease\", \"Drug\",\n",
    "    \"Duration\", \"Frequency\", \"Level\", \"Method\",\n",
    "    \"Operation\", \"Reason\", \"SideEff\", \"Symptom\",\n",
    "    \"Test\", \"Test_Value\", \"Treatment\", \"Location\"\n",
    "]\n",
    "\n",
    "RELATIONS = [\n",
    "    \"Test_Disease\",\"Symptom_Disease\",\"Treatment_Disease\",\n",
    "    \"Drug_Disease\",\"Anatomy_Disease\",\"Frequency_Drug\",\n",
    "    \"Duration_Drug\",\"Amount_Drug\",\"Method_Drug\",\"SideEff-Drug\",\n",
    "    \"Test_TestValue\",\"Anatomy_Symptom\",\"Symptom_Duration\",\"Disease_Duration\"\n",
    "]\n",
    "\n",
    "COLORS = [\n",
    "    '#7aecec','#bfeeb7','#feca74','#ff9561',\n",
    "    '#aa9cfc','#c887fb','#9cc9cc','#ffeb80',\n",
    "    '#ff8197','#ff8197','#f0d0ff','#bfe1d9',\n",
    "    '#bfe1d9','#e4e7d2','#e4e7d2','#e4e7d2',\n",
    "    '#e4e7d2','#e4e7d2','#e4e7d2'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c4745",
   "metadata": {
    "id": "992c4745"
   },
   "source": [
    "## 1.1 utils&evalu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TYNNhoQVi9Z_",
   "metadata": {
    "id": "TYNNhoQVi9Z_",
    "tags": []
   },
   "source": [
    "### ent_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ff9ab",
   "metadata": {
    "id": "8c4ff9ab"
   },
   "outputs": [],
   "source": [
    "COLOR_MAP = dict(zip([ent.upper() for ent in ENTITIES], COLORS[:len(ENTITIES)]))\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, doc_id, offset, text, ents):\n",
    "        self.text = text\n",
    "        self.doc_id = doc_id\n",
    "        self.offset = offset\n",
    "        self.ents = ents\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.text\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.offset > other.offset\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.text[key]\n",
    "        if isinstance(key, slice):\n",
    "            text = self.text[key]\n",
    "            start = key.start or 0\n",
    "            stop = key.stop or len(self.text)\n",
    "            if start < 0:\n",
    "                start += len(self.text)\n",
    "            if stop < 0:\n",
    "                stop += len(self.text)\n",
    "\n",
    "            ents = self.ents.find_entities(start, stop).offset(-start)\n",
    "            offset = self.offset + start\n",
    "            return Sentence(self.doc_id, offset, text, ents)\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        ents = []\n",
    "        for ent in self.ents:\n",
    "            ents.append({'start': ent.start_pos,\n",
    "                         'end': ent.end_pos,\n",
    "                         'label': ent.category})\n",
    "        ex = {'text': self.text, 'ents': ents, 'title': None, 'settings': {}}\n",
    "        return displacy.render(ex,\n",
    "                               style='ent',\n",
    "                               options={'colors': COLOR_MAP},\n",
    "                               manual=True,\n",
    "                               minify=True)\n",
    "\n",
    "class Entity(object):\n",
    "    def __init__(self, ent_id, category, start_pos, end_pos, text):\n",
    "        self.ent_id = ent_id\n",
    "        self.category = category\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.text = text\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.start_pos > other.start_pos\n",
    "\n",
    "    def offset(self, offset_val):\n",
    "        return Entity(self.ent_id,\n",
    "                      self.category,\n",
    "                      self.start_pos + offset_val,\n",
    "                      self.end_pos + offset_val,\n",
    "                      self.text)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '({}, {}, ({}, {}), {})'.format(self.ent_id,\n",
    "                                               self.category,\n",
    "                                               self.start_pos,\n",
    "                                               self.end_pos,\n",
    "                                               self.text)\n",
    "\n",
    "class Entities(object):\n",
    "    def __init__(self, ents):\n",
    "        self.ents = sorted(ents)\n",
    "        self.ent_dict = dict(zip([ent.ent_id for ent in ents], ents))\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int) or isinstance(key, slice):\n",
    "            return self.ents[key]\n",
    "        else:\n",
    "            return self.ent_dict.get(key, None)\n",
    "\n",
    "    def offset(self, offset_val):\n",
    "        ents = [ent.offset(offset_val) for ent in self.ents]\n",
    "        return Entities(ents)\n",
    "\n",
    "    def vectorize(self, vec_len, cate2idx):\n",
    "        res_vec = np.zeros(vec_len, dtype=int)\n",
    "        for ent in self.ents:\n",
    "            res_vec[ent.start_pos: ent.end_pos] = cate2idx[ent.category]\n",
    "        return res_vec\n",
    "\n",
    "    def find_entities(self, start_pos, end_pos):\n",
    "        res = []\n",
    "        for ent in self.ents:\n",
    "            if ent.start_pos > end_pos:\n",
    "                break\n",
    "            sp, ep = (max(start_pos, ent.start_pos), min(end_pos, ent.end_pos))\n",
    "            if ep > sp:\n",
    "                new_ent = Entity(ent.ent_id, ent.category, sp, ep, ent.text[:(ep - sp)])\n",
    "                res.append(new_ent)\n",
    "        return Entities(res)\n",
    "\n",
    "    def merge(self):\n",
    "        merged_ents = []\n",
    "        for ent in self.ents:\n",
    "            if len(merged_ents) == 0:\n",
    "                merged_ents.append(ent)\n",
    "            elif (merged_ents[-1].end_pos == ent.start_pos and\n",
    "                  merged_ents[-1].category == ent.category):\n",
    "                merged_ent = Entity(ent_id=merged_ents[-1].ent_id,\n",
    "                                    category=ent.category,\n",
    "                                    start_pos=merged_ents[-1].start_pos,\n",
    "                                    end_pos=ent.end_pos,\n",
    "                                    text=merged_ents[-1].text + ent.text)\n",
    "                merged_ents[-1] = merged_ent\n",
    "            else:\n",
    "                merged_ents.append(ent)\n",
    "        return Entities(merged_ents)\n",
    "\n",
    "class Document(object):\n",
    "    def __init__(self, doc_id, text, ents=[]):\n",
    "        self.doc_id = doc_id\n",
    "        self.text = text\n",
    "        self.ents = ents\n",
    "        self.sents = self.extract_sentences()\n",
    "\n",
    "    def extract_sentences(self):\n",
    "        offset = 0\n",
    "        ent_iter = iter(self.ents)\n",
    "        ent = next(ent_iter, None)\n",
    "        sents = []\n",
    "        for text in self.text.split('。'):\n",
    "            sent_ents = []\n",
    "            while (ent is not None and\n",
    "                   ent.start_pos >= offset and\n",
    "                   ent.end_pos <= offset + len(text)):\n",
    "                sent_ents.append(ent.offset(-offset))\n",
    "                ent = next(ent_iter, None)\n",
    "            sent = Sentence(self.doc_id, offset, text, sent_ents)\n",
    "            sents.append(sent)\n",
    "            offset += len(text) + 1\n",
    "        return sents\n",
    "\n",
    "    def pad(self, pad_left=0, pad_right=0, pad_val=\" \"):\n",
    "        text = pad_left * pad_val + self.text + pad_right * pad_val\n",
    "        ents = self.ents.offset(pad_left)\n",
    "        return Document(self.doc_id, text, ents)\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        sent = Sentence(self.doc_id, offset=0, text=self.text, ents=self.ents)\n",
    "        return sent._repr_html_()\n",
    "\n",
    "class Documents(object):\n",
    "    def __init__(self, data_dir, doc_ids=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.doc_ids = doc_ids\n",
    "        if self.doc_ids is None:\n",
    "            self.doc_ids = self.scan_doc_ids()\n",
    "\n",
    "    def scan_doc_ids(self):\n",
    "        doc_ids = [fname.split('.')[0] for fname in os.listdir(self.data_dir)]\n",
    "        return np.unique(doc_ids)\n",
    "\n",
    "    def read_txt_file(self, doc_id):\n",
    "        fname = os.path.join(self.data_dir, doc_id + '.txt')\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "\n",
    "    def parse_entity_line(self, raw_str):\n",
    "        ent_id, label, text = raw_str.strip().split('\\t')\n",
    "        category, pos = label.split(' ', 1)\n",
    "        pos = pos.split(' ')\n",
    "        ent = Entity(ent_id, category, int(pos[0]), int(pos[-1]), text)\n",
    "        return ent\n",
    "\n",
    "    def read_anno_file(self, doc_id):\n",
    "        ents = []\n",
    "        fname = os.path.join(self.data_dir, doc_id + '.ann')\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith('T'):\n",
    "                ent = self.parse_entity_line(line)\n",
    "                ents.append(ent)\n",
    "        ents = Entities(ents)\n",
    "\n",
    "        return ents\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def get_doc(self, doc_id):\n",
    "        text = self.read_txt_file(doc_id)\n",
    "        ents = self.read_anno_file(doc_id)\n",
    "        doc = Document(doc_id, text, ents)\n",
    "        return doc\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            doc_id = self.doc_ids[key]\n",
    "            return self.get_doc(doc_id)\n",
    "        if isinstance(key, str):\n",
    "            doc_id = key\n",
    "            return self.get_doc(doc_id)\n",
    "        if isinstance(key, np.ndarray) and key.dtype == int:\n",
    "            doc_ids = self.doc_ids[key]\n",
    "            return Documents(self.data_dir, doc_ids=doc_ids)\n",
    "\n",
    "class SentenceExtractor(object):\n",
    "    def __init__(self, window_size=50, pad_size=10):\n",
    "        self.window_size = window_size\n",
    "        self.pad_size = pad_size\n",
    "\n",
    "    def extract_doc(self, doc):\n",
    "        num_sents = math.ceil(len(doc.text) / self.window_size)\n",
    "        doc = doc.pad(pad_left=self.pad_size, pad_right=num_sents * self.window_size - len(doc.text) + self.pad_size)\n",
    "        sents = []\n",
    "        for cur_idx in range(self.pad_size, len(doc.text) - self.pad_size, self.window_size):\n",
    "            sent_text = doc.text[cur_idx - self.pad_size: cur_idx + self.window_size + self.pad_size]\n",
    "            ents = []\n",
    "            for ent in doc.ents.find_entities(start_pos=cur_idx - self.pad_size,\n",
    "                                              end_pos=cur_idx + self.window_size + self.pad_size):\n",
    "                ents.append(ent.offset(-cur_idx + self.pad_size))\n",
    "            sent = Sentence(doc.doc_id,\n",
    "                            offset=cur_idx - 2 * self.pad_size,\n",
    "                            text=sent_text,\n",
    "                            ents=Entities(ents))\n",
    "            sents.append(sent)\n",
    "        return sents\n",
    "\n",
    "    def __call__(self, docs):\n",
    "        sents = []\n",
    "        for doc in docs:\n",
    "            sents += self.extract_doc(doc)\n",
    "        return sents\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, sentences, word2idx=None, cate2idx=None):\n",
    "        self.sentences = sentences\n",
    "        self.word2idx = word2idx\n",
    "        self.cate2idx = cate2idx\n",
    "\n",
    "    def build_vocab_dict(self, vocab_size=2000):\n",
    "        counter = Counter()\n",
    "        for sent in self.sentences:\n",
    "            for char in sent.text:\n",
    "                counter[char] += 1\n",
    "        word2idx = dict()\n",
    "        word2idx['<unk>'] = 0\n",
    "        if vocab_size > 0:\n",
    "            num_most_common = vocab_size - len(word2idx)\n",
    "        else:\n",
    "            num_most_common = len(counter)\n",
    "        for char, _ in counter.most_common(num_most_common):\n",
    "            word2idx[char] = word2idx.get(char, len(word2idx))\n",
    "        self.word2idx = word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent_vec, labels_vec = [], []\n",
    "        sents = self.sentences[idx]\n",
    "        if not isinstance(sents, list):\n",
    "            sents = [sents]\n",
    "        for sent in sents:\n",
    "            content = [self.word2idx.get(c, 1) for c in sent.text]\n",
    "            sent_vec.append(content)\n",
    "            labels_vec.append(sent.ents.vectorize(vec_len=len(sent.text), cate2idx=self.cate2idx))\n",
    "        return np.array(sent_vec), np.expand_dims(np.array(labels_vec), axis=-1)\n",
    "\n",
    "def make_sentence_prediction(pred, sent, idx2ent):\n",
    "    ents_vec = np.argmax(pred, axis=1)\n",
    "    ents = []\n",
    "    cur_idx = 0\n",
    "    for label_idx, group in groupby(ents_vec):\n",
    "        group = list(group)\n",
    "        start_pos = cur_idx\n",
    "        end_pos = start_pos + len(group)\n",
    "        if label_idx > 0:\n",
    "            text = sent.text[start_pos: end_pos]\n",
    "            category = idx2ent[label_idx]\n",
    "            ent = Entity(None, category, start_pos, end_pos, text)\n",
    "            ents.append(ent)\n",
    "        cur_idx = end_pos\n",
    "    return Sentence(sent.doc_id, sent.offset, sent.text, Entities(ents))\n",
    "\n",
    "def make_doc_prediction(doc_id, sents, docs):\n",
    "    sents = sorted(sents)\n",
    "    ents = []\n",
    "    for sent in sents:\n",
    "        ents += sent.ents.offset(sent.offset)\n",
    "    ents = Entities(ents).merge()\n",
    "\n",
    "    for idx, ent in enumerate(ents):\n",
    "        ent.ent_id = 'T{}'.format(idx + 1)\n",
    "    doc = Document(doc_id, docs[doc_id].text, ents=ents)\n",
    "    return doc\n",
    "\n",
    "def make_predictions(preds, dataset, sent_pad, docs, idx2ent):\n",
    "    pred_sents = []\n",
    "    for sent, pred in zip(dataset.sentences, preds):\n",
    "        pred_sent = make_sentence_prediction(pred, sent, idx2ent)\n",
    "        pred_sent = pred_sent[sent_pad: -sent_pad]\n",
    "        pred_sents.append(pred_sent)\n",
    "\n",
    "    docs_sents = defaultdict(list)\n",
    "    for sent in pred_sents:\n",
    "        docs_sents[sent.doc_id].append(sent)\n",
    "\n",
    "    pred_docs = dict()\n",
    "    for doc_id, sentences in docs_sents.items():\n",
    "        doc = make_doc_prediction(doc_id, sentences, docs)\n",
    "        pred_docs[doc_id] = doc\n",
    "\n",
    "    return pred_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368Gs3sWHpzN",
   "metadata": {
    "id": "368Gs3sWHpzN",
    "tags": []
   },
   "source": [
    "### ent_evalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vy92YZW6HtKy",
   "metadata": {
    "id": "vy92YZW6HtKy"
   },
   "outputs": [],
   "source": [
    "class Evaluator(object):\n",
    "    @staticmethod\n",
    "    def check_match(ent_a, ent_b):\n",
    "        return (ent_a.category == ent_b.category and\n",
    "                max(ent_a.start_pos, ent_b.start_pos) < min(ent_a.end_pos, ent_b.end_pos))\n",
    "\n",
    "    @staticmethod\n",
    "    def count_intersects(ent_list_a, ent_list_b):\n",
    "        num_hits = 0\n",
    "        ent_list_b = ent_list_b.copy()\n",
    "        for ent_a in ent_list_a:\n",
    "            hit_ent = None\n",
    "            for ent_b in ent_list_b:\n",
    "                if Evaluator.check_match(ent_a, ent_b):\n",
    "                    hit_ent = ent_b\n",
    "                    break\n",
    "            if hit_ent is not None:\n",
    "                num_hits += 1\n",
    "                ent_list_b.remove(hit_ent)\n",
    "        return num_hits\n",
    "\n",
    "    @staticmethod\n",
    "    def f1_score(gt_docs, pred_docs):\n",
    "        num_hits = 0\n",
    "        num_preds = 0\n",
    "        num_gts = 0\n",
    "        for doc_id in gt_docs.doc_ids:\n",
    "            gt_ents = gt_docs[doc_id].ents.ents\n",
    "            pred_ents = pred_docs[doc_id].ents.ents\n",
    "            num_gts += len(gt_ents)\n",
    "            num_preds += len(pred_ents)\n",
    "            num_hits += Evaluator.count_intersects(pred_ents, gt_ents)\n",
    "        p = num_hits / num_preds\n",
    "        r = num_hits / num_gts\n",
    "        f = 2 * p * r / (p + r)\n",
    "        return f, p, r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok-ehkq4i-F7",
   "metadata": {
    "id": "ok-ehkq4i-F7",
    "tags": []
   },
   "source": [
    "### re_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_YBufwRSi-Oo",
   "metadata": {
    "id": "_YBufwRSi-Oo"
   },
   "outputs": [],
   "source": [
    "class Entity(object):\n",
    "    def __init__(self, ent_id, category, start_pos, end_pos, text):\n",
    "        self.ent_id = ent_id\n",
    "        self.category = category\n",
    "        self.start_pos = start_pos\n",
    "        self.end_pos = end_pos\n",
    "        self.text = text\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.start_pos > other.start_pos\n",
    "    \n",
    "    def offset(self, offset_val):\n",
    "        return Entity(self.ent_id, \n",
    "                      self.category, \n",
    "                      self.start_pos + offset_val,\n",
    "                      self.end_pos + offset_val,\n",
    "                      self.text)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        fmt = '({ent_id}, {category}, ({start_pos}, {end_pos}), {text})'\n",
    "        return fmt.format(**self.__dict__)\n",
    "\n",
    "class Entities(object):\n",
    "    def __init__(self, ents):\n",
    "        self.ents = sorted(ents)\n",
    "        self.ent_dict = dict(zip([ent.ent_id for ent in ents], ents))\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int) or isinstance(key, slice):\n",
    "            return self.ents[key]\n",
    "        else:\n",
    "            return self.ent_dict.get(key, None)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ents)\n",
    "    \n",
    "    def offset(self, offset_val):\n",
    "        ents = [ent.offset(offset_val) for ent in self.ents]\n",
    "        return Entities(ents)\n",
    "    \n",
    "    def vectorize(self, vec_len, cate2idx):\n",
    "        res_vec = np.zeros(vec_len, dtype=int)\n",
    "        for ent in self.ents:\n",
    "            res_vec[ent.start_pos: ent.end_pos] = cate2idx[ent.category]\n",
    "        return res_vec\n",
    "    \n",
    "    def find_entities(self, start_pos, end_pos):\n",
    "        res = []\n",
    "        for ent in self.ents:\n",
    "            if ent.start_pos > end_pos:\n",
    "                break\n",
    "            sp, ep = (max(start_pos, ent.start_pos), min(end_pos, ent.end_pos))\n",
    "            if ep > sp:\n",
    "                new_ent = Entity(ent.ent_id, ent.category, sp, ep, ent.text[:(ep - sp)])\n",
    "                res.append(new_ent)\n",
    "        return Entities(res)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        ents = self.ents + other.ents\n",
    "        return Entities(ents)\n",
    "    \n",
    "    def merge(self):\n",
    "        merged_ents = []\n",
    "        for ent in self.ents:\n",
    "            if len(merged_ents) == 0:\n",
    "                merged_ents.append(ent)\n",
    "            elif (merged_ents[-1].end_pos == ent.start_pos and \n",
    "                  merged_ents[-1].category == ent.category):\n",
    "                merged_ent = Entity(ent_id=merged_ents[-1].ent_id, \n",
    "                                    category=ent.category,\n",
    "                                    start_pos=merged_ents[-1].start_pos, \n",
    "                                    end_pos=ent.end_pos, \n",
    "                                    text=merged_ents[-1].text + ent.text)\n",
    "                merged_ents[-1] = merged_ent\n",
    "            else:\n",
    "                merged_ents.append(ent)\n",
    "        return Entities(merged_ents)\n",
    "\n",
    "class Relation(object):\n",
    "    def __init__(self, rel_id, category, ent1, ent2):\n",
    "        self.rel_id = rel_id\n",
    "        self.category = category\n",
    "        self.ent1 = ent1\n",
    "        self.ent2 = ent2\n",
    "            \n",
    "    @property\n",
    "    def is_valid(self):\n",
    "        return (isinstance(self.ent1, Entity) and\n",
    "                isinstance(self.ent2, Entity) and\n",
    "                [self.ent1.category, self.ent2.category] == re.split('[-_]', self.category))\n",
    "    \n",
    "    @property\n",
    "    def start_pos(self):\n",
    "        return min(self.ent1.start_pos, self.ent2.start_pos)\n",
    "    \n",
    "    @property\n",
    "    def end_pos(self):\n",
    "        return max(self.ent1.end_pos, self.ent2.end_pos)\n",
    "    \n",
    "    def offset(self, offset_val):\n",
    "        return Relation(self.rel_id, \n",
    "                        self.category, \n",
    "                        self.ent1.offset(offset_val),\n",
    "                        self.ent2.offset(offset_val))\n",
    "    \n",
    "    def __gt__(self, other_rel):\n",
    "        return self.ent1.start_pos > other_rel.ent1.start_pos\n",
    "    \n",
    "    def __repr__(self):\n",
    "        fmt = '({rel_id}, {category} Arg1:{ent1} Arg2:{ent2})'\n",
    "        return fmt.format(**self.__dict__)\n",
    "\n",
    "class Relations(object):\n",
    "    def __init__(self, rels):\n",
    "        self.rels = rels\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            return self.rels[key]\n",
    "        elif isinstance(key, slice):\n",
    "            return Relations(self.rels[key])\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        rels = self.rels + other.rels\n",
    "        return Relations(rels)\n",
    "    \n",
    "    def find_relations(self, start_pos, end_pos):\n",
    "        res = []\n",
    "        for rel in self.rels:\n",
    "            if start_pos <= rel.start_pos and end_pos >= rel.end_pos:\n",
    "                res.append(rel)\n",
    "        return Relations(res)\n",
    "    \n",
    "    def offset(self, offset_val): \n",
    "        return Relations([rel.offset(offset_val) for rel in self.rels])\n",
    "    \n",
    "    @property\n",
    "    def start_pos(self):\n",
    "        return min([rel.start_pos for rel in self.rels])\n",
    "\n",
    "    @property\n",
    "    def end_pos(self):\n",
    "        return max([rel.end_pos for rel in self.rels])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.rels.__repr__()\n",
    "\n",
    "class TextSpan(object):\n",
    "    def __init__(self, text, ents, rels, **kwargs):\n",
    "        self.text = text\n",
    "        self.ents = ents\n",
    "        self.rels = rels\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            start, stop = key, key + 1\n",
    "        elif isinstance(key, slice):\n",
    "            start = key.start if key.start is not None else 0\n",
    "            stop = key.stop if key.stop is not None else len(self.text)\n",
    "        else:\n",
    "            raise ValueError('parameter should be int or slice')\n",
    "        if start < 0:\n",
    "            start += len(self.text)\n",
    "        if stop < 0:\n",
    "            stop += len(self.text)\n",
    "        text = self.text[key]\n",
    "        ents = self.ents.find_entities(start, stop).offset(-start)\n",
    "        rels = self.rels.find_relations(start, stop).offset(-start)\n",
    "        return TextSpan(text, ents, rels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "class Sentence(object):\n",
    "    def __init__(self, doc_id, offset, text='', ents=[], rels=[], textspan=None):\n",
    "        self.doc_id = doc_id\n",
    "        self.offset = offset\n",
    "        if isinstance(textspan, TextSpan):\n",
    "            self.textspan = textspan\n",
    "        else:\n",
    "            self.textspan = TextSpan(text, ents, rels)\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.textspan.text\n",
    "    \n",
    "    @property\n",
    "    def ents(self):\n",
    "        return self.textspan.ents\n",
    "    \n",
    "    @property\n",
    "    def rels(self):\n",
    "        return self.textspan.rels\n",
    "    \n",
    "    def abbreviate(self, max_len, ellipse_chars='$$'):\n",
    "        if max_len <= len(ellipse_chars):\n",
    "            return ''\n",
    "        left_trim = (max_len - len(ellipse_chars)) // 2\n",
    "        right_trim = max_len - len(ellipse_chars) - left_trim\n",
    "        return self[:left_trim] + ellipse_chars + self[-right_trim:]\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            start, stop = key, key + 1\n",
    "        elif isinstance(key, slice):\n",
    "            start = key.start if key.start is not None else 0\n",
    "            stop = key.stop if key.stop is not None else len(self.text)\n",
    "        else:\n",
    "            raise ValueError('parameter should be int or slice')\n",
    "        if start < 0:\n",
    "            start += len(self.text)\n",
    "        if stop < 0:\n",
    "            stop += len(self.text)\n",
    "        offset = self.offset + start\n",
    "        textspan = self.textspan[start: stop]\n",
    "        return Sentence(self.doc_id, offset, textspan=textspan)\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.offset > other.offse\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, str):\n",
    "            return Sentence(doc_id=self.doc_id, offset=self.offset, text=self.text + other, \n",
    "                            ents=self.ents, rels=self.rels)\n",
    "        assert self.doc_id == other.doc_id, 'sentences should be from the same document'\n",
    "        assert self.offset + len(self) <= other.offset, 'sentences should not have overlap'\n",
    "        doc_id = self.doc_id\n",
    "        text = self.text + other.text\n",
    "        offset = self.offset\n",
    "        ents = self.ents + other.ents.offset(len(self.text))\n",
    "        rels = self.rels + other.rels.offset(len(self.text))\n",
    "        return Sentence(doc_id=doc_id, offset=offset, text=text, ents=ents, rels=rels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.textspan)\n",
    "\n",
    "class Document(object):\n",
    "    def __init__(self, doc_id, text, ents, rels):\n",
    "        self.doc_id = doc_id\n",
    "        self.textspan = TextSpan(text, ents, rels)\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.textspan.text\n",
    "    \n",
    "    @property\n",
    "    def ents(self):\n",
    "        return self.textspan.ents\n",
    "    \n",
    "    @property\n",
    "    def rels(self):\n",
    "        return self.textspan.rels\n",
    "\n",
    "class Documents(object):\n",
    "    def __init__(self, data_dir, doc_ids=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.doc_ids = doc_ids\n",
    "        if self.doc_ids is None:\n",
    "            self.doc_ids = self.scan_doc_ids()\n",
    "    \n",
    "    def scan_doc_ids(self):\n",
    "        doc_ids = [fname.split('.')[0] for fname in os.listdir(self.data_dir)]\n",
    "        doc_ids = [doc_id for doc_id in doc_ids if len(doc_id) > 0]\n",
    "        return np.unique(doc_ids)\n",
    "    \n",
    "    def read_txt_file(self, doc_id):\n",
    "        fname = os.path.join(self.data_dir, doc_id + '.txt')\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    \n",
    "    def parse_entity_line(self, raw_str):\n",
    "        ent_id, label, text = raw_str.strip().split('\\t')\n",
    "        category, pos = label.split(' ', 1)\n",
    "        pos = pos.split(' ')\n",
    "        ent = Entity(ent_id, category, int(pos[0]), int(pos[-1]), text)\n",
    "        return ent\n",
    "    \n",
    "    def parse_relation_line(self, raw_str, ents):\n",
    "        rel_id, label = raw_str.strip().split('\\t')\n",
    "        category, arg1, arg2 = label.split(' ')\n",
    "        arg1 = arg1.split(':')[1]\n",
    "        arg2 = arg2.split(':')[1]\n",
    "        ent1 = ents[arg1]\n",
    "        ent2 = ents[arg2]\n",
    "        return Relation(rel_id, category, ent1, ent2)\n",
    "    \n",
    "    def read_anno_file(self, doc_id):\n",
    "        ents = []\n",
    "        rels = []\n",
    "        fname = os.path.join(self.data_dir, doc_id + '.ann')\n",
    "        with open(fname, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('T'):\n",
    "                ent = self.parse_entity_line(line)\n",
    "                ents.append(ent)\n",
    "        ents = Entities(ents)\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith('R'):\n",
    "                rel = self.parse_relation_line(line, ents)\n",
    "                if rel.is_valid:\n",
    "                    rels.append(rel)\n",
    "        rels = Relations(rels)\n",
    "        return ents, rels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doc_ids)\n",
    "    \n",
    "    def get_doc(self, doc_id):\n",
    "        text = self.read_txt_file(doc_id)\n",
    "        ents, rels = self.read_anno_file(doc_id)\n",
    "        doc = Document(doc_id, text, ents, rels)\n",
    "        return doc\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, int):\n",
    "            doc_id = self.doc_ids[key]\n",
    "            return self.get_doc(doc_id)\n",
    "        if isinstance(key, str):\n",
    "            doc_id = key\n",
    "            return self.get_doc(doc_id)\n",
    "        if isinstance(key, np.ndarray) and key.dtype == int:\n",
    "            doc_ids = self.doc_ids[key]\n",
    "            return Documents(self.data_dir, doc_ids=doc_ids)\n",
    "\n",
    "class SentenceExtractor(object):\n",
    "    def __init__(self, sent_split_char, window_size, rel_types, filter_no_rel_candidates_sents=True):\n",
    "        self.sent_split_char = sent_split_char\n",
    "        self.window_size = window_size\n",
    "        self.filter_no_rel_candidates_sents = filter_no_rel_candidates_sents\n",
    "        self.rels_type_set = set()\n",
    "        for rel_type in rel_types:\n",
    "            self.rels_type_set.add(tuple(re.split('[-_]', rel_type)))\n",
    "        \n",
    "    def get_sent_boundaries(self, text):\n",
    "        dot_indices = []\n",
    "        for i, ch in enumerate(text):\n",
    "            if ch == self.sent_split_char:\n",
    "                dot_indices.append(i + 1)\n",
    "        \n",
    "        if len(dot_indices) <= self.window_size - 1:\n",
    "            return [(0, len(text))]\n",
    "        \n",
    "        dot_indices = [0] + dot_indices\n",
    "        if text[-1] != self.sent_split_char:\n",
    "            dot_indices += [len(text)]\n",
    "\n",
    "        boundries = []\n",
    "        for i in range(len(dot_indices) - self.window_size):\n",
    "            start_stop = (\n",
    "                dot_indices[i],\n",
    "                dot_indices[i + self.window_size]\n",
    "            )\n",
    "            boundries.append(start_stop)\n",
    "        return boundries\n",
    "            \n",
    "    def has_rels_candidates(self, ents):\n",
    "        ent_cates = set([ent.category for ent in ents])\n",
    "        for pos_rel in permutations(ent_cates, 2):\n",
    "            if pos_rel in self.rels_type_set:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def extract_doc(self, doc):\n",
    "        sents = []\n",
    "        for start_pos, end_pos in self.get_sent_boundaries(doc.text):\n",
    "            ents = []\n",
    "            sent_text = doc.text[start_pos: end_pos]\n",
    "            for ent in doc.ents.find_entities(start_pos=start_pos, end_pos=end_pos):\n",
    "                ents.append(ent.offset(-start_pos))\n",
    "            if self.filter_no_rel_candidates_sents and not self.has_rels_candidates(ents):\n",
    "                continue\n",
    "            rels = []\n",
    "            for rel in doc.rels.find_relations(start_pos=start_pos, end_pos=end_pos):\n",
    "                rels.append(rel.offset(-start_pos))\n",
    "            sent = Sentence(doc.doc_id, \n",
    "                            offset=start_pos, \n",
    "                            text=sent_text, \n",
    "                            ents=Entities(ents),\n",
    "                            rels=Relations(rels))\n",
    "            sents.append(sent)\n",
    "        return sents\n",
    "        \n",
    "    def __call__(self, docs):\n",
    "        sents = []\n",
    "        for doc in docs:\n",
    "            sents += self.extract_doc(doc)\n",
    "        return sents\n",
    "\n",
    "class EntityPair(object):\n",
    "    def __init__(self, doc_id, sent, from_ent, to_ent):\n",
    "        self.doc_id = doc_id\n",
    "        self.sent = sent\n",
    "        self.from_ent = from_ent\n",
    "        self.to_ent = to_ent\n",
    "        \n",
    "    def __repr__(self):\n",
    "        fmt = 'doc {}, sent {}, {} -> {}'\n",
    "        return fmt.format(self.doc_id, self.sent.text, self.from_ent, self.to_ent)\n",
    "\n",
    "class EntityPairsExtractor(object):\n",
    "    def __init__(self, allow_rel_types, max_len=150, ellipse_chars='$$', pad=10):\n",
    "        self.allow_rel_types = allow_rel_types\n",
    "        self.max_len = max_len\n",
    "        self.pad = pad\n",
    "        self.ellipse_chars = ellipse_chars\n",
    "        \n",
    "    def extract_candidate_rels(self, sent):\n",
    "        candidate_rels = []\n",
    "        for f_ent, t_ent in permutations(sent.ents, 2):\n",
    "            rel_cate = (f_ent.category, t_ent.category)\n",
    "            if rel_cate in self.allow_rel_types:\n",
    "                candidate_rels.append((f_ent, t_ent))\n",
    "        return candidate_rels\n",
    "        \n",
    "    def make_entity_pair(self, sent, f_ent, t_ent):\n",
    "        doc_id = sent.doc_id\n",
    "        if f_ent.start_pos < t_ent.start_pos:\n",
    "            left_ent, right_ent = f_ent, t_ent\n",
    "        else:\n",
    "            left_ent, right_ent = t_ent, f_ent\n",
    "        start_pos = max(0, left_ent.start_pos - self.pad)\n",
    "        end_pos = min(len(sent), right_ent.end_pos + self.pad)\n",
    "        res_sent = sent[start_pos: end_pos]\n",
    "        \n",
    "        if len(res_sent) > self.max_len:\n",
    "            res_sent = res_sent.abbreviate(self.max_len)\n",
    "        f_ent = res_sent.ents[f_ent.ent_id]\n",
    "        t_ent = res_sent.ents[t_ent.ent_id]\n",
    "        return EntityPair(doc_id, res_sent, f_ent, t_ent)\n",
    "    \n",
    "    def __call__(self, sents):\n",
    "        samples = []\n",
    "        for sent in sents:\n",
    "            for f_ent, t_ent in self.extract_candidate_rels(sent.ents):\n",
    "                entity_pair = self.make_entity_pair(sent, f_ent, t_ent)\n",
    "                samples.append(entity_pair)\n",
    "        return samples\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, entity_pairs, doc_ent_pair_ids=set(), word2idx=None, cate2idx=None, max_len=150):\n",
    "        self.entity_pairs = entity_pairs\n",
    "        self.doc_ent_pair_ids = doc_ent_pair_ids\n",
    "        self.max_len = max_len\n",
    "        self.word2idx = word2idx\n",
    "        self.cate2idx = cate2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entity_pairs)\n",
    "    \n",
    "    def build_vocab_dict(self, vocab_size=2000):\n",
    "        counter = Counter()\n",
    "        for ent_pair in self.entity_pairs:\n",
    "            for char in ent_pair.sent.text:\n",
    "                counter[char] += 1\n",
    "        word2idx = dict()\n",
    "        word2idx['<pad>'] = 0\n",
    "        word2idx['<unk>'] = 1\n",
    "        if vocab_size > 0:\n",
    "            num_most_common = vocab_size - len(word2idx)\n",
    "        else:\n",
    "            num_most_common = len(counter)\n",
    "        for char, _ in counter.most_common(num_most_common):\n",
    "            word2idx[char] = word2idx.get(char, len(word2idx))\n",
    "        self.word2idx = word2idx\n",
    "    \n",
    "    def vectorize(self, ent_pair):\n",
    "        sent_vec = np.zeros(self.max_len, dtype='int')\n",
    "        for i, c in enumerate(ent_pair.sent.text):\n",
    "            sent_vec[i] = self.word2idx.get(c, 1) \n",
    "        ents_vec = ent_pair.sent.ents.vectorize(vec_len=self.max_len, cate2idx=self.cate2idx)\n",
    "        from_ent_vec = np.zeros(self.max_len, dtype='int')\n",
    "        from_ent_vec[ent_pair.from_ent.start_pos: ent_pair.from_ent.end_pos] = 1\n",
    "        to_ent_vec = np.zeros(self.max_len, dtype='int')\n",
    "        to_ent_vec[ent_pair.to_ent.start_pos: ent_pair.to_ent.end_pos] = 1\n",
    "        \n",
    "        if (ent_pair.sent.doc_id, ent_pair.from_ent.ent_id, ent_pair.to_ent.ent_id) in self.doc_ent_pair_ids:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        return sent_vec, ents_vec, from_ent_vec, to_ent_vec, label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sent_vecs, ents_vecs, from_ent_vecs, to_ent_vecs, ent_dists, labels = [], [], [], [], [], []\n",
    "        entity_pairs = self.entity_pairs[idx]\n",
    "        if not isinstance(entity_pairs, list):\n",
    "            entity_pairs = [entity_pairs]\n",
    "        for ent_pair in entity_pairs:\n",
    "            sent_vec, ents_vec, from_ent_vec, to_ent_vec, label = self.vectorize(ent_pair)\n",
    "            sent_vecs.append(sent_vec)\n",
    "            ents_vecs.append(ents_vec)\n",
    "            from_ent_vecs.append(from_ent_vec)\n",
    "            to_ent_vecs.append(to_ent_vec)\n",
    "            ent_dists.append(ent_pair.to_ent.start_pos - ent_pair.from_ent.end_pos)\n",
    "            labels.append(label)\n",
    "            \n",
    "        sent_vecs = np.array(sent_vecs)\n",
    "        ents_vecs = np.array(ents_vecs)\n",
    "        from_ent_vecs = np.array(from_ent_vecs)\n",
    "        to_ent_vecs = np.array(to_ent_vecs)\n",
    "        ent_dists = np.array(ent_dists)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        return sent_vecs, ents_vecs, from_ent_vecs, to_ent_vecs, ent_dists, labels\n",
    "\n",
    "def train_word_embeddings(entity_pairs, word2idx, *args, **kwargs):\n",
    "    w2v_train_sents = []\n",
    "    for ent_pair in entity_pairs:\n",
    "        w2v_train_sents.append(list(ent_pair.sent.text))\n",
    "    w2v_model = Word2Vec(w2v_train_sents, *args, **kwargs)\n",
    "    word2idx.update({w: i for i, w in enumerate(w2v_model.wv.index_to_key, start=len(word2idx))})\n",
    "    idx2word = {v: k for k, v in word2idx.items()}\n",
    "    # vocab_size = len(word2idx)\n",
    "    # w2v_embeddings = np.zeros((len(word2idx), w2v_model.vector_size))\n",
    "    w2v_embeddings = np.zeros((10000, w2v_model.vector_size))\n",
    "    for char, char_idx in word2idx.items():\n",
    "        if char in w2v_model.wv:\n",
    "            w2v_embeddings[char_idx] = w2v_model.wv[char]\n",
    "    return word2idx, idx2word, w2v_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGBGEjByc1D0",
   "metadata": {
    "id": "HGBGEjByc1D0",
    "tags": []
   },
   "source": [
    "## 1.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IEVoZ7Ahc2sv",
   "metadata": {
    "id": "IEVoZ7Ahc2sv"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    num_ent_classes = len(ENTITIES) + 1\n",
    "    ent_emb_size = 2\n",
    "    emb_size = w2v_embeddings.shape[-1]\n",
    "    vocab_size = 10000#len(word2idx)\n",
    "    \n",
    "    inp_sent = Input(shape=(max_len,), dtype='int32')\n",
    "    inp_ent = Input(shape=(max_len,), dtype='int32')\n",
    "    inp_f_ent = Input(shape=(max_len,), dtype='float32')\n",
    "    inp_t_ent = Input(shape=(max_len,), dtype='float32')\n",
    "    inp_ent_dist = Input(shape=(1,), dtype='float32')\n",
    "    f_ent = Lambda(lambda x: K.expand_dims(x))(inp_f_ent)\n",
    "    t_ent = Lambda(lambda x: K.expand_dims(x))(inp_t_ent)\n",
    "    \n",
    "    ent_embed = Embedding(num_ent_classes, ent_emb_size)(inp_ent)\n",
    "    sent_embed = Embedding(vocab_size, emb_size, weights=[w2v_embeddings], trainable=False)(inp_sent)\n",
    "    \n",
    "    x = Concatenate()([sent_embed, ent_embed])\n",
    "    x = Conv1D(64, 1, padding='same', activation='relu')(x)\n",
    "    \n",
    "    f_res = layers.multiply([f_ent, x])\n",
    "    t_res = layers.multiply([t_ent, x])\n",
    "    \n",
    "    conv = Conv1D(64, 3, padding='same', activation='relu')\n",
    "    f_x = conv(x)\n",
    "    t_x = conv(x)\n",
    "    f_x = layers.add([f_x, f_res])\n",
    "    t_x = layers.add([t_x, t_res])\n",
    "    \n",
    "    f_res = layers.multiply([f_ent, f_x])\n",
    "    t_res = layers.multiply([t_ent, t_x])\n",
    "    conv = Conv1D(64, 3, padding='same', activation='relu')\n",
    "    f_x = conv(x)\n",
    "    t_x = conv(x)\n",
    "    f_x = layers.add([f_x, f_res])\n",
    "    t_x = layers.add([t_x, t_res])\n",
    "\n",
    "    f_res = layers.multiply([f_ent, f_x])\n",
    "    t_res = layers.multiply([t_ent, t_x])\n",
    "    conv = Conv1D(64, 3, padding='same', activation='relu')\n",
    "    f_x = conv(x)\n",
    "    t_x = conv(x)\n",
    "    f_x = layers.add([f_x, f_res])\n",
    "    t_x = layers.add([t_x, t_res])\n",
    "    \n",
    "    conv = Conv1D(64, 3, activation='relu')\n",
    "    f_x = MaxPool1D(3)(conv(f_x))\n",
    "    t_x = MaxPool1D(3)(conv(t_x))\n",
    "    \n",
    "    conv = Conv1D(64, 3, activation='relu')\n",
    "    f_x = MaxPool1D(3)(conv(f_x))\n",
    "    t_x = MaxPool1D(3)(conv(t_x))\n",
    "    \n",
    "    f_x = Flatten()(f_x)\n",
    "    t_x = Flatten()(t_x)\n",
    "    \n",
    "    x = Concatenate()([f_x, t_x, inp_ent_dist])\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model([inp_sent, inp_ent, inp_f_ent, inp_t_ent, inp_ent_dist], x)\n",
    "    return model\n",
    "\n",
    "def build_lstm_crf_model(num_cates, seq_len, vocab_size, model_opts=dict()):\n",
    "    opts = {\n",
    "        'emb_size': 256,\n",
    "        'emb_trainable': True,\n",
    "        'emb_matrix': None,\n",
    "        'lstm_units': 256,\n",
    "        'optimizer': Adam()# adam_v2.Adam()\n",
    "    }\n",
    "    opts.update(model_opts)\n",
    "\n",
    "    input_seq = Input(shape=(seq_len,), dtype='int32')\n",
    "    if opts.get('emb_matrix') is not None:\n",
    "        embedding = Embedding(vocab_size, opts['emb_size'], weights=[opts['emb_matrix']], trainable=opts['emb_trainable'])\n",
    "    else:\n",
    "        embedding = Embedding(vocab_size, opts['emb_size'])\n",
    "    x = embedding(input_seq)\n",
    "    lstm = LSTM(opts['lstm_units'], return_sequences=True)\n",
    "    x = Bidirectional(lstm)(x)\n",
    "    crf = CRF(num_cates, sparse_target=True)\n",
    "    output = crf(x)\n",
    "\n",
    "    model = Model(input_seq, output)\n",
    "    model.compile(opts['optimizer'], loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1NA8VxCfF8X",
   "metadata": {
    "id": "r1NA8VxCfF8X"
   },
   "source": [
    "## 1.3 实体识别训练预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y31c5lFhfRYj",
   "metadata": {
    "id": "y31c5lFhfRYj"
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hm41uPbxfQqN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "hm41uPbxfQqN",
    "outputId": "21b54be4-acbf-4462-9a91-5413a99ed8f6"
   },
   "outputs": [],
   "source": [
    "ent2idx = dict(zip(ENTITIES, range(1, len(ENTITIES) + 1)))\n",
    "idx2ent = dict([(v, k) for k, v in ent2idx.items()])\n",
    "\n",
    "docs = Documents(data_dir=train_data_dir)\n",
    "rs = ShuffleSplit(n_splits=1, test_size=20, random_state=2018)\n",
    "train_doc_ids, test_doc_ids = next(rs.split(docs))\n",
    "train_docs, test_docs = docs[train_doc_ids], docs[test_doc_ids]\n",
    "train_docs[0].text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbaUf6jrgCfy",
   "metadata": {
    "id": "HbaUf6jrgCfy"
   },
   "outputs": [],
   "source": [
    "num_cates = max(ent2idx.values()) + 1\n",
    "sent_len = 64\n",
    "vocab_size = 10000\n",
    "emb_size = 100\n",
    "sent_pad = 10\n",
    "sent_extrator = SentenceExtractor(window_size=sent_len, pad_size=sent_pad)\n",
    "train_sents = sent_extrator(train_docs)\n",
    "train_data = Dataset(train_sents, cate2idx=ent2idx)\n",
    "train_data.build_vocab_dict(vocab_size=vocab_size)\n",
    "vocab_size = len(train_data.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PhY9QiyNXS_K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "PhY9QiyNXS_K",
    "outputId": "b4ca8847-9cdc-4fc2-c10e-1ae42d4bf0c8"
   },
   "outputs": [],
   "source": [
    "w2v_train_sents = []\n",
    "for doc in docs:\n",
    "    w2v_train_sents.append(list(doc.text))\n",
    "w2v_model = Word2Vec(w2v_train_sents, vector_size=emb_size) # vector_size\n",
    "\n",
    "w2v_embeddings = np.zeros((vocab_size, emb_size))\n",
    "for char, char_idx in train_data.word2idx.items():\n",
    "    if char in w2v_model.wv:\n",
    "        w2v_embeddings[char_idx] = w2v_model.wv[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fdcb9b-cd15-4216-91f8-1b72ae16f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('word2idx.txt','w')\n",
    "fw.write(str(train_data.word2idx))\n",
    "fw.close()\n",
    " \n",
    "#读取\n",
    "fr = open('word2idx.txt','r')\n",
    "word2idx = fr.read()\n",
    "word2idx = eval(word2idx)\n",
    "fr.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F_rdd8MGLowL",
   "metadata": {
    "id": "F_rdd8MGLowL"
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cNTM6uB6pL2J",
   "metadata": {
    "id": "cNTM6uB6pL2J"
   },
   "outputs": [],
   "source": [
    "seq_len = sent_len + 2 * sent_pad\n",
    "ent_model = build_lstm_crf_model(num_cates, seq_len = seq_len, vocab_size = vocab_size, \n",
    "                             model_opts={'emb_matrix': w2v_embeddings, 'emb_size': emb_size, 'emb_trainable': False})\n",
    "print(ent_model.summary())\n",
    "\n",
    "train_x, train_y = train_data[:]\n",
    "print('train_x.shape', train_x.shape)\n",
    "print('train_y.shape', train_y.shape)\n",
    "\n",
    "ent_model.fit(train_x, train_y, batch_size=64, epochs=10)\n",
    "ent_model.save('ner_bs64_epo1.h5')\n",
    "ent_model.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iV0T3-GBLsK1",
   "metadata": {
    "id": "iV0T3-GBLsK1"
   },
   "source": [
    "### 测试评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wcf2f8u1pQUh",
   "metadata": {
    "id": "wcf2f8u1pQUh"
   },
   "outputs": [],
   "source": [
    "test_docs = Documents(data_dir = test_a_data_dir)\n",
    "sent_extrator = SentenceExtractor(window_size=sent_len, pad_size=sent_pad)\n",
    "test_sents = sent_extrator(test_docs)\n",
    "test_data = Dataset(test_sents, cate2idx=ent2idx)\n",
    "test_data.build_vocab_dict(vocab_size=vocab_size)\n",
    "test_data = Dataset(test_sents, word2idx=train_data.word2idx, cate2idx=ent2idx)\n",
    "test_X, _ = test_data[:]\n",
    "\n",
    "preds = ent_model.predict(test_X, batch_size=64, verbose=True)\n",
    "pred_docs = make_predictions(preds, test_data, sent_pad, test_docs, idx2ent)\n",
    "f_score, precision, recall = Evaluator.f1_score(test_docs, pred_docs)\n",
    "print('f_score: ', f_score)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "# sample_doc_id = list(pred_docs.keys())[0]\n",
    "# test_docs[sample_doc_id]\n",
    "# pred_docs[sample_doc_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb788c-eb18-4cc6-8bda-96c9bc08a170",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3890f532-0a92-484c-a406-d96268a3b03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 38ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">主诉&quot;: &quot;发热5天 &quot;, &quot;现病史&quot;: &quot;患儿5天前（25/5）无明显诱因出现<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">发热<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，热峰38.5℃，予<mark class=\"entity\" style=\"background: #ffeb80; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">口服<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Method</span></mark><mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">退热药<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>可降至正常，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">咳嗽<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">气促<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无抽搐，无皮疹，次日家属发现患儿颈部肿大，伴触痛，遂至当地医院就诊，考虑<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">急性淋巴结炎<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>，予喜炎平肌注、<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">奥司他韦<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>、<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">小儿热速清颗粒<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Treatment</span></mark>等治疗，患儿仍有<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">反复发热<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，热峰升至40℃，双足可见皮疹及脱皮，27/5至广州市增城区人民医院就诊住院治疗1天，查血常规<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">示白细胞<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">22.9910^9/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">中性粒细胞百分比<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">92.8%<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">超敏CRP<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">120.95mg/l<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">血沉<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">105mm/h<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，生化示<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">ALT263IU/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">AST4<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">10IU/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">TB41.7<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>umol/l，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">降钙素原<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">8.466mg/ml<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，考虑传染性<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">单核细胞<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>增多症待查，川崎病待查，肝功能损害，予<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">头孢硫<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>脒抗感染、<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">补液<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Treatment</span></mark>、退热等治疗，患儿仍有反复高热，并出现<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">呕吐<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，结膜充血、双脚蜕皮，昨家属签字出院自行至我院急诊留观，予完善相关检查及<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">静滴舒普深<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>治疗，患儿仍有<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">发热<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，现为进一步诊治拟粘<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">膜皮肤淋巴结综合征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>(川崎病)收入我科。起病以来，精神、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">反应一般<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，胃纳、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">睡眠欠佳<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，近两天结<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">稀烂样便<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，小便黄，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">体重<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>无明显变化。&quot;, &quot;既往史&quot;: &quot;2017.11曾因热性惊厥住院治疗，否认<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肝炎<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>、<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">结核<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>等传染病史，否认药物敏史，否认食物过敏史，否认葡萄糖-6-磷酸脱氢酶缺陷症病史、地中海贫血，否认外伤史，否认手术史，否认输血史。 头孢硫脒，舒普深 &quot;, &quot;个人史&quot;: &quot;出生史：第1胎第1产，足月。生<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">产方式：剖宫<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark><mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">产<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Reason</span></mark>，出生于：医院，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">出生体重<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">2.85kg<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，无窒息史，母妊娠期无特殊。喂养史：生后母乳喂养至4个月，按时添加<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">钙剂<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>，按时添加辅食。生长发育史：(精神、运动发育迟缓者及两岁内患者需填写) 于正常同龄儿童相似。 接种疫苗史：预防接种按时，家属未提供疫苗接种本： &quot;, &quot;家族史&quot;: &quot;父亲、母亲体健。否认近亲婚配，否认葡萄糖-6-磷酸脱氢酶缺陷症等遗传病史，否认<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肝炎<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>、<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">结核<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>等传染病史。   同胞情况: &quot;, &quot;体格检查&quot;: <mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">&quot;T<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">38.8℃   P 125次/分   R 26次/分<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>  BP <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">88/56mmHg<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>  <mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">体重<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">15kg<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark> 一般情况:<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">发育正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，营养中等，自主体位，表情自如，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">神志清晰<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">精神疲倦<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，反应一般，无脱水征，查体合作。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">皮肤粘膜<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>：粘膜轻度黄<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">染<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，皮肤弹性良好，皮肤灼热，未见<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">大理石花纹<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。耳后及<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">下肢<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>见散在<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">红色皮疹<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">皮下出血点<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">水肿<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，手足可见<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">硬肿<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，足背侧有脱皮。淋 巴 <mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">结<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>: <mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双侧颈部<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>可触及数<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">个肿大淋巴结，直径约1-2cm<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，伴<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">触痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，活动欠佳。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">头部：头颅<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>：正常，前囟已闭，后囟已闭。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">眼：<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark> 外观正常，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">眼睑正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">睑结膜<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>充血，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">球结膜<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>充血，无脓性分泌物，巩膜无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">黄染<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，角膜透明，双侧瞳孔等大等圆，直径约3mm，对光反射灵敏。耳：耳廓正常，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">畸形<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">结节<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肿胀<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，提拉耳廓无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">疼痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，乳突区无压痛，外耳道无分泌物。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">鼻：鼻部<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>外形无异常，鼻唇沟双侧对称，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">鼻翼扇动<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，鼻腔内无异常分泌物，鼻中隔无偏曲，鼻窦区无压痛。口：口腔无异味，口唇红润，口腔粘膜无出血点及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">溃疡<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，咽部充血 ，扁桃体I肿大，表面无脓点，声<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">音稍嘶哑<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">颈部：颈部：颈软<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>，无抵抗，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">颈静脉<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>无怒张，肝<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">颈静脉<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>回流征阴性，气管居中，未触及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">甲状腺肿大<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胸部：胸<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">廓外形对称<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无局部膨隆或凹陷，胸廓无压痛。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肺脏：望视诊：呼吸平稳<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>规则。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">触诊：触觉语颤对称<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胸膜摩擦感<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">皮下捻发感<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。叩诊<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">：清音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。听诊：<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双肺呼吸音对称<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双肺<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>呼吸音粗，未闻及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">干湿啰音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，未闻及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">支气管呼吸音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，语<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">音传导正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胸膜摩擦音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">捻发音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark> 。心脏：视诊:无心前区隆起。触诊:心尖<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">搏动位<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>置在<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">左侧第五肋间隙锁骨<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>中线。听诊<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">：心率<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">125次/分<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，律齐，未闻及早搏音，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">心音有力<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，未闻及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">杂音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，未闻及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">心包摩擦音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。腹部：视诊：腹部平坦，未见腹部瘢痕，未见<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">腹壁静脉<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>曲张，未见<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胃肠<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>蠕动波。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">触诊：腹部<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>软，压之无哭闹不安，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肝脾肋下未触<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>及，<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Murphy征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Disease</span></mark>-。叩诊：鼓音，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肝区无叩痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，移动性浊音阴性。听诊：<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肠鸣音正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">3-5次/分<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">过气水音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">血管杂音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。肛门、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">外生殖器：肛门<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">外生殖器无畸形<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，肛<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">门外周无脱皮<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">脊柱四肢<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>:<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">脊柱生理弯曲存在<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，棘突无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">压痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">叩痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双手有硬肿<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，无脱皮，四肢无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">畸形<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双下肢<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>无<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">浮肿<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肌肉无萎缩<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">无压痛<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">关节无畸形<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肿胀<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">强直<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、活动受限，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">四肢肌力V级<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肌张力正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">神经系统：生理反射存在<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">H<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>offmann征、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Babinski征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>、<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Oppenheim征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Gordon征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>、<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Chaddock征未<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>引出，Kernig征、<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">Brudzinski征<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>阴性。 &quot;, &quot;体温&quot;: <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">&quot;头孢硫<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>脒，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">舒普<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">深 &quot;<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>, &quot;专科检查&quot;: <mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">&quot;神志清晰<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">反应可<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">全身皮肤<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">轻度黄染<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，耳后及下肢可见散在<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">红色皮疹<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双侧球结膜<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>充血，无脓性分泌物，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双侧颈部<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>可扪及多个肿大<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">淋巴结<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>，直径约<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">1-2cm<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，杨梅舌，舌苔厚，咽充血，扁桃体I肿大，未见脓点。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胸腹部<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>未见<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">皮疹<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">颈软<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>，无抵抗。呼吸规律，平顺，26次/分，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双肺<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">呼吸音粗<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，未闻及明显干湿啰音。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">心率<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark><mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">125次/分<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，律齐，未闻及<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">病理性杂音<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。腹平，肠鸣音正常，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肝脾肋下未<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>及，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">压之无哭闹不安<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">四肢肌力V级<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肌张力正常<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双手有硬肿<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Symptom</span></mark>，<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">足背<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>侧有脱皮。<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肢端暖<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>，CRT 1s。 &quot;, &quot;辅助检查结果&quot;: &quot;我院急诊留观：2019.05.28：ABO血型(微柱凝胶法) A型   Rh(D)血型(微柱凝胶法) 阳性 ；凝血四项：纤维蛋白原 <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">8.17g/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>；|速诊生化：丙氨酸氨基转移酶 181U/L，天门冬氨酸氨基转移<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">酶<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">119U/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">白蛋白<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">35.3g/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">总胆红素<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">99.6umol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">直接胆红素<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">73.1umol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">总胆汁酸<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">444.7 umol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">尿酸<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">448 umol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">肌酐<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">26  umol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">超敏-CRP<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">110.13 mg/L；<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">血气分析：钾<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">2.40  mmol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">钠<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">123.0mmol/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">全血剩余碱<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> -<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">8.30  mmol/L；<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">血常规<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>+<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">CRP：快速C反应蛋白<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">115.5 mg/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark> ，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">白细胞<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">13.9*10^9/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">中性粒细胞绝对值<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">12.65*10^9/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">中性粒细胞百分比<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">91  %<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>。2019.05.29免疫6项：免疫球蛋白A <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">1.58 g/L<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>，免疫球蛋白E 938IU/ML；<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">降钙素原<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">26.900  ng/<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark><mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">ml；<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>粪便分析<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">：隐血试验<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> 阳性 ；尿液分析：红细胞 8.58 个/ul，<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">白细胞<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark> <mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">69.96个/ul<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>。<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">胸片<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test</span></mark>示<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">双肺纹理增强<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Test_Value</span></mark>。九项<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">呼吸道病原<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Anatomy</span></mark>学、<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">EB病毒<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>、<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">咽拭子I<span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">Drug</span></mark>+II类、血培养结果未回。  &quot;, &quot;其他信息&quot;: &quot;姓名: 罗俊宇  住址：广东省广州市增城区派潭镇利迳村围陇路11号  性别: 男  病史陈述者:父母  年龄:5岁  联系人：龙云倩 母子 广东省广州市增城区派潭镇利迳村围陇路11号 13640853793 民族: 汉族</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Document at 0x7fb58014fb10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent2idx = dict(zip(ENTITIES, range(1, len(ENTITIES) + 1)))\n",
    "idx2ent = dict([(v, k) for k, v in ent2idx.items()])\n",
    "num_cates = max(ent2idx.values()) + 1\n",
    "sent_len = 64\n",
    "vocab_size = 10000\n",
    "emb_size = 100\n",
    "sent_pad = 10\n",
    "\n",
    "model_test = load_model('ner_bs64_epo1.h5', custom_objects={\"CRF\": CRF, 'crf_loss': crf_loss, 'crf_viterbi_accuracy': crf_viterbi_accuracy})\n",
    "\n",
    "\n",
    "txt = \"\"\"主诉\": \"发热5天 \", \"现病史\": \"患儿5天前（25/5）无明显诱因出现发热，热峰38.5℃，予口服退热药可降至正常，无咳嗽，无气促，无抽搐，无皮疹，次日家属发现患儿颈部肿大，伴触痛，遂至当地医院就诊，考虑急性淋巴结炎，予喜炎平肌注、奥司他韦、小儿热速清颗粒等治疗，患儿仍有反复发热，热峰升至40℃，双足可见皮疹及脱皮，27/5至广州市增城区人民医院就诊住院治疗1天，查血常规示白细胞22.9910^9/L，中性粒细胞百分比 92.8%，超敏CRP120.95mg/l，血沉105mm/h，生化示ALT263IU/L，AST410IU/L，TB41.7umol/l，降钙素原8.466mg/ml，考虑传染性单核细胞增多症待查，川崎病待查，肝功能损害，予头孢硫脒抗感染、补液、退热等治疗，患儿仍有反复高热，并出现呕吐，结膜充血、双脚蜕皮，昨家属签字出院自行至我院急诊留观，予完善相关检查及静滴舒普深治疗，患儿仍有发热，现为进一步诊治拟粘膜皮肤淋巴结综合征(川崎病)收入我科。起病以来，精神、反应一般，胃纳、睡眠欠佳，近两天结稀烂样便，小便黄，体重无明显变化。                                                                                \", \"既往史\": \"2017.11曾因热性惊厥住院治疗，否认肝炎、结核等传染病史，否认药物敏史，否认食物过敏史，否认葡萄糖-6-磷酸脱氢酶缺陷症病史、地中海贫血，否认外伤史，否认手术史，否认输血史。 头孢硫脒，舒普深 \", \"个人史\": \"出生史：第1胎第1产，足月。生产方式：剖宫产，出生于：医院，出生体重2.85kg，无窒息史，母妊娠期无特殊。喂养史：生后母乳喂养至4个月，按时添加钙剂，按时添加辅食。生长发育史：(精神、运动发育迟缓者及两岁内患者需填写) 于正常同龄儿童相似。 接种疫苗史：预防接种按时，家属未提供疫苗接种本： \", \"家族史\": \"父亲、母亲体健。否认近亲婚配，否认葡萄糖-6-磷酸脱氢酶缺陷症等遗传病史，否认肝炎、结核等传染病史。   同胞情况: \", \"体格检查\": \"T 38.8℃   P 125次/分   R 26次/分  BP 88/56mmHg  体重 15kg 一般情况:发育正常，营养中等，自主体位，表情自如，神志清晰，精神疲倦，反应一般，无脱水征，查体合作。皮肤粘膜：粘膜轻度黄染，皮肤弹性良好，皮肤灼热，未见大理石花纹。耳后及下肢见散在红色皮疹，无皮下出血点，无水肿，手足可见硬肿，足背侧有脱皮。淋 巴 结: 双侧颈部可触及数个肿大淋巴结，直径约1-2cm，伴触痛，活动欠佳。头部：头颅：正常，前囟已闭，后囟已闭。眼： 外观正常，眼睑正常，睑结膜充血，球结膜充血，无脓性分泌物，巩膜无黄染，角膜透明，双侧瞳孔等大等圆，直径约3mm，对光反射灵敏。耳：耳廓正常，无畸形，无结节，无肿胀，提拉耳廓无疼痛，乳突区无压痛，外耳道无分泌物。鼻：鼻部外形无异常，鼻唇沟双侧对称，无鼻翼扇动，鼻腔内无异常分泌物，鼻中隔无偏曲，鼻窦区无压痛。口：口腔无异味，口唇红润，口腔粘膜无出血点及溃疡，咽部充血 ，扁桃体I肿大，表面无脓点，声音稍嘶哑。颈部：颈部：颈软，无抵抗，颈静脉无怒张，肝颈静脉回流征阴性，气管居中，未触及甲状腺肿大。胸部：胸廓外形对称，无局部膨隆或凹陷，胸廓无压痛。肺脏：望视诊：呼吸平稳规则。触诊：触觉语颤对称，无胸膜摩擦感，无皮下捻发感。叩诊：清音。听诊：双肺呼吸音对称，双肺呼吸音粗，未闻及干湿啰音，未闻及支气管呼吸音，语音传导正常，无胸膜摩擦音，无捻发音 。心脏：视诊:无心前区隆起。触诊:心尖搏动位置在左侧第五肋间隙锁骨中线。                    听诊：心率125次/分，律齐，未闻及早搏音，心音有力，未闻及杂音，未闻及心包摩擦音。腹部：视诊：腹部平坦，未见腹部瘢痕，未见腹壁静脉曲张，未见胃肠蠕动波。触诊：腹部软，压之无哭闹不安，肝脾肋下未触及，Murphy征-。叩诊：鼓音，肝区无叩痛，移动性浊音阴性。听诊：肠鸣音正常，3-5次/分，无过气水音，无血管杂音。肛门、外生殖器：肛门、外生殖器无畸形，肛门外周无脱皮。脊柱四肢:脊柱生理弯曲存在，棘突无压痛、叩痛，双手有硬肿，无脱皮，四肢无畸形，双下肢无浮肿。肌肉无萎缩、无压痛，关节无畸形、肿胀、强直、活动受限，四肢肌力V级，肌张力正常。神经系统：生理反射存在。Hoffmann征、Babinski征、Oppenheim征、Gordon征、Chaddock征未引出，Kernig征、Brudzinski征阴性。 \", \"体温\": \"头孢硫脒，舒普深 \", \"专科检查\": \"神志清晰，反应可。全身皮肤轻度黄染，耳后及下肢可见散在红色皮疹，双侧球结膜充血，无脓性分泌物，双侧颈部可扪及多个肿大淋巴结，直径约1-2cm，杨梅舌，舌苔厚，咽充血，扁桃体I肿大，未见脓点。胸腹部未见皮疹。颈软，无抵抗。呼吸规律，平顺，26次/分，双肺呼吸音粗，未闻及明显干湿啰音。心率125次/分，律齐，未闻及病理性杂音。腹平，肠鸣音正常，肝脾肋下未及，压之无哭闹不安。四肢肌力V级，肌张力正常。双手有硬肿，足背侧有脱皮。肢端暖，CRT 1s。 \", \"辅助检查结果\": \"我院急诊留观：2019.05.28：ABO血型(微柱凝胶法) A型   Rh(D)血型(微柱凝胶法) 阳性 ；凝血四项：纤维蛋白原 8.17g/L；|速诊生化：丙氨酸氨基转移酶 181U/L，天门冬氨酸氨基转移酶 119U/L，白蛋白 35.3g/L，总胆红素 99.6umol/L，直接胆红素 73.1umol/L，总胆汁酸 444.7 umol/L，尿酸 448 umol/L，肌酐 26  umol/L，超敏-CRP 110.13 mg/L；血气分析：钾 2.40  mmol/L，钠 123.0mmol/L，全血剩余碱 -8.30  mmol/L；血常规+CRP：快速C反应蛋白 115.5 mg/L ，白细胞 13.9*10^9/L，中性粒细胞绝对值 12.65*10^9/L，中性粒细胞百分比 91  %。2019.05.29免疫6项：免疫球蛋白A 1.58 g/L，免疫球蛋白E 938IU/ML；降钙素原 26.900  ng/ml；粪便分析：隐血试验 阳性 ；尿液分析：红细胞 8.58 个/ul，白细胞 69.96个/ul。胸片示双肺纹理增强。九项呼吸道病原学、EB病毒、咽拭子I+II类、血培养结果未回。  \", \"其他信息\": \"姓名: 罗俊宇                      住址：广东省广州市增城区派潭镇利迳村围陇路11号  性别: 男                          病史陈述者:父母      年龄:5岁                          联系人：龙云倩 母子 广东省广州市增城区派潭镇利迳村围陇路11号 13640853793                                 民族: 汉族    \"\"\"\n",
    "\n",
    "if os.path.isfile(base_path + '/Test'):\n",
    "    os.remove(base_path + '/Test')\n",
    "if not os.path.exists(base_path + '/Test'):\n",
    "    os.makedirs(base_path + '/Test')\n",
    "with open(base_path + '/Test/{}.txt'.format('1'), 'w') as fw:\n",
    "    fw.write(txt)\n",
    "fw.close()\n",
    "with open(base_path + '/Test/{}.ann'.format('1'), 'w') as fw:\n",
    "    fw.write('')\n",
    "fw.close()\n",
    "\n",
    "fr = open('word2idx.txt','r')\n",
    "train_data_word2idx = fr.read()\n",
    "train_data_word2idx = eval(train_data_word2idx)\n",
    "fr.close()\n",
    "\n",
    "test_docs = Documents(data_dir = base_path + '/Test')\n",
    "sent_extrator = SentenceExtractor(window_size=sent_len, pad_size=sent_pad)\n",
    "test_sents = sent_extrator(test_docs)\n",
    "test_data = Dataset(test_sents, cate2idx=ent2idx)\n",
    "test_data.build_vocab_dict(vocab_size=vocab_size)\n",
    "test_data = Dataset(test_sents, word2idx=train_data_word2idx, cate2idx=ent2idx)\n",
    "test_X, _ = test_data[:]\n",
    "\n",
    "preds = model_test.predict(test_X, batch_size=64, verbose=True)\n",
    "pred_docs = make_predictions(preds, test_data, sent_pad, test_docs, idx2ent)\n",
    "sample_doc_id = list(pred_docs.keys())[0]\n",
    "pred_docs[sample_doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Kr5j5w26IWsD",
   "metadata": {
    "id": "Kr5j5w26IWsD"
   },
   "outputs": [],
   "source": [
    "for i in pred_docs.keys():\n",
    "    with open(base_path+'/Test/{}.ann'.format(i), 'w') as fw:\n",
    "        for ent in pred_docs[i].ents.ents:\n",
    "            fw.write(ent.ent_id+'\\t'+ent.category+' '+str(ent.start_pos)+' '+str(ent.end_pos)+'\\t'+ent.text+'\\n')\n",
    "    fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7321b90c",
   "metadata": {
    "id": "7321b90c"
   },
   "source": [
    "## 1.4 关系抽取训练预测\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bki2fl6Ke3pG",
   "metadata": {
    "id": "Bki2fl6Ke3pG"
   },
   "source": [
    "### 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87671da",
   "metadata": {
    "id": "b87671da"
   },
   "outputs": [],
   "source": [
    "all_rel_types = set([tuple(re.split('[-_]' ,rel)) for rel in RELATIONS])\n",
    "ent2idx = dict(zip(ENTITIES, range(1, len(ENTITIES) + 1)))\n",
    "train_docs = Documents(train_data_dir)\n",
    "\n",
    "# 提取训练集中所有的 relation\n",
    "doc_ent_pair_ids = set()\n",
    "for doc in train_docs:\n",
    "    for rel in doc.rels:\n",
    "        doc_ent_pair_id = (doc.doc_id, rel.ent1.ent_id, rel.ent2.ent_id)\n",
    "        doc_ent_pair_ids.add(doc_ent_pair_id)\n",
    "\n",
    "# 从文档中抽取句子\n",
    "sent_extractor = SentenceExtractor(sent_split_char='。', window_size=2, rel_types=RELATIONS, filter_no_rel_candidates_sents=True)\n",
    "train_sents = sent_extractor(train_docs)\n",
    "\n",
    "# 从句子中提取 relation 候选集\n",
    "max_len = 500\n",
    "ent_pair_extractor = EntityPairsExtractor(all_rel_types, max_len=max_len)\n",
    "train_entity_pairs = ent_pair_extractor(train_sents)\n",
    "\n",
    "# 利用候选 relation 所在的句子训练字符级别的字向量\n",
    "word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "word2idx, idx2word, w2v_embeddings = train_word_embeddings(entity_pairs=chain(train_entity_pairs), word2idx=word2idx, vector_size=100) #vector_size,iter=10\n",
    "\n",
    "# 生成训练集\n",
    "train_data = Dataset(train_entity_pairs, doc_ent_pair_ids, word2idx=word2idx, max_len=max_len, cate2idx=ent2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8733e",
   "metadata": {
    "id": "cbc8733e"
   },
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1406674",
   "metadata": {
    "id": "f1406674"
   },
   "outputs": [],
   "source": [
    "num_ent_classes = len(ENTITIES) + 1\n",
    "ent_emb_size = 2\n",
    "emb_size = w2v_embeddings.shape[-1]\n",
    "vocab_size = 10000#len(word2idx)\n",
    "print('------->', emb_size)\n",
    "\n",
    "tr_sent, tr_ent, tr_f_ent, tr_t_ent, tr_ent_dist, tr_y = train_data[:]\n",
    "K.clear_session()\n",
    "model = build_model()\n",
    "model.compile('adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(x=[tr_sent, tr_ent, tr_f_ent, tr_t_ent, tr_ent_dist], y=tr_y, batch_size=64, epochs=2)\n",
    "model.save('re_bs64_epo1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6bd04c-3232-4ff1-ac54-89f2930e148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('re_bs64_epo1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mFpSzFM53C14",
   "metadata": {
    "id": "mFpSzFM53C14"
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CRubicaT3DBg",
   "metadata": {
    "id": "CRubicaT3DBg"
   },
   "outputs": [],
   "source": [
    "def generate_submission(preds, entity_pairs, threshold):\n",
    "    doc_rels = defaultdict(set)\n",
    "    for p, ent_pair in zip(preds, entity_pairs):\n",
    "        if p >= threshold:\n",
    "            doc_id = ent_pair.doc_id\n",
    "            f_ent_id = ent_pair.from_ent.ent_id\n",
    "            t_ent_id = ent_pair.to_ent.ent_id\n",
    "            category = ent_pair.from_ent.category + '_' + ent_pair.to_ent.category\n",
    "            category = category.replace('SideEff_Drug', 'SideEff-Drug')\n",
    "            doc_rels[doc_id].add((f_ent_id, t_ent_id, category))\n",
    "    submits = dict()\n",
    "    tot_num_rels = 0\n",
    "    for doc_id, rels in doc_rels.items():\n",
    "        output_str = ''\n",
    "        for i, rel in enumerate(rels):\n",
    "            tot_num_rels += 1\n",
    "            line = 'R{}\\t{} Arg1:{} Arg2:{}\\n'.format(i + 1, rel[2], rel[0], rel[1])\n",
    "            output_str += line\n",
    "        submits[doc_id] = output_str\n",
    "    # print('Total number of relations: {}. In average {} relations per doc.'.format(tot_num_rels, tot_num_rels / len(submits)))\n",
    "    return submits\n",
    "\n",
    "def output_submission(submit_file, submits, test_dir):\n",
    "    dir_name = os.path.basename(submit_file)\n",
    "    dir_name, _ = os.path.splitext(dir_name)\n",
    "    with zipfile.ZipFile(submit_file, 'w') as zf:\n",
    "        for doc_id, rels_str in submits.items():\n",
    "            fname = '{}.ann'.format(doc_id)\n",
    "            test_file = os.path.join(test_dir, fname)\n",
    "            content = open(test_file, encoding='utf-8').read()\n",
    "            content += rels_str\n",
    "            zf.writestr(os.path.join(dir_name, fname), content)\n",
    "\n",
    "            \n",
    "model_re = load_model('re_bs64_epo1.h5')\n",
    "\n",
    "max_len = 150\n",
    "all_rel_types = set([tuple(re.split('[-_]' ,rel)) for rel in RELATIONS])\n",
    "ent2idx = dict(zip(ENTITIES, range(1, len(ENTITIES) + 1)))\n",
    "ent_pair_extractor = EntityPairsExtractor(all_rel_types, max_len=max_len)\n",
    "\n",
    "test_docs = Documents(base_path + '/Test')\n",
    "sent_extractor = SentenceExtractor(sent_split_char='。', window_size=2, rel_types=RELATIONS, filter_no_rel_candidates_sents=True)\n",
    "test_sents = sent_extractor(test_docs)\n",
    "test_entity_pairs = ent_pair_extractor(test_sents)\n",
    "test_data = Dataset(test_entity_pairs, word2idx=word2idx, max_len=max_len, cate2idx=ent2idx)\n",
    "te_sent, te_ent, te_f_ent, te_t_ent, te_ent_dist, te_y = test_data[:]\n",
    "\n",
    "model_re.compile('adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "preds = model_re.predict(x=[te_sent, te_ent, te_f_ent, te_t_ent, te_ent_dist], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4606de-eaef-485f-a455-f2bcb3cb4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "submits = generate_submission(preds, test_entity_pairs, 0.5)\n",
    "submits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eaae3c-b2ca-4b4f-9e2d-fbbcd1f2dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "submits = generate_submission(preds, test_entity_pairs, 0.5)\n",
    "\n",
    "for doc_id, rels_str in submits.items():\n",
    "    with open(base_path + '/Test/{}.ann'.format(doc_id), 'a+') as fw:\n",
    "        fw.write(rels_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ThnvgGI9CGUW",
   "metadata": {
    "id": "ThnvgGI9CGUW"
   },
   "outputs": [],
   "source": [
    "test_docs = Documents(data_dir = base_path + '/Test')\n",
    "\n",
    "relues = []\n",
    "for i in test_docs:\n",
    "    #print(i.text)\n",
    "    #print(i.ents.ents)\n",
    "    # print(i.rels.rels)\n",
    "    relue_dict = {}\n",
    "    ent_list = []\n",
    "    spo_list = []\n",
    "    \n",
    "    for e in i.ents.ents:\n",
    "        e_spl = str(e).split()\n",
    "        ent_list.append({\n",
    "            'entity':e_spl[4].replace(')',''),\n",
    "            'entity_type':e_spl[1],\n",
    "            'entity_indx':e_spl[2]+e_spl[3]\n",
    "              })\n",
    "    for r in i.rels.rels:\n",
    "        r_spl = str(r).split()\n",
    "        spo_list.append({\n",
    "            'predicate':r_spl[1],\n",
    "            'subject':r_spl[5].replace(')',''),\n",
    "            'subject_type':r_spl[3],\n",
    "            'subject_indx':r_spl[4]+r_spl[5],\n",
    "            'object':{'@value':r_spl[11].replace(')','')},\n",
    "            'object_indx':r_spl[9]+r_spl[10],\n",
    "            'object_type':{'@value':r_spl[8]}\n",
    "            })\n",
    "    \n",
    "    relue_dict['text'] = i.text\n",
    "    relue_dict['ent_list'] = ent_list\n",
    "    relue_dict['spo_list'] = spo_list\n",
    "    relues.append(relue_dict)\n",
    "    \n",
    "# print(relues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5NrKQPsqnNl8",
   "metadata": {
    "id": "5NrKQPsqnNl8",
    "tags": []
   },
   "source": [
    "# 二、SuperGUTScode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Nr3K-Aknn5h",
   "metadata": {
    "id": "7Nr3K-Aknn5h"
   },
   "outputs": [],
   "source": [
    "!pip uninstall keras tensorflow\n",
    "!pip install keras==2.3.1 tensorflow==2.2.0 sklearn #python3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1VYGbdRnS0x",
   "metadata": {
    "id": "l1VYGbdRnS0x"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RozIRnH_r5Wd",
   "metadata": {
    "id": "RozIRnH_r5Wd"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/data_rujun2\n",
    "!mkdir /content/model_rujun2\n",
    "import os\n",
    "base_path = os.getcwd()# os.path.dirname(os.getcwd())\n",
    "data_path = base_path + '/data_rujun2'\n",
    "model_path = base_path + '/model_rujun2'\n",
    "print(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3KjUg8wttDZ",
   "metadata": {
    "id": "d3KjUg8wttDZ"
   },
   "source": [
    "## 2.1 utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aBKpJ7dt0LY",
   "metadata": {
    "id": "0aBKpJ7dt0LY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def f1_score(y_true,y_pred):\n",
    "    y_pred = y_pred.max(axis=1).round()\n",
    "    y_true = y_true.max(axis=1)\n",
    "    return 2*np.sum(y_true*y_pred)/(np.sum(y_true)+np.sum(y_pred))\n",
    "\n",
    "def f1_score_v2(y_true,y_pred,bias):\n",
    "    y_pred = y_pred.max(axis=1).round()\n",
    "    y_true = y_true.max(axis=1)\n",
    "    return 2 * np.sum(y_true * y_pred) / (bias + np.sum(y_pred))\n",
    "\n",
    "def cal_total_relations(file_ids):\n",
    "    all_num = 0\n",
    "    for file_id in file_ids:\n",
    "        label = pd.read_csv(f'{tr_path}{file_id}.ann', header=None, sep='\\t')\n",
    "        all_num += len(label[label[0].str.startswith('R')])\n",
    "    return all_num\n",
    "\n",
    "def split_data(dataset,file_id):\n",
    "    idx = []\n",
    "    for i,f_id in enumerate(dataset['file_id']):\n",
    "        if f_id in file_id:\n",
    "            idx.append(i)\n",
    "    return idx,{k:dataset[k][idx] for k in dataset.keys()}\n",
    "\n",
    "def concat_data(data1,data2):\n",
    "    return {k:np.concatenate([data1[k],data2[k]],axis=0) for k in data1.keys()}\n",
    "\n",
    "\n",
    "def filter_samples(data,distance_thres,pred,score_thres):\n",
    "    condi1 = data['min_len'] < distance_thres\n",
    "    condi2 = ((pred > score_thres) + (pred < (1-score_thres))).astype(bool)\n",
    "    condi = condi2 * condi1\n",
    "    return {k:v[condi] for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K79ZdXczuHIs",
   "metadata": {
    "id": "K79ZdXczuHIs"
   },
   "source": [
    "## 2.2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fFg2CT6uJdp",
   "metadata": {
    "id": "7fFg2CT6uJdp"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "import keras.backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.optimizers import Optimizer\n",
    "\n",
    "\n",
    "class M_Nadam(Optimizer):\n",
    "  def __init__(self, lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004, multipliers=None,**kwargs):\n",
    "    super(M_Nadam, self).__init__(**kwargs)\n",
    "    with K.name_scope(self.__class__.__name__):\n",
    "        self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "        self.m_schedule = K.variable(1., name='m_schedule')\n",
    "        self.lr = K.variable(lr, name='lr')\n",
    "        self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "        self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "    if epsilon is None:\n",
    "        epsilon = K.epsilon()\n",
    "    self.epsilon = epsilon\n",
    "    self.schedule_decay = schedule_decay\n",
    "\n",
    "    if multipliers is None:\n",
    "        multipliers = {}\n",
    "    self.lr_multipliers = multipliers\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "        # Due to the recommendations in [2], i.e. warming momentum schedule\n",
    "        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n",
    "        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n",
    "        m_schedule_new = self.m_schedule * momentum_cache_t\n",
    "        m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n",
    "        self.updates.append((self.m_schedule, m_schedule_new))\n",
    "\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        ms = [K.zeros(shape) for shape in shapes]\n",
    "        vs = [K.zeros(shape) for shape in shapes]\n",
    "\n",
    "        self.weights = [self.iterations] + ms + vs\n",
    "\n",
    "        lr = self.lr\n",
    "\n",
    "        for p, g, m, v in zip(params, grads, ms, vs):\n",
    "\n",
    "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
    "            if matched_layer:\n",
    "                new_lr = lr * self.lr_multipliers[matched_layer[0]]\n",
    "            else:\n",
    "                new_lr = lr\n",
    "\n",
    "            # the following equations given in [1]\n",
    "            g_prime = g / (1. - m_schedule_new)\n",
    "            m_t = self.beta_1 * m + (1. - self.beta_1) * g\n",
    "            m_t_prime = m_t / (1. - m_schedule_next)\n",
    "            v_t = self.beta_2 * v + (1. - self.beta_2) * K.square(g)\n",
    "            v_t_prime = v_t / (1. - K.pow(self.beta_2, t))\n",
    "            m_t_bar = (1. - momentum_cache_t) * g_prime + momentum_cache_t_1 * m_t_prime\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "\n",
    "            p_t = p - new_lr * m_t_bar / (K.sqrt(v_t_prime) + self.epsilon)\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'schedule_decay': self.schedule_decay}\n",
    "        base_config = super(M_Nadam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class M_Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,\n",
    "                 epsilon=None, decay=0., amsgrad=False, multipliers=None,**kwargs):\n",
    "        super(M_Adam, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "        if multipliers is None:\n",
    "            multipliers = {}\n",
    "        self.lr_multipliers = multipliers\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /\n",
    "                     (1. - K.pow(self.beta_1, t)))\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "\n",
    "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
    "            if matched_layer:\n",
    "                new_lr = lr_t * self.lr_multipliers[matched_layer[0]]\n",
    "            else:\n",
    "                new_lr = lr_t\n",
    "\n",
    "\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - new_lr * m_t / (K.sqrt(vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - new_lr * m_t / (K.sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(M_Adam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class M_RMSprop(Optimizer):\n",
    "    def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0.,multipliers=None,\n",
    "                 **kwargs):\n",
    "        super(M_RMSprop, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.lr = K.variable(lr, name='lr')\n",
    "            self.rho = K.variable(rho, name='rho')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "\n",
    "        if multipliers is None:\n",
    "            multipliers = {}\n",
    "        self.lr_multipliers = multipliers\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        self.weights = accumulators\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        for p, g, a in zip(params, grads, accumulators):\n",
    "\n",
    "            matched_layer = [x for x in self.lr_multipliers.keys() if x in p.name]\n",
    "            if matched_layer:\n",
    "                new_lr = lr * self.lr_multipliers[matched_layer[0]]\n",
    "            else:\n",
    "                new_lr = lr\n",
    "\n",
    "            # update accumulator\n",
    "            new_a = self.rho * a + (1. - self.rho) * K.square(g)\n",
    "            self.updates.append(K.update(a, new_a))\n",
    "            new_p = p - new_lr * g / (K.sqrt(new_a) + self.epsilon)\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.lr)),\n",
    "                  'rho': float(K.get_value(self.rho)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon}\n",
    "        base_config = super(M_RMSprop, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "\n",
    "def f1_metric(y_true,y_pred):\n",
    "    y_true = K.max(y_true, axis=1)\n",
    "    y_pred = K.max(y_pred, axis=1)\n",
    "    y_pred = K.round(y_pred)\n",
    "    return 2*K.sum(y_pred*y_true)/(K.sum(y_true)+K.sum(y_pred)+1e-12)\n",
    "\n",
    "def relation_attention(inputs,text):\n",
    "\n",
    "    def share_layer(feats,f):\n",
    "        return [f(x) for x in feats]\n",
    "\n",
    "    e1_mask = Lambda(lambda x: K.expand_dims(K.cast(K.greater(x, 0), 'float32'), axis=1))(inputs['seg'])\n",
    "    e2_mask = Lambda(lambda x: K.expand_dims(K.cast(K.less(x, 0), 'float32'), axis=1))(inputs['seg'])\n",
    "    e1 = Lambda(lambda x: K.batch_dot(x[0], x[1]) / K.sum(x[0], keepdims=True))([e1_mask, text])\n",
    "    e2 = Lambda(lambda x: K.batch_dot(x[0], x[1]) / K.sum(x[0], keepdims=True))([e2_mask, text])\n",
    "\n",
    "\n",
    "    encode_feat = [e2,e1,text]\n",
    "    # encode_feat = share_layer(encode_feat,Conv1D(256,1,activation='relu'))\n",
    "    encode_feat = share_layer(encode_feat,Conv1D(256,1,activation='tanh'))\n",
    "\n",
    "    atte = Lambda(lambda x:(x[2]-(x[1]+x[0])))(encode_feat)\n",
    "    atte = Dense(1)(atte)\n",
    "    atte = Flatten()(atte)\n",
    "\n",
    "    atte = Activation('softmax')(atte)\n",
    "    atte = Reshape((1,-1))(atte)\n",
    "    text = Lambda(lambda x: K.batch_dot(x[0], x[1]))([atte,text])\n",
    "    text = Flatten()(text)\n",
    "\n",
    "\n",
    "    e1 = Flatten()(e1)\n",
    "    e2 = Flatten()(e2)\n",
    "    return text,e1,e2\n",
    "\n",
    "def Input_moudle(cfg):\n",
    "    x_in = Input((cfg['maxlen'],), name='text')\n",
    "    pt_in = Input((cfg['maxlen'],), name='flag')\n",
    "    pe_in = Input((cfg['maxlen'],2),name='position')\n",
    "    seg_in = Input((cfg['maxlen'],), name='segment')\n",
    "    len_in = Input((1,), name='min_len')\n",
    "    prior_in = Input((10,), name='mask')\n",
    "    pos_tag = Embedding(cfg['num_pg'], 16, name='embpos')(pt_in)\n",
    "    emb = Embedding(cfg['num_word'], cfg['word_dim'], trainable=True, name='emb')\n",
    "\n",
    "\n",
    "\n",
    "    seg = Lambda(lambda x:K.expand_dims(x))(seg_in)\n",
    "    x = emb(x_in)\n",
    "    x = concatenate([x, pe_in, pos_tag], axis=-1)\n",
    "    x = add([x,seg])\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "\n",
    "    inputs = {\n",
    "        'text': x_in,\n",
    "        'pos_tag': pt_in,\n",
    "        'position': pe_in,\n",
    "        'mask': prior_in,\n",
    "        'seg': seg_in,\n",
    "        'len': len_in,\n",
    "    }\n",
    "\n",
    "    if cfg['use_adj_feat']:\n",
    "        e1e2_in = Input((4,), name='e1e2')\n",
    "        e2e1_in = Input((4,), name='e2e1')\n",
    "        e1dist_in = Input((4,), name='e1dist')\n",
    "        e2dist_in = Input((4,), name='e2dist')\n",
    "        inputs.update({\n",
    "            'e2e1': e2e1_in,\n",
    "            'e1e2': e1e2_in,\n",
    "            'e1dist': e1dist_in,\n",
    "            'e2dist': e2dist_in,\n",
    "        })\n",
    "\n",
    "    return x,inputs\n",
    "\n",
    "def encoder(x,encode_name,inputs,cfg):\n",
    "    if encode_name == 'gru':\n",
    "        x = Bidirectional(CuDNNGRU(cfg['unit1'], return_sequences=True, name='gru1'), merge_mode='sum')(x)\n",
    "        x = Bidirectional(CuDNNGRU(cfg['unit2'], return_sequences=True, name='gru2'), merge_mode='sum')(x)\n",
    "    elif encode_name == 'lstmgru':\n",
    "        x = Bidirectional(CuDNNLSTM(cfg['unit1'], return_sequences=True, name='gru1'), merge_mode='sum')(x)\n",
    "        x = Bidirectional(CuDNNGRU(cfg['unit2'], return_sequences=True, name='gru2'), merge_mode='sum')(x)\n",
    "    elif encode_name == 'lstm':\n",
    "        x = Bidirectional(CuDNNLSTM(cfg['unit1'], return_sequences=True, name='gru1'), merge_mode='sum')(x)\n",
    "        x = Bidirectional(CuDNNLSTM(cfg['unit2'], return_sequences=True, name='gru2'), merge_mode='sum')(x)\n",
    "    elif encode_name == 'grulstm':\n",
    "        x = Bidirectional(CuDNNGRU(cfg['unit2'], return_sequences=True, name='gru2'), merge_mode='sum')(x)\n",
    "        x = Bidirectional(CuDNNLSTM(cfg['unit2'], return_sequences=True, name='gru2'), merge_mode='sum')(x)\n",
    "    else:\n",
    "        raise RuntimeError(\"no this encode\")\n",
    "    return x\n",
    "\n",
    "def rnn_model(cfg):\n",
    "\n",
    "    def bce_loss(y_true, y_pred):\n",
    "        y_true = K.max(y_true, axis=1)\n",
    "        y_pred = K.max(y_pred, axis=1)\n",
    "        return -K.mean(cfg['alpha']*y_true*K.log(y_pred)+(1-cfg['alpha'])*(1-y_true)*K.log(1-y_pred))\n",
    "\n",
    "\n",
    "    x,inputs = Input_moudle(cfg)\n",
    "    x = encoder(x,cfg['encode_name'],inputs,cfg)\n",
    "\n",
    "    weighted_pool,e1,e2 = relation_attention(inputs,x)\n",
    "    weighted_pool = Dropout(0.2)(weighted_pool)\n",
    "\n",
    "    len_feat = BatchNormalization()(inputs['len'])\n",
    "    feat = [weighted_pool,len_feat]\n",
    "    if cfg['use_adj_feat']:\n",
    "        prob_feat = concatenate([inputs['e2e1'], inputs['e1e2']])\n",
    "        feat.append(prob_feat)\n",
    "    merge = concatenate(feat,axis=1)\n",
    "\n",
    "    output = Dense(256, activation='relu',name='d1')(merge)\n",
    "    output = Dense(256, activation='relu',name='d2')(output)\n",
    "    output = Dense(10,activation='sigmoid',name='d3')(output)\n",
    "    output = multiply([inputs['mask'],output])\n",
    "\n",
    "\n",
    "    lr_dict = {\n",
    "        'emb':cfg['emb'],\n",
    "        'embpos':cfg['emb'],\n",
    "    }\n",
    "    model = Model(list(inputs.values()), output)\n",
    "    model.compile(loss=bce_loss, optimizer=M_Nadam(cfg['lr'],multipliers=lr_dict),metrics=[f1_metric])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16qZwMAnvpxb",
   "metadata": {
    "id": "16qZwMAnvpxb"
   },
   "source": [
    "## 2.3 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93G_05titQbV",
   "metadata": {
    "id": "93G_05titQbV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import getopt\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from model import model_2c_base\n",
    "from model.model_2c_ensemble import EnsembleModel\n",
    "from sklearn.metrics import f1_score\n",
    "from branch_stdout import *\n",
    "from dataset_ss import DataSet\n",
    "import relations_builder_2c as rb\n",
    "import pickle\n",
    "\n",
    "\n",
    "default_config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"display_step\": 60,\n",
    "    \"test_step\": 300,\n",
    "    \"batch_size\": 24,\n",
    "    \"test_batch_size\": 600,\n",
    "    \"module_dir\": 'Demo/DataSets/s{s}_{v}',\n",
    "    \"style\": None,\n",
    "    \"version\": \"1\",\n",
    "    \"k_flod\": 1,\n",
    "    \"max_step\": 3000,\n",
    "}\n",
    "\n",
    "\n",
    "def run(config, ds):\n",
    "    learning_rate = config['learning_rate']\n",
    "    display_step = config['display_step']\n",
    "    test_step = config['test_step']\n",
    "    batch_size = config['batch_size']\n",
    "    test_batch_size = config['test_batch_size']\n",
    "    module_dir = config['module_dir']\n",
    "    style = config['style']\n",
    "    version = config['version']\n",
    "    k_flod = config['k_flod']\n",
    "    max_step = config['max_step']\n",
    "    \n",
    "    module_dir = module_dir.replace(\"{s}\", style + \"_k\" + str(k_flod)).replace(\"{v}\", \"v\" + version)\n",
    "    if not os.path.isdir(module_dir):\n",
    "        os.mkdir(module_dir)\n",
    "    cp_path = module_dir\n",
    "\n",
    "    log_file = module_dir + '/train_log_' + datetime.datetime.now().strftime('%Y%m%d_%H%M%S') + '.log'\n",
    "    sys.stdout = BranchStdout(log_file)\n",
    "\n",
    "    reversed_relation_dict = dict(map(lambda x: (x[1], x[0]), ds.relation_dict.items()))\n",
    "\n",
    "    m = EnsembleModel(max_train_seq_len=ds.max_train_seq_len,\n",
    "                      max_test_seq_len=ds.max_test_seq_len,\n",
    "                      max_train_rel_count=ds.max_train_rel_count,\n",
    "                      max_test_rel_count=ds.max_test_rel_count)\n",
    "\n",
    "    saver_best = []\n",
    "    for i in range(k_flod):\n",
    "        model_name = \"model_\" + str(i + 1) + \"_\" + style\n",
    "        cm = model_2c_base.Model_board(max_train_seq_len=ds.max_train_seq_len,\n",
    "                                       max_test_seq_len=ds.max_test_seq_len,\n",
    "                                       max_train_rel_count=ds.max_train_rel_count,\n",
    "                                       max_test_rel_count=ds.max_test_rel_count,\n",
    "                                       ensemble_model=m,\n",
    "                                       style=style,\n",
    "                                       name=model_name)\n",
    "        m.children_models.append(cm)\n",
    "        sc_file = os.path.join(cp_path, \"model_\" + str(i + 1) + \".sc\")\n",
    "        if os.path.isfile(sc_file):\n",
    "            with open(sc_file, 'rb') as f:\n",
    "                sc_obj = pickle.load(f)\n",
    "                sc_val = sc_obj[0]\n",
    "        else:\n",
    "            sc_val = 0.0\n",
    "        saver_best.append([tf.train.Saver(var_list=cm.get_saving_variables()),\n",
    "                           os.path.join(cp_path, model_name + \"_best\"),\n",
    "                           sc_val])\n",
    "\n",
    "    with m.graph.as_default():\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        training_ops = [[optimizer.minimize(m.children_models[j].cost,\n",
    "                                            var_list=m.children_models[j].get_trainable_variables())\n",
    "                         for j in range(k_flod) if j != i] + [m.get_train_update_ops()]\n",
    "                        for i in range(k_flod)]\n",
    "        watching_ops = [[(m.children_models[j].train_cost, m.children_models[j].regularization_cost,\n",
    "                          m.children_models[j].cost, m.children_models[j].pred)\n",
    "                         for j in range(k_flod) if j != i]\n",
    "                        for i in range(k_flod)]\n",
    "        message_template = ['\\n'.join([\"M\" + str(j + 1) + \", Minibatch Loss={\" +\n",
    "                                       str(j * 4 if j < i else (j - 1) * 4) + \":f}/{\" +\n",
    "                                       str((j * 4 if j < i else (j - 1) * 4) + 1) + \":f}/{\" +\n",
    "                                       str((j * 4 if j < i else (j - 1) * 4) + 2) + \":f}, F1={\" +\n",
    "                                       str((j * 4 if j < i else (j - 1) * 4) + 3) + \":f}\"\n",
    "                                       for j in range(k_flod) if j != i])\n",
    "                            for i in range(k_flod)]\n",
    "        test_watching_ops = [(m.children_models[j].test_cost,\n",
    "                              m.children_models[j].test_pred,\n",
    "                              m.children_models[j].test_score)\n",
    "                             for j in range(k_flod)]\n",
    "\n",
    "        with tf.Session(graph=m.graph) as sess:\n",
    "            tf.set_random_seed(20140630)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            print(\"learning_rate=\", learning_rate)\n",
    "            print(\"batch_size=\", batch_size)\n",
    "            print(\"seq_mask_rate\", ds.seq_mask_rate)\n",
    "            print(\"max_rel_distance=\", ds.max_rel_distance)\n",
    "            print(\"min_rel_distance=\", ds.min_rel_distance)\n",
    "            print(\"split_size=\", ds.split_size)\n",
    "\n",
    "            step = 0\n",
    "            ds.reset_train()\n",
    "            epoch_steps = math.ceil(ds.train_count / batch_size)\n",
    "            sess_time_in_display = 0\n",
    "            valid_labels = [1]\n",
    "\n",
    "            while max_step > 0 and step < max_step:\n",
    "                es = step % epoch_steps\n",
    "                step += 1\n",
    "\n",
    "                batch_data_seq, batch_data_len, \\\n",
    "                batch_data_rel, batch_data_rsl, batch_doc, batch_len = \\\n",
    "                    ds.next_train(batch_size=batch_size)\n",
    "                batch_data_lbl = np.cast[np.float32](batch_data_rel[:, :, 0] > 0)\n",
    "\n",
    "                if step % display_step == 0:\n",
    "                    st = time.time()\n",
    "                    w_v, _ = \\\n",
    "                        sess.run([watching_ops[es % k_flod], training_ops[es % k_flod]],\n",
    "                                 feed_dict={\n",
    "                                     m.xci: batch_data_seq[:, :, 0],\n",
    "                                     m.xtc: batch_data_seq[:, :, 1],\n",
    "                                     m.xsl: batch_data_len,\n",
    "                                     m.rel: batch_data_rel[:, :, 1:3],\n",
    "                                     m.lbl: batch_data_lbl,\n",
    "                                     m.rsl: batch_data_rsl\n",
    "                                 })\n",
    "                    sess_time_in_step = time.time() - st\n",
    "                    sess_time_in_display += sess_time_in_step\n",
    "                    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \"Iter \" + str(step))\n",
    "                    print(message_template[es % k_flod].format(\n",
    "                        *[v for sl in\n",
    "                          [[w_v[i][0], w_v[i][1], w_v[i][2],\n",
    "                            f1_score(batch_data_lbl.reshape([-1]), w_v[i][3].reshape([-1]),\n",
    "                                     labels=valid_labels,\n",
    "                                     average=\"micro\")] for i in range(k_flod - 1)]\n",
    "                          for v in sl]\n",
    "                    ))\n",
    "                    print(\"Sess Time={0:f}/{1:f}\".format(sess_time_in_step, sess_time_in_display))\n",
    "                    sess_time_in_display = 0\n",
    "                    sys.stdout.flush()\n",
    "                else:\n",
    "                    st = time.time()\n",
    "                    sess.run(training_ops[es % k_flod], feed_dict={m.xci: batch_data_seq[:, :, 0],\n",
    "                                                                   m.xtc: batch_data_seq[:, :, 1],\n",
    "                                                                   m.xsl: batch_data_len,\n",
    "                                                                   m.rel: batch_data_rel[:, :, 1:3],\n",
    "                                                                   m.lbl: batch_data_lbl,\n",
    "                                                                   m.rsl: batch_data_rsl})\n",
    "                    sess_time_in_display += time.time() - st\n",
    "\n",
    "                if step % test_step == 0:\n",
    "                    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \"Iter \" + str(step))\n",
    "                    for i in range(k_flod):\n",
    "                        epoch_test_stat = [0.0, 0.0, 0.0, 0.0]\n",
    "                        epoch_test_count = 0\n",
    "\n",
    "                        all_data_seq, all_data_len, \\\n",
    "                        all_data_rel, all_data_rsl, all_doc, all_entities, all_len = \\\n",
    "                            ds.next_flod_test(i, k_flod, batch_size)\n",
    "                        all_data_lbl = np.cast[np.float32](all_data_rel[:, :, 0] > 0)\n",
    "\n",
    "                        j = 0\n",
    "                        all_pred_score = np.zeros((all_len, ds.all_regular_rel_count), dtype=np.float32)\n",
    "                        while j < len(all_data_seq):\n",
    "                            batch_data_seq = all_data_seq[j:j + test_batch_size]\n",
    "                            batch_data_len = all_data_len[j:j + test_batch_size]\n",
    "                            batch_data_rel = all_data_rel[j:j + test_batch_size]\n",
    "                            batch_data_rsl = all_data_rsl[j:j + test_batch_size]\n",
    "                            batch_data_lbl = all_data_lbl[j:j + test_batch_size]\n",
    "\n",
    "                            w_v = \\\n",
    "                                sess.run(test_watching_ops,\n",
    "                                         feed_dict={\n",
    "                                             m.test_xci: batch_data_seq[:, :, 0],\n",
    "                                             m.test_xtc: batch_data_seq[:, :, 1],\n",
    "                                             m.test_xsl: batch_data_len,\n",
    "                                             m.test_rel: batch_data_rel[:, :, 1:3],\n",
    "                                             m.test_lbl: batch_data_lbl,\n",
    "                                             m.test_rsl: batch_data_rsl\n",
    "                                         })\n",
    "\n",
    "                            epoch_test_stat[0] += f1_score(batch_data_lbl.reshape([-1]), w_v[i][1].reshape([-1]),\n",
    "                                                           labels=valid_labels,\n",
    "                                                           average=\"micro\")\n",
    "                            epoch_test_stat[1] += w_v[i][0]\n",
    "                            epoch_test_count += 1\n",
    "                            if j + test_batch_size <= len(all_data_seq):\n",
    "                                all_pred_score[j:j + test_batch_size, :] = w_v[i][2]\n",
    "                            else:\n",
    "                                all_pred_score[j:, :] = w_v[i][2]\n",
    "                            j += test_batch_size\n",
    "\n",
    "                        raw_relations_p = rb.build_raw_relations(rb.build_relations_from_data(\n",
    "                            doc=all_doc,\n",
    "                            filenames=ds.train_filenames,\n",
    "                            entities=all_entities,\n",
    "                            rel=all_data_rel,\n",
    "                            rsl=all_data_rsl,\n",
    "                            pred_score=all_pred_score,\n",
    "                            relation_labels=reversed_relation_dict))\n",
    "\n",
    "                        raw_relations_f = ds.get_raw_relations_of_flod_k(i, k_flod, batch_size)\n",
    "                        cr = rb.compare_result(raw_relations_p, raw_relations_f)\n",
    "                        model_score = cr['micro']['F1']\n",
    "\n",
    "                        print(\"M\" + str(i + 1),\n",
    "                              \", Test Loss={0:f}\".format(epoch_test_stat[1] / epoch_test_count),\n",
    "                              \", F1={0:f}\".format(epoch_test_stat[0] / epoch_test_count),\n",
    "                              \", Micro[F1={0:f}|P={1:f}|R={2:f}]\".format(cr['micro']['F1'],\n",
    "                                                                         cr['micro']['P'],\n",
    "                                                                         cr['micro']['R']),\n",
    "                              \", Macro[F1={0:f}|P={1:f}|R={2:f}]\".format(cr['macro']['F1'],\n",
    "                                                                         cr['macro']['P'],\n",
    "                                                                         cr['macro']['R']))\n",
    "\n",
    "                        if model_score > saver_best[i][2]:\n",
    "                            saver_best[i][0].save(sess, saver_best[i][1])\n",
    "                            sc_file = os.path.join(cp_path, \"model_\" + str(i + 1) + \".sc\")\n",
    "                            with open(sc_file, 'wb') as f:\n",
    "                                pickle.dump((model_score, cr['micro'], cr['macro']), f)\n",
    "                            saver_best[i][2] = model_score\n",
    "                            print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                                  'Got best model of model', str(i + 1), '-', model_score)\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    stdout = sys.stdout\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \"Run Start\")\n",
    "    config = default_config.copy()\n",
    "    data_path = 'Demo/DataSets'\n",
    "\n",
    "    try:\n",
    "        opts, args = getopt.getopt(sys.argv[1:], \"d:r:s:v:k:\",\n",
    "                                   [\"dpath=\", \"mdir=\", \"style=\", \"version=\", \"k-flod=\", \"max-step=\"])\n",
    "    except getopt.GetoptError:\n",
    "        print('arguments error')\n",
    "        sys.exit(2)\n",
    "\n",
    "    for opt, arg in opts:\n",
    "        if opt in (\"-d\", \"--dpath\"):\n",
    "            data_path = arg\n",
    "        elif opt in (\"-r\", \"--mdir\"):\n",
    "            config['module_dir'] = arg\n",
    "        elif opt in (\"-v\", \"--version\"):\n",
    "            config['version'] = arg\n",
    "        elif opt in (\"-k\", \"--k-flod\"):\n",
    "            config['k_flod'] = int(arg)\n",
    "        elif opt in (\"--max-step\",):\n",
    "            config['max_step'] = int(arg)\n",
    "\n",
    "    np.random.seed(20140630)\n",
    "    ts = DataSet(os.path.join(data_path, 'ruijin_round2_train/ruijin_round2_train'),\n",
    "                 test_size=0,\n",
    "                 seq_mask_rate=0.05,\n",
    "                 min_rel_distance=0,\n",
    "                 max_rel_distance=70,\n",
    "                 max_split_rel_count=139,\n",
    "                 all_regular_rel_count=260,\n",
    "                 split_size=140,\n",
    "                 worker_count=4,\n",
    "                 capacity=240,\n",
    "                 return_entity_detail_id=False)\n",
    "\n",
    "    config[\"style\"] = \"1\"\n",
    "    run(config, ts)\n",
    "    sys.stdout = stdout\n",
    "    config[\"style\"] = \"6\"\n",
    "    run(config, ts)\n",
    "    print(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \"Finished\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TYNNhoQVi9Z_",
    "368Gs3sWHpzN",
    "ok-ehkq4i-F7",
    "HGBGEjByc1D0",
    "y31c5lFhfRYj",
    "Bki2fl6Ke3pG",
    "d3KjUg8wttDZ"
   ],
   "name": "nlp_relation_extraction_rujun2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
