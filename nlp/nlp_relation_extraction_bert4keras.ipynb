{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zhousanfu/NLP_relation_extraction/blob/master/nlp_relation_extraction_bert4keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqaPTbYUZdfv"
   },
   "outputs": [],
   "source": [
    "!mkdir bert_data\n",
    "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
    "!unzip chinese_L-12_H-768_A-12.zip\n",
    "!mv chinese_L-12_H-768_A-12 bert_data\n",
    "!wget https://github.com/zhousanfu/NLP_relation_extraction/raw/master/data/nlp_relation_extraction_bert4keras.zip\n",
    "!unzip nlp_relation_extraction_bert4keras.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH9B1asFrGC2"
   },
   "outputs": [],
   "source": [
    "!pip install bert4keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "VBE-7AR1XCPO",
    "outputId": "1a8873b8-e2c7-403a-ed56-5de9db5ec98b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cd056beac5d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/bert4keras/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keras'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m'2.0.0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mBaseMeanIoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanIoU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Import the target module and insert it into the parent's namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.api'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "# 三元组抽取任务，基于“半指针-半标注”结构\n",
    "# 文章介绍：https://kexue.fm/archives/7161\n",
    "# 数据集：http://ai.baidu.com/broad/download?dataset=sked\n",
    "# 最优f1=0.82198\n",
    "# 换用RoBERTa Large可以达到f1=0.829+\n",
    "# 说明：由于使用了EMA，需要跑足够多的步数(5000步以上）才生效，如果\n",
    "#      你的数据总量比较少，那么请务必跑足够多的epoch数，或者去掉EMA。\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K, batch_gather\n",
    "from bert4keras.layers import Loss\n",
    "from bert4keras.layers import LayerNormalization\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, to_array\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "maxlen = 128\n",
    "batch_size = 64\n",
    "config_path = '/content/bert_data/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = '/content/bert_data/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '/content/bert_data/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：{'text': text, 'spo_list': [(s, p, o)]}\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            l = json.loads(l)\n",
    "            D.append({\n",
    "                'text': l['text'],\n",
    "                'spo_list': [(spo['subject'], spo['predicate'], spo['object'])\n",
    "                             for spo in l['spo_list']]\n",
    "            })\n",
    "    return D\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "train_data = load_data('/root/kg/datasets/train_data.json')\n",
    "valid_data = load_data('/root/kg/datasets/dev_data.json')\n",
    "predicate2id, id2predicate = {}, {}\n",
    "\n",
    "with open('/root/kg/datasets/all_50_schemas') as f:\n",
    "    for l in f:\n",
    "        l = json.loads(l)\n",
    "        if l['predicate'] not in predicate2id:\n",
    "            id2predicate[len(predicate2id)] = l['predicate']\n",
    "            predicate2id[l['predicate']] = len(predicate2id)\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "\n",
    "def search(pattern, sequence):\n",
    "    \"\"\"从sequence中寻找子串pattern\n",
    "    如果找到，返回第一个下标；否则返回-1。\n",
    "    \"\"\"\n",
    "    n = len(pattern)\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i + n] == pattern:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(d['text'], maxlen=maxlen)\n",
    "            # 整理三元组 {s: [(o, p)]}\n",
    "            spoes = {}\n",
    "            for s, p, o in d['spo_list']:\n",
    "                s = tokenizer.encode(s)[0][1:-1]\n",
    "                p = predicate2id[p]\n",
    "                o = tokenizer.encode(o)[0][1:-1]\n",
    "                s_idx = search(s, token_ids)\n",
    "                o_idx = search(o, token_ids)\n",
    "                if s_idx != -1 and o_idx != -1:\n",
    "                    s = (s_idx, s_idx + len(s) - 1)\n",
    "                    o = (o_idx, o_idx + len(o) - 1, p)\n",
    "                    if s not in spoes:\n",
    "                        spoes[s] = []\n",
    "                    spoes[s].append(o)\n",
    "            if spoes:\n",
    "                # subject标签\n",
    "                subject_labels = np.zeros((len(token_ids), 2))\n",
    "                for s in spoes:\n",
    "                    subject_labels[s[0], 0] = 1\n",
    "                    subject_labels[s[1], 1] = 1\n",
    "                # 随机选一个subject（这里没有实现错误！这就是想要的效果！！）\n",
    "                start, end = np.array(list(spoes.keys())).T\n",
    "                start = np.random.choice(start)\n",
    "                end = np.random.choice(end[end >= start])\n",
    "                subject_ids = (start, end)\n",
    "                # 对应的object标签\n",
    "                object_labels = np.zeros((len(token_ids), len(predicate2id), 2))\n",
    "                for o in spoes.get(subject_ids, []):\n",
    "                    object_labels[o[0], o[2], 0] = 1\n",
    "                    object_labels[o[1], o[2], 1] = 1\n",
    "                # 构建batch\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_subject_labels.append(subject_labels)\n",
    "                batch_subject_ids.append(subject_ids)\n",
    "                batch_object_labels.append(object_labels)\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                    batch_subject_labels = sequence_padding(\n",
    "                        batch_subject_labels\n",
    "                    )\n",
    "                    batch_subject_ids = np.array(batch_subject_ids)\n",
    "                    batch_object_labels = sequence_padding(batch_object_labels)\n",
    "                    yield [\n",
    "                        batch_token_ids, batch_segment_ids,\n",
    "                        batch_subject_labels, batch_subject_ids,\n",
    "                        batch_object_labels\n",
    "                    ], None\n",
    "                    batch_token_ids, batch_segment_ids = [], []\n",
    "                    batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "\n",
    "\n",
    "def extract_subject(inputs):\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "    \"\"\"\n",
    "    output, subject_ids = inputs\n",
    "    start = batch_gather(output, subject_ids[:, :1])\n",
    "    end = batch_gather(output, subject_ids[:, 1:])\n",
    "    subject = K.concatenate([start, end], 2)\n",
    "    return subject[:, 0]\n",
    "\n",
    "\n",
    "# 补充输入\n",
    "subject_labels = Input(shape=(None, 2), name='Subject-Labels')\n",
    "subject_ids = Input(shape=(2,), name='Subject-Ids')\n",
    "object_labels = Input(shape=(None, len(predicate2id), 2), name='Object-Labels')\n",
    "\n",
    "# 加载预训练模型\n",
    "bert = build_transformer_model(\n",
    "    config_path=config_path,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    return_keras_model=False,\n",
    ")\n",
    "\n",
    "# 预测subject\n",
    "output = Dense(\n",
    "    units=2, activation='sigmoid', kernel_initializer=bert.initializer\n",
    ")(bert.model.output)\n",
    "subject_preds = Lambda(lambda x: x**2)(output)\n",
    "\n",
    "subject_model = Model(bert.model.inputs, subject_preds)\n",
    "\n",
    "# 传入subject，预测object\n",
    "# 通过Conditional Layer Normalization将subject融入到object的预测中\n",
    "output = bert.model.layers[-2].get_output_at(-1)  # 自己想为什么是-2而不是-1\n",
    "subject = Lambda(extract_subject)([output, subject_ids])\n",
    "output = LayerNormalization(conditional=True)([output, subject])\n",
    "output = Dense(\n",
    "    units=len(predicate2id) * 2,\n",
    "    activation='sigmoid',\n",
    "    kernel_initializer=bert.initializer\n",
    ")(output)\n",
    "output = Lambda(lambda x: x**4)(output)\n",
    "object_preds = Reshape((-1, len(predicate2id), 2))(output)\n",
    "\n",
    "object_model = Model(bert.model.inputs + [subject_ids], object_preds)\n",
    "\n",
    "\n",
    "class TotalLoss(Loss):\n",
    "    \"\"\"subject_loss与object_loss之和，都是二分类交叉熵\n",
    "    \"\"\"\n",
    "    def compute_loss(self, inputs, mask=None):\n",
    "        subject_labels, object_labels = inputs[:2]\n",
    "        subject_preds, object_preds, _ = inputs[2:]\n",
    "        if mask[4] is None:\n",
    "            mask = 1.0\n",
    "        else:\n",
    "            mask = K.cast(mask[4], K.floatx())\n",
    "        # sujuect部分loss\n",
    "        subject_loss = K.binary_crossentropy(subject_labels, subject_preds)\n",
    "        subject_loss = K.mean(subject_loss, 2)\n",
    "        subject_loss = K.sum(subject_loss * mask) / K.sum(mask)\n",
    "        # object部分loss\n",
    "        object_loss = K.binary_crossentropy(object_labels, object_preds)\n",
    "        object_loss = K.sum(K.mean(object_loss, 3), 2)\n",
    "        object_loss = K.sum(object_loss * mask) / K.sum(mask)\n",
    "        # 总的loss\n",
    "        return subject_loss + object_loss\n",
    "\n",
    "\n",
    "subject_preds, object_preds = TotalLoss([2, 3])([\n",
    "    subject_labels, object_labels, subject_preds, object_preds,\n",
    "    bert.model.output\n",
    "])\n",
    "\n",
    "# 训练模型\n",
    "train_model = Model(\n",
    "    bert.model.inputs + [subject_labels, subject_ids, object_labels],\n",
    "    [subject_preds, object_preds]\n",
    ")\n",
    "\n",
    "AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
    "optimizer = AdamEMA(learning_rate=1e-5)\n",
    "train_model.compile(optimizer=optimizer)\n",
    "\n",
    "\n",
    "def extract_spoes(text):\n",
    "    \"\"\"抽取输入text所包含的三元组\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "    # 抽取subject\n",
    "    subject_preds = subject_model.predict([token_ids, segment_ids])\n",
    "    subject_preds[:, [0, -1]] *= 0\n",
    "    start = np.where(subject_preds[0, :, 0] > 0.6)[0]\n",
    "    end = np.where(subject_preds[0, :, 1] > 0.5)[0]\n",
    "    subjects = []\n",
    "    for i in start:\n",
    "        j = end[end >= i]\n",
    "        if len(j) > 0:\n",
    "            j = j[0]\n",
    "            subjects.append((i, j))\n",
    "    if subjects:\n",
    "        spoes = []\n",
    "        token_ids = np.repeat(token_ids, len(subjects), 0)\n",
    "        segment_ids = np.repeat(segment_ids, len(subjects), 0)\n",
    "        subjects = np.array(subjects)\n",
    "        # 传入subject，抽取object和predicate\n",
    "        object_preds = object_model.predict([token_ids, segment_ids, subjects])\n",
    "        object_preds[:, [0, -1]] *= 0\n",
    "        for subject, object_pred in zip(subjects, object_preds):\n",
    "            start = np.where(object_pred[:, :, 0] > 0.6)\n",
    "            end = np.where(object_pred[:, :, 1] > 0.5)\n",
    "            for _start, predicate1 in zip(*start):\n",
    "                for _end, predicate2 in zip(*end):\n",
    "                    if _start <= _end and predicate1 == predicate2:\n",
    "                        spoes.append(\n",
    "                            ((mapping[subject[0]][0],\n",
    "                              mapping[subject[1]][-1]), predicate1,\n",
    "                             (mapping[_start][0], mapping[_end][-1]))\n",
    "                        )\n",
    "                        break\n",
    "        return [(text[s[0]:s[1] + 1], id2predicate[p], text[o[0]:o[1] + 1])\n",
    "                for s, p, o, in spoes]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "class SPO(tuple):\n",
    "    \"\"\"用来存三元组的类\n",
    "    表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，\n",
    "    使得在判断两个三元组是否等价时容错性更好。\n",
    "    \"\"\"\n",
    "    def __init__(self, spo):\n",
    "        self.spox = (\n",
    "            tuple(tokenizer.tokenize(spo[0])),\n",
    "            spo[1],\n",
    "            tuple(tokenizer.tokenize(spo[2])),\n",
    "        )\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.spox.__hash__()\n",
    "\n",
    "    def __eq__(self, spo):\n",
    "        return self.spox == spo.spox\n",
    "\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"评估函数，计算f1、precision、recall\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    f = open('dev_pred.json', 'w', encoding='utf-8')\n",
    "    pbar = tqdm()\n",
    "    for d in data:\n",
    "        R = set([SPO(spo) for spo in extract_spoes(d['text'])])\n",
    "        T = set([SPO(spo) for spo in d['spo_list']])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            'f1: %.5f, precision: %.5f, recall: %.5f' % (f1, precision, recall)\n",
    "        )\n",
    "        s = json.dumps({\n",
    "            'text': d['text'],\n",
    "            'spo_list': list(T),\n",
    "            'spo_list_pred': list(R),\n",
    "            'new': list(R - T),\n",
    "            'lack': list(T - R),\n",
    "        },\n",
    "                       ensure_ascii=False,\n",
    "                       indent=4)\n",
    "        f.write(s + '\\n')\n",
    "    pbar.close()\n",
    "    f.close()\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_f1 = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        optimizer.apply_ema_weights()\n",
    "        f1, precision, recall = evaluate(valid_data)\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            train_model.save_weights('best_model.weights')\n",
    "        optimizer.reset_old_weights()\n",
    "        print(\n",
    "            'f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "            (f1, precision, recall, self.best_val_f1)\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    train_model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=20,\n",
    "        callbacks=[evaluator]\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    train_model.load_weights('best_model.weights')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "nlp_relation_extraction_bert4keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
