{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zhousanfu/NLP_relation_extraction/blob/master/nlp_relation_extraction_bert4keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqaPTbYUZdfv"
   },
   "outputs": [],
   "source": [
    "!mkdir bert_data\n",
    "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
    "!unzip chinese_L-12_H-768_A-12.zip\n",
    "!mv chinese_L-12_H-768_A-12 bert_data\n",
    "!wget https://github.com/zhousanfu/NLP_relation_extraction/raw/master/data/nlp_relation_extraction_bert4keras.zip\n",
    "!unzip nlp_relation_extraction_bert4keras.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oH9B1asFrGC2"
   },
   "outputs": [],
   "source": [
    "!pip install bert4keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aid/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K, batch_gather\n",
    "from bert4keras.layers import Loss\n",
    "from bert4keras.layers import LayerNormalization\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, to_array\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "VBE-7AR1XCPO",
    "outputId": "1a8873b8-e2c7-403a-ed56-5de9db5ec98b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/aid/miniconda3/envs/py36/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python.ops.custom_gradient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-031363f47f40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_gather\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert4keras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayerNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aid/miniconda3/envs/py36/lib/python3.6/site-packages/bert4keras/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcustom_gradient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 判断是tf.keras还是纯keras的标记\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python.ops.custom_gradient'"
     ]
    }
   ],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "# 三元组抽取任务，基于“半指针-半标注”结构\n",
    "# 文章介绍：https://kexue.fm/archives/7161\n",
    "# 数据集：http://ai.baidu.com/broad/download?dataset=sked\n",
    "# 最优f1=0.82198\n",
    "# 换用RoBERTa Large可以达到f1=0.829+\n",
    "# 说明：由于使用了EMA，需要跑足够多的步数(5000步以上）才生效，如果\n",
    "#      你的数据总量比较少，那么请务必跑足够多的epoch数，或者去掉EMA。\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from bert4keras.backend import keras, K, batch_gather\n",
    "from bert4keras.layers import Loss\n",
    "from bert4keras.layers import LayerNormalization\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.snippets import sequence_padding, DataGenerator\n",
    "from bert4keras.snippets import open, to_array\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\n",
    "from keras.models import Model\n",
    "from tqdm import tqdm\n",
    "\n",
    "maxlen = 128\n",
    "batch_size = 64\n",
    "config_path = '/content/bert_data/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "checkpoint_path = '/content/bert_data/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "dict_path = '/content/bert_data/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"加载数据\n",
    "    单条格式：{'text': text, 'spo_list': [(s, p, o)]}\n",
    "    \"\"\"\n",
    "    D = []\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        for l in f:\n",
    "            l = json.loads(l)\n",
    "            D.append({\n",
    "                'text': l['text'],\n",
    "                'spo_list': [(spo['subject'], spo['predicate'], spo['object'])\n",
    "                             for spo in l['spo_list']]\n",
    "            })\n",
    "    return D\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "train_data = load_data('/root/kg/datasets/train_data.json')\n",
    "valid_data = load_data('/root/kg/datasets/dev_data.json')\n",
    "predicate2id, id2predicate = {}, {}\n",
    "\n",
    "with open('/root/kg/datasets/all_50_schemas') as f:\n",
    "    for l in f:\n",
    "        l = json.loads(l)\n",
    "        if l['predicate'] not in predicate2id:\n",
    "            id2predicate[len(predicate2id)] = l['predicate']\n",
    "            predicate2id[l['predicate']] = len(predicate2id)\n",
    "\n",
    "# 建立分词器\n",
    "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "\n",
    "\n",
    "def search(pattern, sequence):\n",
    "    \"\"\"从sequence中寻找子串pattern\n",
    "    如果找到，返回第一个下标；否则返回-1。\n",
    "    \"\"\"\n",
    "    n = len(pattern)\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i + n] == pattern:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "class data_generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            token_ids, segment_ids = tokenizer.encode(d['text'], maxlen=maxlen)\n",
    "            # 整理三元组 {s: [(o, p)]}\n",
    "            spoes = {}\n",
    "            for s, p, o in d['spo_list']:\n",
    "                s = tokenizer.encode(s)[0][1:-1]\n",
    "                p = predicate2id[p]\n",
    "                o = tokenizer.encode(o)[0][1:-1]\n",
    "                s_idx = search(s, token_ids)\n",
    "                o_idx = search(o, token_ids)\n",
    "                if s_idx != -1 and o_idx != -1:\n",
    "                    s = (s_idx, s_idx + len(s) - 1)\n",
    "                    o = (o_idx, o_idx + len(o) - 1, p)\n",
    "                    if s not in spoes:\n",
    "                        spoes[s] = []\n",
    "                    spoes[s].append(o)\n",
    "            if spoes:\n",
    "                # subject标签\n",
    "                subject_labels = np.zeros((len(token_ids), 2))\n",
    "                for s in spoes:\n",
    "                    subject_labels[s[0], 0] = 1\n",
    "                    subject_labels[s[1], 1] = 1\n",
    "                # 随机选一个subject（这里没有实现错误！这就是想要的效果！！）\n",
    "                start, end = np.array(list(spoes.keys())).T\n",
    "                start = np.random.choice(start)\n",
    "                end = np.random.choice(end[end >= start])\n",
    "                subject_ids = (start, end)\n",
    "                # 对应的object标签\n",
    "                object_labels = np.zeros((len(token_ids), len(predicate2id), 2))\n",
    "                for o in spoes.get(subject_ids, []):\n",
    "                    object_labels[o[0], o[2], 0] = 1\n",
    "                    object_labels[o[1], o[2], 1] = 1\n",
    "                # 构建batch\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_subject_labels.append(subject_labels)\n",
    "                batch_subject_ids.append(subject_ids)\n",
    "                batch_object_labels.append(object_labels)\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                    batch_subject_labels = sequence_padding(\n",
    "                        batch_subject_labels\n",
    "                    )\n",
    "                    batch_subject_ids = np.array(batch_subject_ids)\n",
    "                    batch_object_labels = sequence_padding(batch_object_labels)\n",
    "                    yield [\n",
    "                        batch_token_ids, batch_segment_ids,\n",
    "                        batch_subject_labels, batch_subject_ids,\n",
    "                        batch_object_labels\n",
    "                    ], None\n",
    "                    batch_token_ids, batch_segment_ids = [], []\n",
    "                    batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "\n",
    "\n",
    "def extract_subject(inputs):\n",
    "    \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "    \"\"\"\n",
    "    output, subject_ids = inputs\n",
    "    start = batch_gather(output, subject_ids[:, :1])\n",
    "    end = batch_gather(output, subject_ids[:, 1:])\n",
    "    subject = K.concatenate([start, end], 2)\n",
    "    return subject[:, 0]\n",
    "\n",
    "\n",
    "# 补充输入\n",
    "subject_labels = Input(shape=(None, 2), name='Subject-Labels')\n",
    "subject_ids = Input(shape=(2,), name='Subject-Ids')\n",
    "object_labels = Input(shape=(None, len(predicate2id), 2), name='Object-Labels')\n",
    "\n",
    "# 加载预训练模型\n",
    "bert = build_transformer_model(\n",
    "    config_path=config_path,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    return_keras_model=False,\n",
    ")\n",
    "\n",
    "# 预测subject\n",
    "output = Dense(\n",
    "    units=2, activation='sigmoid', kernel_initializer=bert.initializer\n",
    ")(bert.model.output)\n",
    "subject_preds = Lambda(lambda x: x**2)(output)\n",
    "\n",
    "subject_model = Model(bert.model.inputs, subject_preds)\n",
    "\n",
    "# 传入subject，预测object\n",
    "# 通过Conditional Layer Normalization将subject融入到object的预测中\n",
    "output = bert.model.layers[-2].get_output_at(-1)  # 自己想为什么是-2而不是-1\n",
    "subject = Lambda(extract_subject)([output, subject_ids])\n",
    "output = LayerNormalization(conditional=True)([output, subject])\n",
    "output = Dense(\n",
    "    units=len(predicate2id) * 2,\n",
    "    activation='sigmoid',\n",
    "    kernel_initializer=bert.initializer\n",
    ")(output)\n",
    "output = Lambda(lambda x: x**4)(output)\n",
    "object_preds = Reshape((-1, len(predicate2id), 2))(output)\n",
    "\n",
    "object_model = Model(bert.model.inputs + [subject_ids], object_preds)\n",
    "\n",
    "\n",
    "class TotalLoss(Loss):\n",
    "    \"\"\"subject_loss与object_loss之和，都是二分类交叉熵\n",
    "    \"\"\"\n",
    "    def compute_loss(self, inputs, mask=None):\n",
    "        subject_labels, object_labels = inputs[:2]\n",
    "        subject_preds, object_preds, _ = inputs[2:]\n",
    "        if mask[4] is None:\n",
    "            mask = 1.0\n",
    "        else:\n",
    "            mask = K.cast(mask[4], K.floatx())\n",
    "        # sujuect部分loss\n",
    "        subject_loss = K.binary_crossentropy(subject_labels, subject_preds)\n",
    "        subject_loss = K.mean(subject_loss, 2)\n",
    "        subject_loss = K.sum(subject_loss * mask) / K.sum(mask)\n",
    "        # object部分loss\n",
    "        object_loss = K.binary_crossentropy(object_labels, object_preds)\n",
    "        object_loss = K.sum(K.mean(object_loss, 3), 2)\n",
    "        object_loss = K.sum(object_loss * mask) / K.sum(mask)\n",
    "        # 总的loss\n",
    "        return subject_loss + object_loss\n",
    "\n",
    "\n",
    "subject_preds, object_preds = TotalLoss([2, 3])([\n",
    "    subject_labels, object_labels, subject_preds, object_preds,\n",
    "    bert.model.output\n",
    "])\n",
    "\n",
    "# 训练模型\n",
    "train_model = Model(\n",
    "    bert.model.inputs + [subject_labels, subject_ids, object_labels],\n",
    "    [subject_preds, object_preds]\n",
    ")\n",
    "\n",
    "AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
    "optimizer = AdamEMA(learning_rate=1e-5)\n",
    "train_model.compile(optimizer=optimizer)\n",
    "\n",
    "\n",
    "def extract_spoes(text):\n",
    "    \"\"\"抽取输入text所包含的三元组\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
    "    mapping = tokenizer.rematch(text, tokens)\n",
    "    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
    "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
    "    # 抽取subject\n",
    "    subject_preds = subject_model.predict([token_ids, segment_ids])\n",
    "    subject_preds[:, [0, -1]] *= 0\n",
    "    start = np.where(subject_preds[0, :, 0] > 0.6)[0]\n",
    "    end = np.where(subject_preds[0, :, 1] > 0.5)[0]\n",
    "    subjects = []\n",
    "    for i in start:\n",
    "        j = end[end >= i]\n",
    "        if len(j) > 0:\n",
    "            j = j[0]\n",
    "            subjects.append((i, j))\n",
    "    if subjects:\n",
    "        spoes = []\n",
    "        token_ids = np.repeat(token_ids, len(subjects), 0)\n",
    "        segment_ids = np.repeat(segment_ids, len(subjects), 0)\n",
    "        subjects = np.array(subjects)\n",
    "        # 传入subject，抽取object和predicate\n",
    "        object_preds = object_model.predict([token_ids, segment_ids, subjects])\n",
    "        object_preds[:, [0, -1]] *= 0\n",
    "        for subject, object_pred in zip(subjects, object_preds):\n",
    "            start = np.where(object_pred[:, :, 0] > 0.6)\n",
    "            end = np.where(object_pred[:, :, 1] > 0.5)\n",
    "            for _start, predicate1 in zip(*start):\n",
    "                for _end, predicate2 in zip(*end):\n",
    "                    if _start <= _end and predicate1 == predicate2:\n",
    "                        spoes.append(\n",
    "                            ((mapping[subject[0]][0],\n",
    "                              mapping[subject[1]][-1]), predicate1,\n",
    "                             (mapping[_start][0], mapping[_end][-1]))\n",
    "                        )\n",
    "                        break\n",
    "        return [(text[s[0]:s[1] + 1], id2predicate[p], text[o[0]:o[1] + 1])\n",
    "                for s, p, o, in spoes]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "class SPO(tuple):\n",
    "    \"\"\"用来存三元组的类\n",
    "    表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，\n",
    "    使得在判断两个三元组是否等价时容错性更好。\n",
    "    \"\"\"\n",
    "    def __init__(self, spo):\n",
    "        self.spox = (\n",
    "            tuple(tokenizer.tokenize(spo[0])),\n",
    "            spo[1],\n",
    "            tuple(tokenizer.tokenize(spo[2])),\n",
    "        )\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.spox.__hash__()\n",
    "\n",
    "    def __eq__(self, spo):\n",
    "        return self.spox == spo.spox\n",
    "\n",
    "\n",
    "def evaluate(data):\n",
    "    \"\"\"评估函数，计算f1、precision、recall\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    f = open('dev_pred.json', 'w', encoding='utf-8')\n",
    "    pbar = tqdm()\n",
    "    for d in data:\n",
    "        R = set([SPO(spo) for spo in extract_spoes(d['text'])])\n",
    "        T = set([SPO(spo) for spo in d['spo_list']])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            'f1: %.5f, precision: %.5f, recall: %.5f' % (f1, precision, recall)\n",
    "        )\n",
    "        s = json.dumps({\n",
    "            'text': d['text'],\n",
    "            'spo_list': list(T),\n",
    "            'spo_list_pred': list(R),\n",
    "            'new': list(R - T),\n",
    "            'lack': list(T - R),\n",
    "        },\n",
    "                       ensure_ascii=False,\n",
    "                       indent=4)\n",
    "        f.write(s + '\\n')\n",
    "    pbar.close()\n",
    "    f.close()\n",
    "    return f1, precision, recall\n",
    "\n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best_val_f1 = 0.\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        optimizer.apply_ema_weights()\n",
    "        f1, precision, recall = evaluate(valid_data)\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            train_model.save_weights('best_model.weights')\n",
    "        optimizer.reset_old_weights()\n",
    "        print(\n",
    "            'f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
    "            (f1, precision, recall, self.best_val_f1)\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_generator = data_generator(train_data, batch_size)\n",
    "    evaluator = Evaluator()\n",
    "\n",
    "    train_model.fit(\n",
    "        train_generator.forfit(),\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=20,\n",
    "        callbacks=[evaluator]\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    train_model.load_weights('best_model.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "nlp_relation_extraction_bert4keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
