{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "699773a2-c53d-43c3-9e70-9b9ee415314d",
   "metadata": {
    "id": "699773a2-c53d-43c3-9e70-9b9ee415314d",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **一**、Sofa 中文医疗关系抽取\n",
    "kelvincjr/shared/casrel-main/\n",
    "19年关系抽取sofa论文的pytorch复现, 用于天池的中文医疗关系抽取数据集,Result: F1 = 54.6\n",
    "!pip install keras==2.2.4 tensorflow==1.13.1 keras-bert==0.81.1 tensorflow-gpu==1.13.1 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yG3ZZOHI9I4_",
   "metadata": {
    "id": "yG3ZZOHI9I4_"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JEcNy1P1i03A",
   "metadata": {
    "id": "JEcNy1P1i03A"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/Data/NLP/NLP_中文医疗信息处理挑战榜CBLUE/CMeIE.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b0420-e21c-4ae9-9622-0f20c1caba4c",
   "metadata": {
    "id": "628b0420-e21c-4ae9-9622-0f20c1caba4c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "data_path = base_path + 'content/CMeIE'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc13a2a-9fe6-4505-8706-27234346f9f3",
   "metadata": {
    "id": "fdc13a2a-9fe6-4505-8706-27234346f9f3"
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2776a98-dbb4-45b8-a282-35175f1f15f7",
   "metadata": {
    "id": "d2776a98-dbb4-45b8-a282-35175f1f15f7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from transformers import BertTokenizer\n",
    "from random import choice\n",
    "import codecs\n",
    "\n",
    "device = 'cuda:0' #device = 'cpu'\n",
    "\n",
    "class MyDataset(Dataset):    \n",
    "    def __get_train_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "        res = [json.loads(i) for i in data]\n",
    "        return res\n",
    "\n",
    "    def __get_test_data(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "        data = [json.loads(i) for i in data]\n",
    "        res = []\n",
    "        for entry in data:\n",
    "            entry['spo_list'] = []\n",
    "            res.append(entry)\n",
    "        return res\n",
    "    \n",
    "    def __init__(self, path, config):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.config = config\n",
    "        if(self.config['mode'] == 'train'):\n",
    "            self.data = self.__get_train_data(path)\n",
    "        elif(self.config['mode'] == 'test'):\n",
    "            self.data = self.__get_test_data(path)\n",
    "        with open(data_path+'/relation2idx.json', 'r', encoding='utf-8') as f:\n",
    "            self.relation2idx = json.load(f)\n",
    "        self.idx2relation = dict()\n",
    "        for key in self.relation2idx:\n",
    "            self.idx2relation[self.relation2idx[key]] = key\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.config['model_name'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text, gold = self.data[item]['text'], self.data[item]['spo_list']\n",
    "        text = text if len(text) <= 512 else text[:512]\n",
    "        sample = list(text)\n",
    "        sample = self.tokenizer.convert_tokens_to_ids(sample)\n",
    "        sub_start = len(sample)*[0]\n",
    "        sub_end = len(sample)*[0]\n",
    "        relation_start = [[0 for _ in range(self.config['relation_types'])] for _ in range(len(sample))]\n",
    "        relation_end = [[0 for _ in range(self.config['relation_types'])] for _ in range(len(sample))]\n",
    "        #   dim = (seq_len, relation_types)\n",
    "        sub_start_single = len(sample) * [0]\n",
    "        sub_end_single = len(sample)*[0]\n",
    "        s2ro_map = {}\n",
    "        for entry in gold:\n",
    "            sub = entry['subject']\n",
    "            obj = entry['object']['@value']\n",
    "            relation = '同义词-' + entry['subject_type'] if entry['predicate'] == '同义词' else entry['predicate']\n",
    "            #正则表达式无法处理小括号, 所以抛出异常\n",
    "            try:\n",
    "                sub_pos = re.search(sub, text).span()\n",
    "                obj_pos = re.search(obj, text).span()\n",
    "                relation_idx = self.relation2idx[relation]\n",
    "                sub_start[sub_pos[0]] = 1\n",
    "                sub_end[sub_pos[1]-1] = 1\n",
    "                if sub_pos not in s2ro_map:\n",
    "                    s2ro_map[sub_pos] = []\n",
    "                s2ro_map[sub_pos].append((obj_pos, relation_idx))\n",
    "            except:\n",
    "                pass\n",
    "        if s2ro_map:\n",
    "            sub_pos = choice(list(s2ro_map.keys()))\n",
    "            sub_start_single[sub_pos[0]] = 1\n",
    "            sub_end_single[sub_pos[1] - 1] = 1\n",
    "            for obj_pos, relation_idx in s2ro_map.get(sub_pos, []):\n",
    "                relation_start[obj_pos[0]][relation_idx] = 1\n",
    "                relation_end[obj_pos[1]-1][relation_idx] = 1\n",
    "        return sample, sub_start, sub_end, relation_start, relation_end, sub_start_single, sub_end_single\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key= lambda x: len(x[0]), reverse = True)\n",
    "    sample, sub_start, sub_end, relation_start, relation_end, sub_start_single, sub_end_single = zip(*data)\n",
    "    mask = [[1 if j < len(i) else 0 for j in range(len(sample[0]))] for i in sample]\n",
    "    sample = [torch.tensor(i).long().to(device) for i in sample]\n",
    "    sub_start = [torch.tensor(i).long().to(device) for i in sub_start]\n",
    "    sub_end = [torch.tensor(i).long().to(device) for i in sub_end]\n",
    "    relation_start = [torch.tensor(i).long().to(device) for i in relation_start]\n",
    "    relation_end = [torch.tensor(i).long().to(device) for i in relation_end]\n",
    "    sub_start_single = [torch.tensor(i).long().to(device) for i in sub_start_single]\n",
    "    sub_end_single = [torch.tensor(i).long().to(device) for i in sub_end_single]\n",
    "    mask = torch.tensor(mask).long().to(device)\n",
    "    sample = pad_sequence(sample, batch_first=True, padding_value=0)\n",
    "    sub_start = pad_sequence(sub_start, batch_first=True, padding_value=0)\n",
    "    sub_end = pad_sequence(sub_end, batch_first=True, padding_value=0)\n",
    "    relation_start = pad_sequence(relation_start, batch_first=True, padding_value=0)\n",
    "    relation_end = pad_sequence(relation_end, batch_first=True, padding_value=0)\n",
    "    sub_start_single = pad_sequence(sub_start_single, batch_first=True, padding_value=0)\n",
    "    sub_end_single = pad_sequence(sub_end_single, batch_first=True, padding_value=0)\n",
    "    return sample, sub_start, sub_end, relation_start, relation_end, mask, sub_start_single, sub_end_single\n",
    "    # dim(sample) = dim(sub_start) = dim(sub_end) = (batch_size, seq_len]\n",
    "    # dim(relation_start) = dim(relation_end) = (batch_size, seq_len, relation_types)\n",
    "\n",
    "predicate2id = {}\n",
    "id2predicate = {}\n",
    "with open(data_path+'/53_schemas.json', encoding=\"utf-8\") as f:\n",
    "    for l in f:\n",
    "        l = json.loads(l)\n",
    "        if l['predicate'] not in predicate2id:\n",
    "            id2predicate[len(predicate2id)] = l['predicate']\n",
    "            predicate2id[l['predicate']] = len(predicate2id)\n",
    "f.close()\n",
    "\n",
    "with open(data_path+'/relation2idx.json', 'w', encoding='utf-8') as fw:\n",
    "    fw.write(json.dumps(id2predicate, ensure_ascii=False))\n",
    "fw.close()\n",
    "\n",
    "config = {\n",
    "    'mode': 'test', \n",
    "    'model_name': 'bert-base-multilingual-cased',\n",
    "    'batch_size': 512,\n",
    "    'relation_types': 53\n",
    "}\n",
    "path = data_path+'/CMeIE_test.json'\n",
    "data = MyDataset(path, config)\n",
    "dataloader = DataLoader(data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "batch_data = next(iter(dataloader))\n",
    "a, b = batch_data[3][37], batch_data[4][37]\n",
    "print(len(batch_data), len(batch_data[0]))\n",
    "print(len(a), a)\n",
    "print(len(b), b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3f374-ff9e-4be0-a9e4-66e40e1bf900",
   "metadata": {
    "id": "58a3f374-ff9e-4be0-a9e4-66e40e1bf900"
   },
   "source": [
    "## CasRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8795ad-4913-436c-b98c-9f95652ad5fb",
   "metadata": {
    "id": "bc8795ad-4913-436c-b98c-9f95652ad5fb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "class CasRel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CasRel, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert_dim = 768\n",
    "        self.bert_encoder = BertModel.from_pretrained(config['model_name'])\n",
    "        self.sub_start_tageer = nn.Linear(self.bert_dim, 1)\n",
    "        self.sub_end_tagger = nn.Linear(self.bert_dim, 1)\n",
    "        self.obj_start_tagger = nn.Linear(self.bert_dim, config['relation_types'])\n",
    "        self.obj_end_tagger = nn.Linear(self.bert_dim, config['relation_types'])\n",
    "\n",
    "    def get_encoded_text(self, data):\n",
    "        # with torch.no_grad():   #   out of GPU Memory\n",
    "        encoded_text = self.bert_encoder(data['token_ids'], attention_mask=data['mask'])[0]\n",
    "        return encoded_text #   dim  = (batch_size, seq_len, bert_dim)\n",
    "\n",
    "    def get_sub(self, encoded_text):\n",
    "        #   dim(pred) = (batch_size, seq_len, 1)\n",
    "        pred_sub_start = self.sub_start_tageer(encoded_text)\n",
    "        pred_sub_start = torch.sigmoid(pred_sub_start)\n",
    "        pred_sub_end = self.sub_end_tagger(encoded_text)\n",
    "        pred_sub_end = torch.sigmoid(pred_sub_end)\n",
    "        return pred_sub_start, pred_sub_end\n",
    "\n",
    "    # def get_sub_info(self, encoded_text, pred_sub_start, pred_sub_end, real_sub_start, real_sub_end):\n",
    "    #     if(self.config['mode'] == 'train'):\n",
    "    #         start = real_sub_start\n",
    "    #         end = real_sub_end\n",
    "    #     elif(self.config['mode'] == 'test'):\n",
    "    #         threshold = 0.5\n",
    "    #         start = [pred_sub_start > threshold]\n",
    "    #         end = [pred_sub_end > threshold]\n",
    "    #     pred_sub_lst = []\n",
    "    #     for idx_start, i in enumerate(start):\n",
    "    #         if(i == 1):\n",
    "    #             for idx_end in range(idx_start, len(end)):\n",
    "    #                 if(end[idx_end] == 1):\n",
    "    #    Problem: 一个batch中不同样本的sub_list长度不同\n",
    "\n",
    "    def get_obj(self, sub_start_mapping, sub_end_mapping, encoded_text):\n",
    "        #   dim(sub_start_mapping) = dim(sub_end_mapping) = (batch_size, 1, seq_len)\n",
    "        #   dim(encoded_text) = (batch_size, seq_len, bert_dim)\n",
    "        sub_start = torch.matmul(sub_start_mapping.float(), encoded_text)\n",
    "        sub_end = torch.matmul(sub_end_mapping.float(), encoded_text)\n",
    "        #   dim(sub_start) = dim(sub_end) = (batch_size, 1, bert_dim)\n",
    "        sub = (sub_start + sub_end) / 2\n",
    "        encoded_text = encoded_text + sub\n",
    "        pred_obj_start = self.obj_start_tagger(encoded_text)\n",
    "        pred_obj_end = self.obj_end_tagger(encoded_text)\n",
    "        pred_obj_start = torch.sigmoid(pred_obj_start)\n",
    "        pred_obj_end = torch.sigmoid(pred_obj_end)\n",
    "        return pred_obj_start, pred_obj_end\n",
    "        #   dim = (batch_size, seq_len, relation_types)\n",
    "\n",
    "    def get_list(self, start, end, text, h_bar=0.5, t_bar=0.5):\n",
    "        res = []\n",
    "        start, end = start[: 512], end[: 512]\n",
    "        start_idxs, end_idxs = [], []\n",
    "        \n",
    "        for idx in range(len(start)):\n",
    "            print('---> strt[idx]:', start[idx])\n",
    "            if(start[idx] > h_bar):\n",
    "                start_idxs.append(idx)\n",
    "            if(end[idx] > t_bar):\n",
    "                end_idxs.append(idx)\n",
    "        for start_idx in start_idxs:\n",
    "            for end_idx in end_idxs:\n",
    "                if(end_idx >= start_idx):\n",
    "                    entry = {}\n",
    "                    entry['text'] = text[start_idx: end_idx+1]\n",
    "                    entry['start'] = start_idx\n",
    "                    entry['end'] = end_idx\n",
    "                    res.append(entry)\n",
    "                    break\n",
    "        return res\n",
    "\n",
    "    def forward(self, data):\n",
    "        encoded_text = self.get_encoded_text(data)\n",
    "        pred_sub_start, pred_sub_end = self.get_sub(encoded_text)\n",
    "        sub_start_mapping = data['sub_start'].unsqueeze(1)\n",
    "        # (batch_size, seq_len) --> (batch_size, 1, seq_len)\n",
    "        sub_end_mapping = data['sub_end'].unsqueeze(1)\n",
    "        pred_obj_start, pred_obj_end = self.get_obj(sub_start_mapping, sub_end_mapping, encoded_text)\n",
    "        return pred_sub_start, pred_sub_end, pred_obj_start, pred_obj_end\n",
    "        #   dim(pred_sub_start) = dim(pred_sub_end) = (batch_size, seq_len, 1)\n",
    "        #   dim(pred_obj_start) = dim(pred_obj_end) = (batch_size, seq_len, realtion_types)\n",
    "\n",
    "    def test(self, data):\n",
    "        encoded_text = self.get_encoded_text(data)\n",
    "        pred_sub_start, pred_sub_end = self.get_sub(encoded_text)\n",
    "        sub_list = self.get_list(pred_sub_start.squeeze(0).squeeze(-1), pred_sub_end.squeeze(0).squeeze(-1), data['text'])\n",
    "        if(sub_list):\n",
    "            repeated_encoded_text = encoded_text.repeat(len(sub_list), 1, 1)\n",
    "            sub_start_mapping = torch.zeros(len(sub_list), 1, encoded_text.shape[1]).to(device)\n",
    "            sub_end_mapping = torch.zeros(len(sub_list), 1, encoded_text.shape[1]).to(device)\n",
    "            for idx, sub in enumerate(sub_list):\n",
    "                sub_start_mapping[idx][0][sub['start']] = 1\n",
    "                sub_end_mapping[idx][0][sub['end']] = 1\n",
    "            pred_obj_start, pred_obj_end = self.get_obj(sub_start_mapping, sub_end_mapping, repeated_encoded_text)\n",
    "            return sub_list, pred_obj_start, pred_obj_end\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07907bd8-48df-4022-b5bc-617ada86d599",
   "metadata": {
    "id": "07907bd8-48df-4022-b5bc-617ada86d599"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d785aea-b37f-4c2a-a430-dbd6be129071",
   "metadata": {
    "id": "9d785aea-b37f-4c2a-a430-dbd6be129071"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = 'cpu' #'cuda:0'# 'cpu'\n",
    "# torch.set_num_threads(6)\n",
    "\n",
    "def get_loss(pred, gold, mask):\n",
    "    pred = pred.squeeze(-1)\n",
    "    loss = F.binary_cross_entropy(pred, gold.float(), reduction='none') #以向量形式返回loss\n",
    "    if loss.shape != mask.shape:\n",
    "        mask = mask.unsqueeze(-1)\n",
    "    loss = torch.sum(loss*mask)/torch.sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "config = {\n",
    "    'mode': 'train',\n",
    "    'model_name': 'bert-base-multilingual-cased', # 'bert-base-chinese', #'bert-base-multilingual-cased'\n",
    "    'batch_size': 40,\n",
    "    'epoch': 1,\n",
    "    'relation_types': 53,\n",
    "    'sub_weight': 1,\n",
    "    'obj_weight': 1\n",
    "}\n",
    "path = data_path+'/CMeIE_train.json'\n",
    "data = MyDataset(path, config)\n",
    "dataloader = DataLoader(data, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = CasRel(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "loss_recorder = 0\n",
    "for epoch_index in range(config['epoch']):\n",
    "    print('---> epoch_index:', epoch_index)\n",
    "    time_start = time.perf_counter()\n",
    "    for batch_index, (sample, sub_start, sub_end, relation_start, relation_end, mask, sub_start_single, sub_end_single) in enumerate(iter(dataloader)):\n",
    "        batch_data = dict()\n",
    "        batch_data['token_ids'] = sample\n",
    "        batch_data['mask'] = mask\n",
    "        batch_data['sub_start'] = sub_start_single\n",
    "        batch_data['sub_end'] = sub_end_single\n",
    "        pred_sub_start, pred_sub_end, pred_obj_start, pred_obj_end = model(batch_data)\n",
    "        sub_start_loss = get_loss(pred_sub_start, sub_start, mask)\n",
    "        sub_end_loss = get_loss(pred_sub_end, sub_end, mask)\n",
    "        obj_start_loss = get_loss(pred_obj_start, relation_start, mask)\n",
    "        obj_end_loss = get_loss(pred_obj_end, relation_end, mask)\n",
    "        loss = config['sub_weight']*(sub_start_loss + sub_end_loss) + config['obj_weight']*(obj_start_loss + obj_end_loss)\n",
    "        loss_recorder += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(\"epoch: %d batch: %d loss: %f\"% (epoch_index, batch_index, loss))\n",
    "        if(batch_index%100 == 99):\n",
    "            print('--->loss:', loss_recorder)\n",
    "            loss_recorder = 0\n",
    "    time_end = time.perf_counter()\n",
    "    torch.save(model.state_dict(), data_path+'/models.pkl')\n",
    "    print(\"successfully saved! time used = %fs.\"% (time_end-time_start), batch_index, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182a94f6-734b-4948-a3e4-95cb5ff610b8",
   "metadata": {
    "id": "182a94f6-734b-4948-a3e4-95cb5ff610b8"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = TFBertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96b1b8-b721-4d3e-90dd-f2a1fde490d7",
   "metadata": {
    "id": "bd96b1b8-b721-4d3e-90dd-f2a1fde490d7"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda:0'\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "def trans_schemas(path):\n",
    "    re2sub = dict()\n",
    "    re2obj = dict()\n",
    "    with open(path, \"r\", encoding='utf-8') as f:\n",
    "        sens = f.readlines()\n",
    "    schemas = []\n",
    "    for sen in sens:\n",
    "        schemas.append(json.loads(sen.strip()))\n",
    "    for entry in schemas:\n",
    "        re2sub[entry['predicate']] = entry['subject_type']\n",
    "        re2obj[entry['predicate']] = entry['object_type']\n",
    "    return re2sub, re2obj\n",
    "\n",
    "\n",
    "def get_list(start, end, text, h_bar=0.5, t_bar=0.5):\n",
    "    res = []\n",
    "    start, end = start[: 512], end[: 512]\n",
    "    start_idxs, end_idxs = [], []\n",
    "    for idx in range(len(start)):\n",
    "        if (start[idx] > h_bar):\n",
    "            start_idxs.append(idx)\n",
    "        if (end[idx] > t_bar):\n",
    "            end_idxs.append(idx)\n",
    "    for start_idx in start_idxs:\n",
    "        for end_idx in end_idxs:\n",
    "            if (end_idx >= start_idx):\n",
    "                entry = {}\n",
    "                entry['text'] = text[start_idx: end_idx+1]\n",
    "                entry['start'] = start_idx\n",
    "                entry['end'] = end_idx\n",
    "                res.append(entry)\n",
    "                break\n",
    "    return res\n",
    "\n",
    "def get_text(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    data = [json.loads(i) for i in data]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    'mode': 'test',\n",
    "    'model_name': 'bert-base-multilingual-cased', # 'bert-base-multilingual-cased'\n",
    "    'batch_size': 512,\n",
    "    'epoch': 1,\n",
    "    'relation_types': 53,\n",
    "    'sub_weight': 1,\n",
    "    'obj_weight': 1\n",
    "}\n",
    "path = data_path+'/CMeIE_test.json'\n",
    "schemas_path = data_path+'/53_schemas.json'\n",
    "res_path = data_path+'/CMeIE_test_res.json'\n",
    "res_file = codecs.open(res_path, 'w', encoding='utf-8')\n",
    "raw_data = get_text(path)\n",
    "re2sub, re2obj = trans_schemas(schemas_path)\n",
    "data = MyDataset(path, config)\n",
    "dataloader = DataLoader(data, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "model = CasRel(config).to(device)\n",
    "model.load_state_dict(torch.load(data_path+'/models.pkl'))\n",
    "for batch_index, (sample, sub_start, sub_end, relation_start, relation_end, mask, _, _) in enumerate(iter(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        text =  raw_data[batch_index]['text']\n",
    "        batch_data = dict()\n",
    "        batch_data['token_ids'] = sample\n",
    "        batch_data['mask'] = mask\n",
    "        batch_data['text'] = text\n",
    "        ret = model.test(batch_data)\n",
    "        spo_list = []\n",
    "        if ret:\n",
    "            sub_list, pred_obj_start, pred_obj_end = ret\n",
    "            for idx, sub in enumerate(sub_list):\n",
    "                obj_start, obj_end = pred_obj_start[idx].transpose(0, 1), pred_obj_end[idx].transpose(0, 1)\n",
    "                for i in range(config['relation_types']):\n",
    "                    obj_list = get_list(obj_start[i], obj_end[i], text)\n",
    "                    for obj in obj_list:\n",
    "                        entry = {}\n",
    "                        entry['Combined'] = '。' in text[sub['end']: obj['start']] or '。' in text[obj['end']: sub['start']]\n",
    "                        entry['subject'] = sub['text']\n",
    "                        entry['predicate'] = data.idx2relation[i]\n",
    "                        entry['object'] = {'@value': obj['text']}\n",
    "                        entry['subject_type'] = re2sub[data.idx2relation[i]]\n",
    "                        entry['object_type'] = {'@value': re2obj[data.idx2relation[i]]}\n",
    "                        spo_list.append(entry)\n",
    "        res = {}\n",
    "        res['text'] = text\n",
    "        res['spo_list'] = spo_list\n",
    "        json.dump(res, res_file, ensure_ascii=False)\n",
    "        res_file.write('\\n')\n",
    "        print('--->', batch_index, spo_list, text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BTddnzf4Sfgn",
   "metadata": {
    "id": "BTddnzf4Sfgn"
   },
   "source": [
    "# **二**、CasRel-bert 关系抽取\n",
    "[{\"subject\": \"阿司匹林\", \"relation\": \"病因\", \"object\": \"药物 因素\"}, {\"subject\": \"阿司匹林\", \"relation\": \"病因\", \"object\": \"保泰松\"}, {\"subject\": \"阿司匹林\", \"relation\": \"病因\", \"object\": \"精神 因素\"}, {\"subject\": \"消化性溃疡\", \"relation\": \"病因\", \"object\": \"吲哚美辛\"}, {\"subject\": \"消化性溃疡\", \"relation\": \"病因\", \"object\": \"肾上腺皮质激素\"}, {\"subject\": \"消化性溃疡\", \"relation\": \"病因\", \"object\": \"精神 因素\"}, {\"subject\": \"阿司匹林\", \"relation\": \"病因\", \"object\": \"吲哚美辛\"}, {\"subject\": \"消化性溃疡\", \"relation\": \"病因\", \"object\": \"药物 因素\"}, {\"subject\": \"消化性溃疡\", \"relation\": \"病因\", \"object\": \"保泰松\"}, {\"subject\": \"阿司匹林\", \"relation\": \"病因\", \"object\": \"肾上腺皮质激素\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1q5eO3odWX4z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2715,
     "status": "ok",
     "timestamp": 1637199669063,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "1q5eO3odWX4z",
    "outputId": "b4e38d55-5b19-4289-e402-7fb99b2276ce"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/longlongman/CasRel-pytorch-reimplement.git\n",
    "!cp -r /content/CasRel-pytorch-reimplement/CasRel-reimplement/data/CMED /content/CMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rcSDEPmZBg6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28060,
     "status": "ok",
     "timestamp": 1637199697120,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "rcSDEPmZBg6a",
    "outputId": "cdb3a01c-0db0-4739-96a8-5ecbfd3178eb"
   },
   "outputs": [],
   "source": [
    "!gdown --id '1AQitrjbvCWc51SYiLN-cJq4e0WiNN4KY' --output /content/chinese-bert-wwm.zip\n",
    "!mkdir /content/Models\n",
    "!mkdir /content/Models/cinese-bert-wwm\n",
    "!unzip -d /content/Models/cinese-bert-wwm /content/chinese-bert-wwm.zip\n",
    "# from transformers import BertModel\n",
    "# bert_encoder = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\", cache_dir=data_path+'/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RvRpqwAtUEMX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1841,
     "status": "ok",
     "timestamp": 1637199707604,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "RvRpqwAtUEMX",
    "outputId": "264d4b97-da4b-440c-861f-b4d395f76a75"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = '/home/aid/Github/NLP_relation_extraction'#os.path.dirname(os.getcwd())\n",
    "data_path = base_path + '/data/CMED'#'content/CMED'\n",
    "model_path = base_path + '/models/chinese-bert-wwm'#'content/Models/chinese-bert-wwm'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pswNSZDbFrVC",
   "metadata": {
    "id": "pswNSZDbFrVC"
   },
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_E2f5Xi2FtyB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1637199707605,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "_E2f5Xi2FtyB",
    "outputId": "daa1e874-fca6-4791-81a8-30e84a7c26e1"
   },
   "outputs": [],
   "source": [
    "from keras_bert import Tokenizer\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "class HBTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = unicodedata.normalize('NFD', text)\n",
    "            text = ''.join([ch for ch in text if unicodedata.category(ch) != 'Mn'])\n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "            tokens.append('[unused1]')\n",
    "        return tokens\n",
    "\n",
    "\n",
    "def get_tokenizer(vocab_path):\n",
    "    token_dict = {}\n",
    "    with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "        for line in reader:\n",
    "            token = line.strip()\n",
    "            token_dict[token] = len(token_dict)\n",
    "    return HBTokenizer(token_dict, cased=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0uhstLO1SvqC",
   "metadata": {
    "id": "0uhstLO1SvqC"
   },
   "source": [
    "## data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GqKuN3prVk5o",
   "metadata": {
    "executionInfo": {
     "elapsed": 27147,
     "status": "ok",
     "timestamp": 1637199734746,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "GqKuN3prVk5o"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import choice\n",
    "\n",
    "tokenizer = get_tokenizer(model_path+'/vocab.txt')\n",
    "BERT_MAX_LEN = 512\n",
    "\n",
    "\n",
    "def find_head_idx(source, target):\n",
    "    target_len = len(target)\n",
    "    for i in range(len(source)):\n",
    "        if source[i: i + target_len] == target:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "class CMEDDataset(Dataset):\n",
    "    def __init__(self, config, prefix, is_test, tokenizer):\n",
    "        self.config = config\n",
    "        self.prefix = prefix\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        if self.config.debug:\n",
    "            self.json_data = json.load(open(os.path.join(self.config.data_path, prefix + '.json')))[:500]\n",
    "        else:\n",
    "            self.json_data = json.load(open(os.path.join(self.config.data_path, prefix + '.json')))\n",
    "        self.rel2id = json.load(open(os.path.join(self.config.data_path, 'rel2id.json')))[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ins_json_data = self.json_data[idx]\n",
    "        text = ins_json_data['text']\n",
    "        text = ' '.join(text.split()[:self.config.max_len])\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        if len(tokens) > BERT_MAX_LEN:\n",
    "            tokens = tokens[: BERT_MAX_LEN]\n",
    "        text_len = len(tokens)\n",
    "\n",
    "        if not self.is_test:\n",
    "            s2ro_map = {}\n",
    "            for triple in ins_json_data['triple_list']:\n",
    "                triple = (self.tokenizer.tokenize(triple[0])[1:-1], triple[1], self.tokenizer.tokenize(triple[2])[1:-1])\n",
    "                sub_head_idx = find_head_idx(tokens, triple[0])\n",
    "                obj_head_idx = find_head_idx(tokens, triple[2])\n",
    "                if sub_head_idx != -1 and obj_head_idx != -1:\n",
    "                    sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
    "                    if sub not in s2ro_map:\n",
    "                        s2ro_map[sub] = []\n",
    "                    s2ro_map[sub].append((obj_head_idx, obj_head_idx + len(triple[2]) - 1, self.rel2id[triple[1]]))\n",
    "\n",
    "            if s2ro_map:\n",
    "                token_ids, segment_ids = self.tokenizer.encode(first=text)\n",
    "                masks = segment_ids\n",
    "                if len(token_ids) > text_len:\n",
    "                    token_ids = token_ids[:text_len]\n",
    "                    masks = masks[:text_len]\n",
    "                token_ids = np.array(token_ids)\n",
    "                masks = np.array(masks) + 1\n",
    "                sub_heads, sub_tails = np.zeros(text_len), np.zeros(text_len)\n",
    "                for s in s2ro_map:\n",
    "                    sub_heads[s[0]] = 1\n",
    "                    sub_tails[s[1]] = 1\n",
    "                sub_head_idx, sub_tail_idx = choice(list(s2ro_map.keys()))\n",
    "                sub_head, sub_tail = np.zeros(text_len), np.zeros(text_len)\n",
    "                sub_head[sub_head_idx] = 1\n",
    "                sub_tail[sub_tail_idx] = 1\n",
    "                obj_heads, obj_tails = np.zeros((text_len, self.config.rel_num)), np.zeros((text_len, self.config.rel_num))\n",
    "                for ro in s2ro_map.get((sub_head_idx, sub_tail_idx), []):\n",
    "                    obj_heads[ro[0]][ro[2]] = 1\n",
    "                    obj_tails[ro[1]][ro[2]] = 1\n",
    "                return token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, ins_json_data['triple_list'], tokens\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            token_ids, segment_ids = self.tokenizer.encode(first=text)\n",
    "            masks = segment_ids\n",
    "            if len(token_ids) > text_len:\n",
    "                token_ids = token_ids[:text_len]\n",
    "                masks = masks[:text_len]\n",
    "            token_ids = np.array(token_ids)\n",
    "            masks = np.array(masks) + 1\n",
    "            sub_heads, sub_tails = np.zeros(text_len), np.zeros(text_len)\n",
    "            sub_head, sub_tail = np.zeros(text_len), np.zeros(text_len)\n",
    "            obj_heads, obj_tails = np.zeros((text_len, self.config.rel_num)), np.zeros((text_len, self.config.rel_num))\n",
    "            return token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, ins_json_data['triple_list'], tokens\n",
    "\n",
    "\n",
    "def cmed_collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    token_ids, masks, text_len, sub_heads, sub_tails, sub_head, sub_tail, obj_heads, obj_tails, triples, tokens = zip(*batch)\n",
    "    cur_batch = len(batch)\n",
    "    max_text_len = max(text_len)\n",
    "    batch_token_ids = torch.LongTensor(cur_batch, max_text_len).zero_()\n",
    "    batch_masks = torch.LongTensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_heads = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_tails = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_head = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_sub_tail = torch.Tensor(cur_batch, max_text_len).zero_()\n",
    "    batch_obj_heads = torch.Tensor(cur_batch, max_text_len, 44).zero_()\n",
    "    batch_obj_tails = torch.Tensor(cur_batch, max_text_len, 44).zero_()\n",
    "\n",
    "    for i in range(cur_batch):\n",
    "        batch_token_ids[i, :text_len[i]].copy_(torch.from_numpy(token_ids[i]))\n",
    "        batch_masks[i, :text_len[i]].copy_(torch.from_numpy(masks[i]))\n",
    "        batch_sub_heads[i, :text_len[i]].copy_(torch.from_numpy(sub_heads[i]))\n",
    "        batch_sub_tails[i, :text_len[i]].copy_(torch.from_numpy(sub_tails[i]))\n",
    "        batch_sub_head[i, :text_len[i]].copy_(torch.from_numpy(sub_head[i]))\n",
    "        batch_sub_tail[i, :text_len[i]].copy_(torch.from_numpy(sub_tail[i]))\n",
    "        batch_obj_heads[i, :text_len[i], :].copy_(torch.from_numpy(obj_heads[i]))\n",
    "        batch_obj_tails[i, :text_len[i], :].copy_(torch.from_numpy(obj_tails[i]))\n",
    "\n",
    "    return {'token_ids': batch_token_ids,\n",
    "            'mask': batch_masks,\n",
    "            'sub_heads': batch_sub_heads,\n",
    "            'sub_tails': batch_sub_tails,\n",
    "            'sub_head': batch_sub_head,\n",
    "            'sub_tail': batch_sub_tail,\n",
    "            'obj_heads': batch_obj_heads,\n",
    "            'obj_tails': batch_obj_tails,\n",
    "            'triples': triples,\n",
    "            'tokens': tokens}\n",
    "\n",
    "\n",
    "def get_loader(config, prefix, is_test=False, num_workers=0, collate_fn=cmed_collate_fn):\n",
    "    dataset = CMEDDataset(config, prefix, is_test, tokenizer)\n",
    "    if not is_test:\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                                 batch_size=config.batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "    else:\n",
    "        data_loader = DataLoader(dataset=dataset,\n",
    "                                 batch_size=1,\n",
    "                                 shuffle=False,\n",
    "                                 pin_memory=True,\n",
    "                                 num_workers=num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class DataPreFetcher(object):\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.next_data = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.next_data = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            for k, v in self.next_data.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    self.next_data[k] = self.next_data[k].cuda(non_blocking=True)\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        data = self.next_data\n",
    "        self.preload()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KAtS4E4hVzVZ",
   "metadata": {
    "id": "KAtS4E4hVzVZ"
   },
   "source": [
    "## models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffqucRV00T",
   "metadata": {
    "executionInfo": {
     "elapsed": 908,
     "status": "ok",
     "timestamp": 1637201682877,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "89ffqucRV00T"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class Casrel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Casrel, self).__init__()\n",
    "        self.config = config\n",
    "        self.bert_dim = 768\n",
    "        self.bert_encoder = BertModel.from_pretrained(\"hfl/chinese-bert-wwm\", cache_dir=model_path)\n",
    "        # self.bert_encoder = BertTokenizer.from_pretrained(config.bert_path)\n",
    "        self.sub_heads_linear = nn.Linear(self.bert_dim, 1)\n",
    "        self.sub_tails_linear = nn.Linear(self.bert_dim, 1)\n",
    "        self.obj_heads_linear = nn.Linear(self.bert_dim, self.config.rel_num)\n",
    "        self.obj_tails_linear = nn.Linear(self.bert_dim, self.config.rel_num)\n",
    "        \n",
    "\n",
    "    def get_objs_for_specific_sub(self, sub_head_mapping, sub_tail_mapping, encoded_text):\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub_head = torch.matmul(sub_head_mapping, encoded_text)\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub_tail = torch.matmul(sub_tail_mapping, encoded_text)\n",
    "        # [batch_size, 1, bert_dim]\n",
    "        sub = (sub_head + sub_tail) / 2\n",
    "        # [batch_size, seq_len, bert_dim]\n",
    "        encoded_text = encoded_text + sub\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_heads = self.obj_heads_linear(encoded_text)\n",
    "        pred_obj_heads = torch.sigmoid(pred_obj_heads)\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_tails = self.obj_tails_linear(encoded_text)\n",
    "        pred_obj_tails = torch.sigmoid(pred_obj_tails)\n",
    "        return pred_obj_heads, pred_obj_tails\n",
    "\n",
    "    def get_encoded_text(self, token_ids, mask):\n",
    "        # [batch_size, seq_len, bert_dim(768)]\n",
    "        encoded_text = self.bert_encoder(token_ids, attention_mask=mask)[0]\n",
    "        return encoded_text\n",
    "\n",
    "    def get_subs(self, encoded_text):\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_heads = self.sub_heads_linear(encoded_text)\n",
    "        pred_sub_heads = torch.sigmoid(pred_sub_heads)\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_tails = self.sub_tails_linear(encoded_text)\n",
    "        pred_sub_tails = torch.sigmoid(pred_sub_tails)\n",
    "        return pred_sub_heads, pred_sub_tails\n",
    "\n",
    "    def forward(self, data):\n",
    "        # [batch_size, seq_len]\n",
    "        token_ids = data['token_ids']\n",
    "        # [batch_size, seq_len]\n",
    "        mask = data['mask']\n",
    "        # [batch_size, seq_len, bert_dim(768)]\n",
    "        encoded_text = self.get_encoded_text(token_ids, mask)\n",
    "        # [batch_size, seq_len, 1]\n",
    "        pred_sub_heads, pred_sub_tails = self.get_subs(encoded_text)\n",
    "        # [batch_size, 1, seq_len]\n",
    "        sub_head_mapping = data['sub_head'].unsqueeze(1)\n",
    "        # [batch_size, 1, seq_len]\n",
    "        sub_tail_mapping = data['sub_tail'].unsqueeze(1)\n",
    "        # [batch_size, seq_len, rel_num]\n",
    "        pred_obj_heads, pred_obj_tails = self.get_objs_for_specific_sub(sub_head_mapping, sub_tail_mapping, encoded_text)\n",
    "        return pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3cd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "class Framework(object):\n",
    "    def __init__(self, con):\n",
    "        self.config = con\n",
    "\n",
    "    def logging(self, s, print_=True, log_=True):\n",
    "        if print_:\n",
    "            print(s)\n",
    "        if log_:\n",
    "            with open(os.path.join(self.config.log_dir, self.config.log_save_name), 'a+') as f_log:f_log.write(s + '\\n')\n",
    "\n",
    "    def train(self, model_pattern):\n",
    "        # initialize the model\n",
    "        ori_model = model_pattern(self.config)\n",
    "        ori_model.cuda()\n",
    "\n",
    "        # define the optimizer\n",
    "        optimizer = optim.Adam(filter(lambda p: p.requires_grad, ori_model.parameters()), lr=self.config.learning_rate)\n",
    "\n",
    "        # whether use multi GPU\n",
    "        if self.config.multi_gpu:\n",
    "            model = nn.DataParallel(ori_model)\n",
    "        else:\n",
    "            model = ori_model\n",
    "\n",
    "        # define the loss function\n",
    "        def loss(gold, pred, mask):\n",
    "            pred = pred.squeeze(-1)\n",
    "            los = F.binary_cross_entropy(pred, gold, reduction='none')\n",
    "            if los.shape != mask.shape:\n",
    "                mask = mask.unsqueeze(-1)\n",
    "            los = torch.sum(los * mask) / torch.sum(mask)\n",
    "            return los\n",
    "\n",
    "        # check the checkpoint dir\n",
    "        if not os.path.exists(self.config.checkpoint_dir):\n",
    "            os.mkdir(self.config.checkpoint_dir)\n",
    "\n",
    "        # check the log dir\n",
    "        if not os.path.exists(self.config.log_dir):\n",
    "            os.mkdir(self.config.log_dir)\n",
    "\n",
    "        # get the data loader\n",
    "        train_data_loader = get_loader(self.config, prefix=self.config.train_prefix)\n",
    "        dev_data_loader = get_loader(self.config, prefix=self.config.dev_prefix, is_test=True)\n",
    "\n",
    "        # other\n",
    "        model.train()\n",
    "        global_step = 0\n",
    "        loss_sum = 0\n",
    "\n",
    "        best_f1_score = 0\n",
    "        best_precision = 0\n",
    "        best_recall = 0\n",
    "\n",
    "        best_epoch = 0\n",
    "        init_time = time.time()\n",
    "        start_time = time.time()\n",
    "\n",
    "        # the training loop\n",
    "        for epoch in range(self.config.max_epoch):\n",
    "            train_data_prefetcher = DataPreFetcher(train_data_loader)\n",
    "            data = train_data_prefetcher.next()\n",
    "            while data is not None:\n",
    "                pred_sub_heads, pred_sub_tails, pred_obj_heads, pred_obj_tails = model(data)\n",
    "\n",
    "                sub_heads_loss = loss(data['sub_heads'], pred_sub_heads, data['mask'])\n",
    "                sub_tails_loss = loss(data['sub_tails'], pred_sub_tails, data['mask'])\n",
    "                obj_heads_loss = loss(data['obj_heads'], pred_obj_heads, data['mask'])\n",
    "                obj_tails_loss = loss(data['obj_tails'], pred_obj_tails, data['mask'])\n",
    "                total_loss = (sub_heads_loss + sub_tails_loss) + (obj_heads_loss + obj_tails_loss)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                global_step += 1\n",
    "                loss_sum += total_loss.item()\n",
    "\n",
    "                if global_step % self.config.period == 0:\n",
    "                    cur_loss = loss_sum / self.config.period\n",
    "                    elapsed = time.time() - start_time\n",
    "                    self.logging(\"epoch: {:3d}, step: {:4d}, speed: {:5.2f}ms/b, train loss: {:5.3f}\".\n",
    "                                 format(epoch, global_step, elapsed * 1000 / self.config.period, cur_loss))\n",
    "                    loss_sum = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "                data = train_data_prefetcher.next()\n",
    "\n",
    "            if (epoch + 1) % self.config.test_epoch == 0:\n",
    "                eval_start_time = time.time()\n",
    "                model.eval()\n",
    "                # call the test function\n",
    "                precision, recall, f1_score = self.test(dev_data_loader, model)\n",
    "                model.train()\n",
    "                self.logging('epoch {:3d}, eval time: {:5.2f}s, f1: {:4.2f}, precision: {:4.2f}, recall: {:4.2f}'.\n",
    "                             format(epoch, time.time() - eval_start_time, f1_score, precision, recall))\n",
    "\n",
    "                if f1_score > best_f1_score:\n",
    "                    best_f1_score = f1_score\n",
    "                    best_epoch = epoch\n",
    "                    best_precision = precision\n",
    "                    best_recall = recall\n",
    "                    self.logging(\"saving the model, epoch: {:3d}, best f1: {:4.2f}, precision: {:4.2f}, recall: {:4.2f}\".\n",
    "                                 format(best_epoch, best_f1_score, precision, recall))\n",
    "                    # save the best model\n",
    "                    path = os.path.join(self.config.checkpoint_dir, self.config.model_save_name)\n",
    "                    if not self.config.debug:\n",
    "                        torch.save(ori_model.state_dict(), path)\n",
    "\n",
    "            # manually release the unused cache\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        self.logging(\"finish training\")\n",
    "        self.logging(\"best epoch: {:3d}, best f1: {:4.2f}, precision: {:4.2f}, recall: {:4.2}, total time: {:5.2f}s\".format(best_epoch, best_f1_score, best_precision, best_recall, time.time() - init_time))\n",
    "\n",
    "    def test(self, test_data_loader, model, output=False, h_bar=0.5, t_bar=0.5):\n",
    "\n",
    "        if output:\n",
    "            # check the result dir\n",
    "            if not os.path.exists(self.config.result_dir):\n",
    "                os.mkdir(self.config.result_dir)\n",
    "\n",
    "            path = os.path.join(self.config.result_dir, self.config.result_save_name)\n",
    "\n",
    "            fw = open(path, 'w')\n",
    "\n",
    "        orders = ['subject', 'relation', 'object']\n",
    "\n",
    "        def to_tup(triple_list):\n",
    "            ret = []\n",
    "            for triple in triple_list:\n",
    "                ret.append(tuple(triple))\n",
    "            return ret\n",
    "\n",
    "        test_data_prefetcher = DataPreFetcher(test_data_loader)\n",
    "        data = test_data_prefetcher.next()\n",
    "        id2rel = json.load(open(os.path.join(self.config.data_path, 'rel2id.json')))[0]\n",
    "        correct_num, predict_num, gold_num = 0, 0, 0\n",
    "\n",
    "        while data is not None:\n",
    "            with torch.no_grad():\n",
    "                token_ids = data['token_ids']\n",
    "                tokens = data['tokens'][0]\n",
    "                mask = data['mask']\n",
    "                encoded_text = model.get_encoded_text(token_ids, mask)\n",
    "                pred_sub_heads, pred_sub_tails = model.get_subs(encoded_text)\n",
    "                sub_heads, sub_tails = np.where(pred_sub_heads.cpu()[0] > h_bar)[0], np.where(pred_sub_tails.cpu()[0] > t_bar)[0]\n",
    "                subjects = []\n",
    "                for sub_head in sub_heads:\n",
    "                    sub_tail = sub_tails[sub_tails >= sub_head]\n",
    "                    if len(sub_tail) > 0:\n",
    "                        sub_tail = sub_tail[0]\n",
    "                        subject = tokens[sub_head: sub_tail]\n",
    "                        subjects.append((subject, sub_head, sub_tail))\n",
    "                if subjects:\n",
    "                    triple_list = []\n",
    "                    # [subject_num, seq_len, bert_dim]\n",
    "                    repeated_encoded_text = encoded_text.repeat(len(subjects), 1, 1)\n",
    "                    # [subject_num, 1, seq_len]\n",
    "                    sub_head_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()\n",
    "                    sub_tail_mapping = torch.Tensor(len(subjects), 1, encoded_text.size(1)).zero_()\n",
    "                    for subject_idx, subject in enumerate(subjects):\n",
    "                        sub_head_mapping[subject_idx][0][subject[1]] = 1\n",
    "                        sub_tail_mapping[subject_idx][0][subject[2]] = 1\n",
    "                    sub_tail_mapping = sub_tail_mapping.to(repeated_encoded_text)\n",
    "                    sub_head_mapping = sub_head_mapping.to(repeated_encoded_text)\n",
    "                    pred_obj_heads, pred_obj_tails = model.get_objs_for_specific_sub(sub_head_mapping, sub_tail_mapping, repeated_encoded_text)\n",
    "                    for subject_idx, subject in enumerate(subjects):\n",
    "                        sub = subject[0]\n",
    "                        sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
    "                        sub = ' '.join(sub.split('[unused1]'))\n",
    "                        obj_heads, obj_tails = np.where(pred_obj_heads.cpu()[subject_idx] > h_bar), np.where(pred_obj_tails.cpu()[subject_idx] > t_bar)\n",
    "                        for obj_head, rel_head in zip(*obj_heads):\n",
    "                            for obj_tail, rel_tail in zip(*obj_tails):\n",
    "                                if obj_head <= obj_tail and rel_head == rel_tail:\n",
    "                                    rel = id2rel[str(int(rel_head))]\n",
    "                                    obj = tokens[obj_head: obj_tail]\n",
    "                                    obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
    "                                    obj = ' '.join(obj.split('[unused1]'))\n",
    "                                    triple_list.append((sub, rel, obj))\n",
    "                                    break\n",
    "                    triple_set = set()\n",
    "                    for s, r, o in triple_list:\n",
    "                        triple_set.add((s, r, o))\n",
    "                    pred_list = list(triple_set)\n",
    "                else:\n",
    "                    pred_list = []\n",
    "                pred_triples = set(pred_list)\n",
    "                gold_triples = set(to_tup(data['triples'][0]))\n",
    "\n",
    "                correct_num += len(pred_triples & gold_triples)\n",
    "                predict_num += len(pred_triples)\n",
    "                gold_num += len(gold_triples)\n",
    "\n",
    "                if output:\n",
    "                    result = json.dumps({\n",
    "                        # 'text': ' '.join(tokens),\n",
    "                        'triple_list_gold': [\n",
    "                            dict(zip(orders, triple)) for triple in gold_triples\n",
    "                        ],\n",
    "                        'triple_list_pred': [\n",
    "                            dict(zip(orders, triple)) for triple in pred_triples\n",
    "                        ],\n",
    "                        'new': [\n",
    "                            dict(zip(orders, triple)) for triple in pred_triples - gold_triples\n",
    "                        ],\n",
    "                        'lack': [\n",
    "                            dict(zip(orders, triple)) for triple in gold_triples - pred_triples\n",
    "                        ]\n",
    "                    }, ensure_ascii=False)\n",
    "                    fw.write(result + '\\n')\n",
    "\n",
    "                data = test_data_prefetcher.next()\n",
    "\n",
    "        print(\"correct_num: {:3d}, predict_num: {:3d}, gold_num: {:3d}\".format(correct_num, predict_num, gold_num))\n",
    "\n",
    "        precision = correct_num / (predict_num + 1e-10)\n",
    "        recall = correct_num / (gold_num + 1e-10)\n",
    "        f1_score = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "        return precision, recall, f1_score\n",
    "\n",
    "    def testall(self, model_pattern):\n",
    "        model = model_pattern(self.config)\n",
    "        path = os.path.join(self.config.checkpoint_dir, self.config.model_save_name)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "        test_data_loader = get_loader(self.config, prefix=self.config.test_prefix, is_test=True)\n",
    "        precision, recall, f1_score = self.test(test_data_loader, model, True)\n",
    "        print(\"f1: {:4.2f}, precision: {:4.2f}, recall: {:4.2f}\".format(f1_score, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZF1kHBQvTZwd",
   "metadata": {
    "id": "ZF1kHBQvTZwd"
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R7d6wToIV8xp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 28443,
     "status": "error",
     "timestamp": 1637203505903,
     "user": {
      "displayName": "周三甫",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgNfAkDFnkyBOheBYb2h_gVzUKvDaftIG2_kq3p=s64",
      "userId": "06644976248886113218"
     },
     "user_tz": -480
    },
    "id": "R7d6wToIV8xp",
    "outputId": "bf4f2055-3b64-4254-fa26-0fc9d0ab6009"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "config = {\n",
    "    'model_name' : 'Casrel',\n",
    "    'lr' : 1e-5,\n",
    "    'learning_rate' : 1e-5,\n",
    "    'multi_gpu' : False,\n",
    "    'dataset' : 'CMED',\n",
    "    'batch_size' : 4,\n",
    "    'max_epoch' : 5,\n",
    "    'test_epoch' : 20,\n",
    "    'train_prefix' : 'my_train_triples',\n",
    "    'dev_prefix' : 'dev_triples',\n",
    "    'test_prefix' : 'my_train_triples',\n",
    "    'max_len' : 512,\n",
    "    'rel_num' : 44,\n",
    "    'period' : 50,\n",
    "    'debug' : False,\n",
    "    'bert_path' : model_path\n",
    "}\n",
    "config['checkpoint_dir'] = base_path + '/checkpoint'\n",
    "config['log_dir'] = base_path + '/log'\n",
    "config['data_path'] = data_path\n",
    "config['result_dir'] = data_path \n",
    "config['model_save_name'] = config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "config['log_save_name'] = \"LOG_\" + config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "config['result_save_name'] = \"RESULT_\" + config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "config = argparse.Namespace(**config)\n",
    "framework = Framework(config)\n",
    "framework.train(Casrel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X0yvJW7cV5t1",
   "metadata": {
    "id": "X0yvJW7cV5t1"
   },
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "config = {\n",
    "    'model_name' : 'Casrel',\n",
    "    'lr' : 1e-5,\n",
    "    'learning_rate' : 1e-5,\n",
    "    'multi_gpu' : False,\n",
    "    'dataset' : 'CMED',\n",
    "    'batch_size' : 6,\n",
    "    'max_epoch' : 5,\n",
    "    'test_epoch' : 5,\n",
    "    'train_prefix' : 'my_train_triples',\n",
    "    'dev_prefix' : 'dev_triples',\n",
    "    'test_prefix' : 'my_test_triples',\n",
    "    'max_len' : 150,\n",
    "    'rel_num' : 44,\n",
    "    'period' : 50,\n",
    "    'debug' : False,\n",
    "    'bert_path' : model_path\n",
    "}\n",
    "config['checkpoint_dir'] = base_path + '/checkpoint'\n",
    "config['log_dir'] = base_path + '/log'\n",
    "config['data_path'] = data_path\n",
    "config['result_dir'] = data_path \n",
    "config['model_save_name'] = config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "config['log_save_name'] = \"LOG_\" + config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "config['result_save_name'] = \"RESULT_\" + config['model_name'] + '_DATASET_' + config['dataset'] + \"_LR_\" + str(config['lr']) + \"_BS_\" + str(config['batch_size'])\n",
    "config = argparse.Namespace(**config)\n",
    "\n",
    "framework = Framework(config)\n",
    "framework.testall(Casrel)\n",
    "\n",
    "# test_data_loader = get_loader(config, prefix=config.test_prefix)\n",
    "# print(test_data_loader)\n",
    "# framework = Framework(config)\n",
    "# framework.test(test_data_loader=config.test_prefix, model=Casrel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ed80e",
   "metadata": {},
   "source": [
    "## 数据生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a915237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    line = str(line)\n",
    "    if line.strip()=='':\n",
    "        return ''\n",
    "    #rule = re.compile(u\"\"\"[^a-zA-Z0-9\\u4E00-\\u9FA5]|[a-zA-Z0-9'!\"#$%&\\'()*+,-./:;<=>?@，。?★、…【】《》？“”‘'！[\\\\]^_`{|}~\\s]+|[\\001\\002\\003\\004\\005\\006\\007\\x08\\x09\\x0a\\x0b\\x0c\\x0d\\x0e\\x0f\\x10\\x11\\x12\\x13\\x14\\x15\\x16\\x17\\x18\\x19\\x1a]+\"\"\")\n",
    "    #line = rule.sub('',line)\n",
    "    # line = line.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "    return line\n",
    "\n",
    "with open(base_path+'/data/MyData/茂名人民医院重症专病入院记录_额外原文.txt', 'r')as fr:\n",
    "    text_list = fr.readlines()\n",
    "fr.close()\n",
    "\n",
    "text_data = []\n",
    "for i in text_list:\n",
    "    try:\n",
    "        tmp_text = []\n",
    "        i = eval(i)\n",
    "        for k, v in i.items():\n",
    "            tmp_text.append(v)\n",
    "        text_data.append(\" \".join(jieba.cut(remove_punctuation(' '.join(tmp_text)))))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "test_data = []\n",
    "for i in text_data:\n",
    "    test_data.append({'text': i,'triple_list': [['subject_placeholder','relation_placeholder','object_placeholder']]})\n",
    "print(test_data[0])\n",
    "print(len(test_data))\n",
    "\n",
    "with open(base_path+'/data/CMED/my_test_triples.json', 'w')as fw:\n",
    "    json.dump(test_data, fw, ensure_ascii=False)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec78c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jieba\n",
    "\n",
    "with open(base_path+'/data/CMeIE/CMeIE_train.json', 'r')as fr:\n",
    "    train_list = fr.readlines()\n",
    "fr.close()\n",
    "\n",
    "with open(base_path+'/data/CMeIE/CMeIE_dev.json', 'r')as fr:\n",
    "    add_dev_list = fr.readlines()\n",
    "fr.close()\n",
    "\n",
    "with open(base_path+'/data/CMED/train_triples.json', 'r')as fr:\n",
    "    add_train_list = fr.readlines()\n",
    "fr.close()\n",
    "\n",
    "train_list.extend(add_dev_list)\n",
    "print(train_list[0])\n",
    "\n",
    "set_data = []\n",
    "for i in train_list:\n",
    "    try:\n",
    "        tmp_dict = json.loads(i)\n",
    "        for k, v in tmp_dict.items():\n",
    "            if k == 'text':\n",
    "                t_text = \" \".join(jieba.cut(v))\n",
    "            elif k == 'spo_list':\n",
    "                for i in v:\n",
    "                    t_object = i['object']['@value']\n",
    "                    t_predicate = i['predicate']\n",
    "                    t_subject = i['subject']\n",
    "                tmp_text = {'text': t_text,'triple_list': [[\" \".join(jieba.cut(t_object)),t_predicate,\" \".join(jieba.cut(t_subject))]]}\n",
    "                set_data.append(tmp_text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "train_list.extend(add_train_list)\n",
    "print(len(set_data), set_data[0])\n",
    "with open(base_path+'/data/CMED/my_train_triples.json', 'w')as fw:\n",
    "    json.dump(set_data, fw, ensure_ascii=False)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307e2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel2id = json.load(open(base_path+'/data/CMED/my_train_triples.json'))\n",
    "print(len(rel2id))\n",
    "rel_id = []\n",
    "for i in rel2id:\n",
    "    rel_id.append(i['triple_list'][0][1])\n",
    "rel_id = list(set(rel_id))\n",
    "print(rel_id)\n",
    "print(len(rel_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88e047-981f-4428-90c9-b85b52014893",
   "metadata": {},
   "source": [
    "# 四、苏神bert4keras\n",
    "模型设计过程如下：基于“半指针-半标注”的方式来做抽取，顺序是先抽取s，然后传入s来抽取o、p，不同的只是将模型的整体架构换成了bert：\n",
    "1、原始序列转id后，传入bert的编码器，得到编码序列；\n",
    "2、编码序列接两个二分类器，预测s；\n",
    "3、根据传入的s，从编码序列中抽取出s的首和尾对应的编码向量；\n",
    "4、以s的编码向量作为条件，对编码序列做一次条件Layer Norm；\n",
    "5、条件Layer Norm后的序列来预测该s对应的o、p。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039356a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aid/Github/NLP_relation_extraction/data/CMeIE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_path = '/home/aid/Github/NLP_relation_extraction'#os.path.dirname(os.getcwd())\n",
    "data_path = base_path + '/data/CMeIE'#'content/CMED'\n",
    "model_path = base_path + '/models/chinese-bert-wwm'#'content/Models/chinese-bert-wwm'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f843ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    bert4keras==0.7.8\n",
    "    Keras==2.2.4\n",
    "    numpy==1.16.4\n",
    "    tensorflow-gpu==1.14.0\n",
    "    tqdm==4.43.0\n",
    "'''\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, Reshape\n",
    "from bert4keras.snippets import open\n",
    "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
    "from bert4keras.models import build_transformer_model\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from bert4keras.layers import LayerNormalization\n",
    "from bert4keras.backend import K, batch_gather\n",
    "from bert4keras.snippets import DataGenerator, sequence_padding\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "rootPath = os.path.dirname(os.path.abspath(__file__)) # os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "modelsPath = rootPath + '/models_tmp/'\n",
    "dataPath = rootPath + '/data'\n",
    "\n",
    "class ReextractBertTrainHandler():\n",
    "    def __init__(self, params, Train=False):\n",
    "        self.bert_config_path = modelsPath + \"chinese_L-12_H-768_A-12/bert_config.json\"\n",
    "        self.bert_checkpoint_path = modelsPath + \"chinese_L-12_H-768_A-12/bert_model.ckpt\"\n",
    "        self.bert_vocab_path = modelsPath + \"chinese_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.tokenizer = Tokenizer(self.bert_vocab_path, do_lower_case=True)\n",
    "        self.model_path = modelsPath + \"best_model.weights\"\n",
    "        self.params_path = modelsPath + 'params.json'\n",
    "        gpu_id = params.get(\"gpu_id\", None)\n",
    "        print(\"-----> 选择的gpu_id===\", gpu_id)\n",
    "        self._set_gpu_id(gpu_id)  # 设置训练的GPU_ID\n",
    "        self.memory_fraction = params.get('memory_fraction')\n",
    "        if Train:\n",
    "            self.train_data_file_path = params.get('train_data_path')\n",
    "            self.valid_data_file_path = params.get('valid_data_path')\n",
    "            self.maxlen = params.get('maxlen', 128)\n",
    "            self.batch_size = params.get('batch_size', 32)\n",
    "            self.epoch = params.get('epoch')\n",
    "            self.data_process()\n",
    "        else:\n",
    "            load_params = json.load(open(self.params_path, encoding='utf-8'))\n",
    "            self.maxlen = load_params.get('maxlen')\n",
    "            self.num_classes = load_params.get('num_classes')\n",
    "            self.p2s_dict = load_params.get('p2s_dict')\n",
    "            self.i2p_dict = load_params.get('i2p_dict')\n",
    "            self.p2o_dict = load_params.get('p2o_dict')\n",
    "        self.build_model()\n",
    "        if not Train:\n",
    "            self.load_model()\n",
    "\n",
    "    def _set_gpu_id(self, gpu_id):\n",
    "        if gpu_id:\n",
    "            print(\"---> gpu_id:\", gpu_id, os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "\n",
    "    def data_process(self):\n",
    "        self.train_data, self.valid_data, self.p2s_dict, self.p2o_dict, self.i2p_dict, self.p2i_dict = data_process(\n",
    "            self.train_data_file_path, self.valid_data_file_path, self.maxlen, self.params_path)\n",
    "        self.num_classes = len(self.i2p_dict)\n",
    "        self.train_generator = Data_Generator(self.train_data, self.batch_size, self.tokenizer, self.p2i_dict,\n",
    "                                              self.maxlen)\n",
    "\n",
    "    def extrac_subject(self, inputs):\n",
    "        \"\"\"根据subject_ids从output中取出subject的向量表征\n",
    "        \"\"\"\n",
    "        output, subject_ids = inputs\n",
    "        subject_ids = K.cast(subject_ids, 'int32')\n",
    "        start = batch_gather(output, subject_ids[:, :1])\n",
    "        end = batch_gather(output, subject_ids[:, 1:])\n",
    "        subject = K.concatenate([start, end], 2)\n",
    "        return subject[:, 0]\n",
    "\n",
    "    def build_model(self):\n",
    "        import tensorflow as tf\n",
    "        from keras.backend.tensorflow_backend import set_session\n",
    "        config = tf.ConfigProto()\n",
    "        # A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "        config.gpu_options.allocator_type = 'BFC'\n",
    "        if self.memory_fraction:\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = self.memory_fraction\n",
    "            config.gpu_options.allow_growth = False\n",
    "        else:\n",
    "            print('------> no memory_fraction')\n",
    "            # 重点：设置动态分配GPU\n",
    "            config.gpu_options.allow_growth = True\n",
    "            # 设置最大占有GPU不超过显存的80%\n",
    "            config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "        set_session(tf.Session(config=config))\n",
    "\n",
    "        # 补充输入\n",
    "        subject_labels = Input(shape=(None, 2), name='Subject-Labels')\n",
    "        subject_ids = Input(shape=(2,), name='Subject-Ids')\n",
    "        object_labels = Input(\n",
    "            shape=(None, self.num_classes, 2), name='Object-Labels')\n",
    "        # 加载预训练模型\n",
    "        bert = build_transformer_model(\n",
    "            config_path=self.bert_config_path,\n",
    "            checkpoint_path=self.bert_checkpoint_path,\n",
    "            return_keras_model=False,\n",
    "        )\n",
    "        # 预测subject\n",
    "        output = Dense(units=2,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=bert.initializer)(bert.model.output)\n",
    "        subject_preds = Lambda(lambda x: x ** 2)(output)\n",
    "        self.subject_model = Model(bert.model.inputs, subject_preds)\n",
    "        # 传入subject，预测object\n",
    "        # 通过Conditional Layer Normalization将subject融入到object的预测中\n",
    "        output = bert.model.layers[-2].get_output_at(-1)\n",
    "        subject = Lambda(self.extrac_subject)([output, subject_ids])\n",
    "        output = LayerNormalization(conditional=True)([output, subject])\n",
    "        output = Dense(units=self.num_classes * 2,\n",
    "                       activation='sigmoid',\n",
    "                       kernel_initializer=bert.initializer)(output)\n",
    "        output = Lambda(lambda x: x ** 4)(output)\n",
    "        object_preds = Reshape((-1, self.num_classes, 2))(output)\n",
    "        self.object_model = Model(\n",
    "            bert.model.inputs + [subject_ids], object_preds)\n",
    "        # 训练模型\n",
    "        self.train_model = Model(bert.model.inputs + [subject_labels, subject_ids, object_labels],\n",
    "                                 [subject_preds, object_preds])\n",
    "        mask = bert.model.get_layer('Embedding-Token').output_mask\n",
    "        mask = K.cast(mask, K.floatx())\n",
    "        subject_loss = K.binary_crossentropy(subject_labels, subject_preds)\n",
    "        subject_loss = K.mean(subject_loss, 2)\n",
    "        subject_loss = K.sum(subject_loss * mask) / K.sum(mask)\n",
    "        object_loss = K.binary_crossentropy(object_labels, object_preds)\n",
    "        object_loss = K.sum(K.mean(object_loss, 3), 2)\n",
    "        object_loss = K.sum(object_loss * mask) / K.sum(mask)\n",
    "        self.train_model.add_loss(subject_loss + object_loss)\n",
    "        AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
    "        self.optimizer = AdamEMA(lr=1e-4)\n",
    "        self.train_model.compile(optimizer=self.optimizer)\n",
    "\n",
    "    def load_model(self):\n",
    "        self.train_model.load_weights(self.model_path)\n",
    "\n",
    "    def predict(self, text):\n",
    "        \"\"\"\n",
    "        抽取输入text所包含的三元组\n",
    "        text：str(<离开>是由张宇谱曲，演唱)\n",
    "        \"\"\"\n",
    "        # print('--->', text)\n",
    "        tokens = self.tokenizer.tokenize(text, max_length=self.maxlen)\n",
    "        token_ids, segment_ids = self.tokenizer.encode(\n",
    "            text, max_length=self.maxlen)\n",
    "        # 抽取subject\n",
    "        subject_preds = self.subject_model.predict(\n",
    "            [[token_ids], [segment_ids]])\n",
    "        # print('---->', subject_preds)\n",
    "        start = np.where(subject_preds[0, :, 0] > 0.6)[0]\n",
    "        end = np.where(subject_preds[0, :, 1] > 0.5)[0]\n",
    "        subjects = []\n",
    "        for i in start:\n",
    "            j = end[end >= i]\n",
    "            if len(j) > 0:\n",
    "                j = j[0]\n",
    "                subjects.append((i, j))\n",
    "        if subjects:\n",
    "            spoes = []\n",
    "            token_ids = np.repeat([token_ids], len(subjects), 0)\n",
    "            segment_ids = np.repeat([segment_ids], len(subjects), 0)\n",
    "            subjects = np.array(subjects)\n",
    "            # 传入subject，抽取object和predicate\n",
    "            object_preds = self.object_model.predict(\n",
    "                [token_ids, segment_ids, subjects])\n",
    "            for subject, object_pred in zip(subjects, object_preds):\n",
    "                start = np.where(object_pred[:, :, 0] > 0.6)\n",
    "                end = np.where(object_pred[:, :, 1] > 0.5)\n",
    "                for _start, predicate1 in zip(*start):\n",
    "                    for _end, predicate2 in zip(*end):\n",
    "                        if _start <= _end and predicate1 == predicate2:\n",
    "                            spoes.append((subject, predicate1, (_start, _end)))\n",
    "                            break\n",
    "            i2p_values = []\n",
    "            for k, v in self.i2p_dict.items():\n",
    "                i2p_values.append(v)\n",
    "\n",
    "            return [\n",
    "                (\n",
    "                    [self.tokenizer.decode(token_ids[0, s[0]:s[1] + 1], tokens[s[0]:s[1] + 1]),\n",
    "                     self.p2s_dict[i2p_values[p]]],\n",
    "                    i2p_values[p],\n",
    "                    [self.tokenizer.decode(token_ids[0, o[0]:o[1] + 1], tokens[o[0]:o[1] + 1]),\n",
    "                     self.p2o_dict[i2p_values[p]]],\n",
    "                    (s[0], s[1] + 1),\n",
    "                    (o[0], o[1] + 1)\n",
    "                ) for s, p, o in spoes\n",
    "            ]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def train(self):\n",
    "        evaluator = Evaluator(self.train_model, self.model_path, self.tokenizer, self.predict, self.optimizer,\n",
    "                              self.valid_data)\n",
    "\n",
    "        self.train_model.fit_generator(self.train_generator.forfit(),\n",
    "                                       steps_per_epoch=len(\n",
    "                                           self.train_generator),\n",
    "                                       epochs=self.epoch,\n",
    "                                       callbacks=[evaluator])\n",
    "        \n",
    "class Data_Generator(DataGenerator):\n",
    "    \"\"\"数据生成器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, batch_size, tokenizer, p2i_dict, maxlen):\n",
    "        super().__init__(data, batch_size=batch_size)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.p2i_dict = p2i_dict\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def sample(self, random=False):\n",
    "        \"\"\"采样函数，每个样本同时返回一个is_end标记\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            if self.steps is None:\n",
    "\n",
    "                def generator():\n",
    "                    caches, isfull = [], False\n",
    "                    for d in self.data:\n",
    "                        caches.append(d)\n",
    "                        if isfull:\n",
    "                            i = np.random.randint(len(caches))\n",
    "                            yield caches.pop(i)\n",
    "                        # elif len(caches) == self.buffer_size:\n",
    "                        #     isfull = True\n",
    "                    while caches:\n",
    "                        i = np.random.randint(len(caches))\n",
    "                        yield caches.pop(i)\n",
    "\n",
    "            else:\n",
    "\n",
    "                def generator():\n",
    "                    indices = list(range(len(self.data)))\n",
    "                    np.random.shuffle(indices)\n",
    "                    for i in indices:\n",
    "                        yield self.data[i]\n",
    "\n",
    "            data = generator()\n",
    "        else:\n",
    "            data = iter(self.data)\n",
    "\n",
    "        d_current = next(data)\n",
    "        for d_next in data:\n",
    "            yield False, d_current\n",
    "            d_current = d_next\n",
    "\n",
    "        yield True, d_current\n",
    "\n",
    "    def __iter__(self, random=False):\n",
    "        batch_token_ids, batch_segment_ids = [], []\n",
    "        batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "        for is_end, d in self.sample(random):\n",
    "            token_ids, segment_ids = self.tokenizer.encode(first_text=d['text'], max_length=self.maxlen)\n",
    "            # 整理三元组 {s: [(o_start,0_end, p)]}/{s_token_ids:[]}\n",
    "            spoes = {}\n",
    "            for spo in d['new_spo_list']:\n",
    "                s = spo['s']\n",
    "                p = spo['p']\n",
    "                o = spo['o']\n",
    "                s_token = self.tokenizer.encode(s['entity'])[0][1:-1]\n",
    "                p = self.p2i_dict[p['entity']]\n",
    "                o_token = self.tokenizer.encode(o['entity'])[0][1:-1]\n",
    "                s_idx = search(s_token, token_ids)  # s_idx s起始位置\n",
    "                o_idx = search(o_token, token_ids)  # o_idx o起始位置\n",
    "                if s_idx != -1 and o_idx != -1:\n",
    "                    s = (s_idx, s_idx + len(s_token) - 1)  # s s起始结束位置，s的类别\n",
    "                    o = (o_idx, o_idx + len(o_token) - 1, p)  # o o起始结束位置及p的id,o的类别\n",
    "                    if s not in spoes:\n",
    "                        spoes[s] = []\n",
    "                    spoes[s].append(o)\n",
    "            if spoes:\n",
    "                # subject标签，采用二维向量分别标记subject的起始位置和结束位置\n",
    "                subject_labels = np.zeros((len(token_ids), 2))\n",
    "                for s in spoes:\n",
    "                    subject_labels[s[0], 0] = 1\n",
    "                    subject_labels[s[1], 1] = 1\n",
    "                # 随机选一个subject\n",
    "                start, end = np.array(list(spoes.keys())).T\n",
    "                start = np.random.choice(start)\n",
    "                end = np.random.choice(end[end >= start])\n",
    "                subject_ids = (start, end)\n",
    "                # 对应的object标签\n",
    "                object_labels = np.zeros((len(token_ids), len(self.p2i_dict), 2))\n",
    "                for o in spoes.get(subject_ids, []):\n",
    "                    object_labels[o[0], o[2], 0] = 1\n",
    "                    object_labels[o[1], o[2], 1] = 1\n",
    "                # 构建batch\n",
    "                batch_token_ids.append(token_ids)\n",
    "                batch_segment_ids.append(segment_ids)\n",
    "                batch_subject_labels.append(subject_labels)\n",
    "                batch_subject_ids.append(subject_ids)\n",
    "                batch_object_labels.append(object_labels)\n",
    "                if len(batch_token_ids) == self.batch_size or is_end:\n",
    "                    batch_token_ids = sequence_padding(batch_token_ids)\n",
    "                    batch_segment_ids = sequence_padding(batch_segment_ids)\n",
    "                    batch_subject_labels = sequence_padding(batch_subject_labels, padding=np.zeros(2))\n",
    "                    batch_subject_ids = np.array(batch_subject_ids)\n",
    "                    batch_object_labels = sequence_padding(batch_object_labels,\n",
    "                                                           padding=np.zeros((3, 2)))\n",
    "                    yield [\n",
    "                              batch_token_ids, batch_segment_ids,\n",
    "                              batch_subject_labels, batch_subject_ids, batch_object_labels\n",
    "\n",
    "                          ], None\n",
    "                    batch_token_ids, batch_segment_ids = [], []\n",
    "                    batch_subject_labels, batch_subject_ids, batch_object_labels = [], [], []\n",
    "                    \n",
    "\n",
    "class Evaluator(keras.callbacks.Callback):\n",
    "    \"\"\"评估和保存模型\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, model_path, tokenizer,predict,optimizer,valid_data):\n",
    "        self.EMAer = optimizer\n",
    "        self.best_val_f1 = 0.\n",
    "        self.model = model\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.predict = predict\n",
    "        self.valid_data = valid_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.EMAer.apply_ema_weights()\n",
    "        f1, precision, recall = evaluate(self.tokenizer,self.valid_data,self.predict)\n",
    "        if f1 >= self.best_val_f1:\n",
    "            self.best_val_f1 = f1\n",
    "            self.model.save_weights(self.model_path)\n",
    "        self.EMAer.reset_old_weights()\n",
    "        print('f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f/n' %\n",
    "              (f1, precision, recall, self.best_val_f1))\n",
    "\n",
    "def search(pattern, sequence):\n",
    "    \"\"\"从sequence中寻找子串pattern\n",
    "    如果找到，返回第一个下标；否则返回-1。\n",
    "    \"\"\"\n",
    "    n = len(pattern)\n",
    "    for i in range(len(sequence)):\n",
    "        if sequence[i:i + n] == pattern:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def evaluate(tokenizer,data,predict):\n",
    "    \"\"\"评估函数，计算f1、precision、recall\n",
    "    \"\"\"\n",
    "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
    "    f = open('dev_pred.json', 'w', encoding='utf-8')\n",
    "    pbar = tqdm()\n",
    "\n",
    "    class SPO(tuple):\n",
    "        \"\"\"用来存三元组的类\n",
    "        表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，\n",
    "        使得在判断两个三元组是否等价时容错性更好。\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, spo):\n",
    "            self.spox = (\n",
    "                tuple(spo[0]),\n",
    "                spo[1],\n",
    "                tuple(spo[2]),\n",
    "            )\n",
    "\n",
    "        def __hash__(self):\n",
    "            return self.spox.__hash__()\n",
    "\n",
    "        def __eq__(self, spo):\n",
    "            return self.spox == spo.spox\n",
    "\n",
    "    for d in data:\n",
    "        R = set([SPO(spo) for spo in\n",
    "                 [[tokenizer.tokenize(spo_str[0][0]), spo_str[1], tokenizer.tokenize(spo_str[2][0])] for\n",
    "                  spo_str\n",
    "                  in predict(d['text'])]])\n",
    "        T = set([SPO(spo) for spo in\n",
    "                 [[tokenizer.tokenize(spo_str['s']['entity']), spo_str['p']['entity'],\n",
    "                   tokenizer.tokenize(spo_str['o']['entity'])] for spo_str\n",
    "                  in d['new_spo_list']]])\n",
    "        X += len(R & T)\n",
    "        Y += len(R)\n",
    "        Z += len(T)\n",
    "        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
    "        pbar.update()\n",
    "        pbar.set_description('f1: %.5f, precision: %.5f, recall: %.5f' %\n",
    "                             (f1, precision, recall))\n",
    "        s = json.dumps(\n",
    "            {\n",
    "                'text': d['text'],\n",
    "                'spo_list': list(T),\n",
    "                'spo_list_pred': list(R),\n",
    "                'new': list(R - T),\n",
    "                'lack': list(T - R),\n",
    "            },\n",
    "            ensure_ascii=False,\n",
    "            indent=4)\n",
    "        f.write(s + '/n')\n",
    "    pbar.close()\n",
    "    f.close()\n",
    "    return f1, precision, recall\n",
    "\n",
    "def data_process(train_data_file_path, valid_data_file_path, max_len, params_path):\n",
    "    train_data = json.load(open(train_data_file_path, encoding='utf-8'))\n",
    "\n",
    "    if valid_data_file_path:\n",
    "        train_data_ret = train_data\n",
    "        valid_data_ret = json.load(open(valid_data_file_path, encoding='utf-8'))\n",
    "    else:\n",
    "        split = int(len(train_data) * 0.8)\n",
    "        train_data_ret, valid_data_ret = train_data[:split], train_data[split:]\n",
    "    p2s_dict = {}\n",
    "    p2o_dict = {}\n",
    "    predicate = []\n",
    "\n",
    "    for content in train_data:\n",
    "        for spo in content.get('new_spo_list'):\n",
    "            s_type = spo.get('s').get('type')\n",
    "            p_key = spo.get('p').get('entity')\n",
    "            o_type = spo.get('o').get('type')\n",
    "            if p_key not in p2s_dict:\n",
    "                p2s_dict[p_key] = s_type\n",
    "            if p_key not in p2o_dict:\n",
    "                p2o_dict[p_key] = o_type\n",
    "            if p_key not in predicate:\n",
    "                predicate.append(p_key)\n",
    "    i2p_dict = {i: key for i, key in enumerate(predicate)}\n",
    "    p2i_dict = {key: i for i, key in enumerate(predicate)}\n",
    "    save_params = {}\n",
    "    save_params['p2s_dict'] = p2s_dict\n",
    "    save_params['i2p_dict'] = i2p_dict\n",
    "    save_params['p2o_dict'] = p2o_dict\n",
    "    save_params['maxlen'] = max_len\n",
    "    save_params['num_classes'] = len(i2p_dict)\n",
    "    # 数据保存\n",
    "    json.dump(save_params,\n",
    "              open(params_path, 'w', encoding='utf-8'),\n",
    "              ensure_ascii=False, indent=4)\n",
    "    return train_data_ret, valid_data_ret, p2s_dict, p2o_dict, i2p_dict, p2i_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # 训练\n",
    "    params = {\n",
    "        \"gpu_id\": 0,\n",
    "        \"maxlen\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"epoch\": 10,\n",
    "        \"train_data_path\": dataPath + \"/train_data.json\",\n",
    "        # \"valid_data_path\": dataPath + \"/valid_test.json\",\n",
    "        \"dev_data_path\": dataPath + \"/valid_data.json\",\n",
    "    }\n",
    "    model = ReextractBertTrainHandler(params, Train=True)\n",
    "    model.train()\n",
    "    \n",
    "    text = \"胃壁、肠管壁未见明确增厚及肿块影，肠腔未见异常扩张\"\n",
    "    print('-->结果：', model.predict(text))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "699773a2-c53d-43c3-9e70-9b9ee415314d",
    "pswNSZDbFrVC",
    "KAtS4E4hVzVZ"
   ],
   "name": "nlp_re_CBLUE_CMeIE_Sofa.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ef37f6f80445dd6573c38f877e770fd30cefd9344f2056ddffb031383f91db8c"
  },
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
