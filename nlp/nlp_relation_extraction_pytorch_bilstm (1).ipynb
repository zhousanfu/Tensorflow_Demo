{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abcea712-653d-4458-9e08-94e5e0ebe0a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pytorch版联合关系抽取。\n",
    "    zhengyima/kg-baseline-pytorch\n",
    "    苏神Keras版链接：https://github.com/bojone/kg-2019-baseline\n",
    "    用BiLSTM做联合标注，先预测subject，然后根据suject同时预测object和predicate，标注结构是“半指针-半标注”结构，以前也曾介绍过（ https://kexue.fm/archives/5409 ）\n",
    "    CSDN上基于本代码的算法简介：https://blog.csdn.net/qq_35268841/article/details/107063066"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863e6ab8-4e20-4142-b126-455e417c4be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aid/Github/NLP_relation_extraction\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "basePath = os.path.dirname(os.getcwd())\n",
    "dataPath = basePath + '/data/nlp_relation_extraction_百度人物'\n",
    "modelsPath = basePath + '/models'\n",
    "print(basePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a498368-4036-4c97-ad37-1f73601deb4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d05855e-ad75-40f5-9f7a-d445e9538d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 168988.88it/s]\n",
      "172983it [00:05, 32719.82it/s]\n",
      "21626it [00:00, 29333.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding:utf-8 -*-\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "\n",
    "\n",
    "all_50_schemas = set()\n",
    "with open(dataPath + '/all_50_schemas') as f:\n",
    "    for l in tqdm(f):\n",
    "        a = json.loads(l)\n",
    "        all_50_schemas.add(a['predicate'])\n",
    "\n",
    "id2predicate = {i+1:j for i,j in enumerate(all_50_schemas)} # 0表示终止类别\n",
    "predicate2id = {j:i for i,j in id2predicate.items()}\n",
    "with codecs.open(dataPath + '/all_50_schemas_me.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump([id2predicate, predicate2id], f, indent=4, ensure_ascii=False)\n",
    "\n",
    "chars = {}\n",
    "min_count = 2\n",
    "train_data = []\n",
    "with open(dataPath + '/train_data.json') as f:\n",
    "    for l in tqdm(f):\n",
    "        a = json.loads(l)\n",
    "        train_data.append(\n",
    "            {\n",
    "                'text': a['text'],\n",
    "                'spo_list': [(i['subject'], i['predicate'], i['object']) for i in a['spo_list']]\n",
    "            }\n",
    "        )\n",
    "        for c in a['text']:\n",
    "            chars[c] = chars.get(c, 0) + 1\n",
    "\n",
    "with codecs.open(dataPath + '/train_data_me.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "dev_data = []\n",
    "with open(dataPath + '/dev_data.json') as f:\n",
    "    for l in tqdm(f):\n",
    "        a = json.loads(l)\n",
    "        dev_data.append(\n",
    "            {\n",
    "                'text': a['text'],\n",
    "                'spo_list': [(i['subject'], i['predicate'], i['object']) for i in a['spo_list']]\n",
    "            }\n",
    "        )\n",
    "        for c in a['text']:\n",
    "            chars[c] = chars.get(c, 0) + 1\n",
    "\n",
    "with codecs.open(dataPath + '/dev_data_me.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dev_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "with codecs.open(dataPath + '/all_chars_me.json', 'w', encoding='utf-8') as f:\n",
    "    chars = {i:j for i,j in chars.items() if j >= min_count}\n",
    "    id2char = {i+2:j for i,j in enumerate(chars)} # padding: 0, unk: 1\n",
    "    char2id = {j:i for i,j in id2char.items()}\n",
    "    json.dump([id2char, char2id], f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903870d6-2d1d-43e0-9dbc-e3b59dc9039b",
   "metadata": {},
   "source": [
    "### 模型层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e093bfc3-1caa-4a70-a12a-64edb14f1b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def seq_max_pool(x):\n",
    "    \"\"\"seq是[None, seq_len, s_size]的格式，\n",
    "    mask是[None, seq_len, 1]的格式，先除去mask部分，\n",
    "    然后再做maxpooling。\n",
    "    \"\"\"\n",
    "    seq, mask = x\n",
    "    seq = seq - (1 - mask) * 1e10\n",
    "    return torch.max(seq, 1)\n",
    "\n",
    "def seq_and_vec(x):\n",
    "    \"\"\"seq是[None, seq_len, s_size]的格式，\n",
    "    vec是[None, v_size]的格式，将vec重复seq_len次，拼到seq上，\n",
    "    得到[None, seq_len, s_size+v_size]的向量。\n",
    "    \"\"\"\n",
    "    seq , vec  = x\n",
    "    vec = torch.unsqueeze(vec,1)\n",
    "    \n",
    "    vec = torch.zeros_like(seq[:, :, :1]) + vec\n",
    "    return torch.cat([seq, vec], 2)\n",
    "\n",
    "def seq_gather(x):\n",
    "    \"\"\"seq是[None, seq_len, s_size]的格式，\n",
    "    idxs是[None, 1]的格式，在seq的第i个序列中选出第idxs[i]个向量，\n",
    "    最终输出[None, s_size]的向量。\n",
    "    \"\"\"\n",
    "    seq, idxs = x\n",
    "    batch_idxs = torch.arange(0,seq.size(0)).cuda()\n",
    "\n",
    "    batch_idxs = torch.unsqueeze(batch_idxs,1)\n",
    "\n",
    "    idxs = torch.cat([batch_idxs, idxs], 1)\n",
    "\n",
    "    res = []\n",
    "    for i in range(idxs.size(0)):\n",
    "        vec = seq[idxs[i][0],idxs[i][1],:]\n",
    "        res.append(torch.unsqueeze(vec,0))\n",
    "    \n",
    "    res = torch.cat(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "class s_model(nn.Module):\n",
    "    def __init__(self,word_dict_length,word_emb_size,lstm_hidden_size):\n",
    "        super(s_model,self).__init__()\n",
    "\n",
    "        self.embeds = nn.Embedding(word_dict_length, word_emb_size).cuda()\n",
    "        self.fc1_dropout = nn.Sequential(\n",
    "            nn.Dropout(0.25).cuda(),  # drop 20% of the neuron \n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size = word_emb_size,\n",
    "            hidden_size = int(word_emb_size/2),\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size = word_emb_size,\n",
    "            hidden_size = int(word_emb_size/2),\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        ).cuda()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=word_emb_size*2, #输入的深度\n",
    "                out_channels=word_emb_size,#filter 的个数，输出的高度\n",
    "                kernel_size = 3,#filter的长与宽\n",
    "                stride=1,#每隔多少步跳一下\n",
    "                padding=1,#周围围上一圈 if stride= 1, pading=(kernel_size-1)/2\n",
    "            ).cuda(),\n",
    "            nn.ReLU().cuda(),\n",
    "        ).cuda()\n",
    "        self.fc_ps1 = nn.Sequential(\n",
    "            nn.Linear(word_emb_size,1),\n",
    "        ).cuda()\n",
    "\n",
    "        self.fc_ps2 = nn.Sequential(\n",
    "            nn.Linear(word_emb_size,1),\n",
    "        ).cuda()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,t):\n",
    "        mask = torch.gt(torch.unsqueeze(t,2),0).type(torch.cuda.FloatTensor) #(batch_size,sent_len,1)\n",
    "        mask.requires_grad = False\n",
    "        outs = self.embeds(t)\n",
    "        t = outs\n",
    "        t = self.fc1_dropout(t)\n",
    "        t = t.mul(mask) # (batch_size,sent_len,char_size)\n",
    "        t, (h_n, c_n) = self.lstm1(t,None)\n",
    "        t, (h_n, c_n) = self.lstm2(t,None)\n",
    "        t_max,t_max_index = seq_max_pool([t,mask])\n",
    "        t_dim = list(t.size())[-1]\n",
    "        h = seq_and_vec([t, t_max])\n",
    "        h = h.permute(0,2,1)\n",
    "        h = self.conv1(h)\n",
    "        h = h.permute(0,2,1)\n",
    "        ps1 = self.fc_ps1(h)\n",
    "        ps2 = self.fc_ps2(h)\n",
    "        \n",
    "        return [ps1.cuda(),ps2.cuda(),t.cuda(),t_max.cuda(),mask.cuda()]\n",
    "\n",
    "class po_model(nn.Module):\n",
    "    def __init__(self,word_dict_length,word_emb_size,lstm_hidden_size,num_classes):\n",
    "        super(po_model,self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=word_emb_size*4, #输入的深度\n",
    "                out_channels=word_emb_size,#filter 的个数，输出的高度\n",
    "                kernel_size = 3,#filter的长与宽\n",
    "                stride=1,#每隔多少步跳一下\n",
    "                padding=1,#周围围上一圈 if stride= 1, pading=(kernel_size-1)/2\n",
    "            ).cuda(),\n",
    "            nn.ReLU().cuda(),\n",
    "        ).cuda()\n",
    "\n",
    "        self.fc_ps1 = nn.Sequential(\n",
    "            nn.Linear(word_emb_size,num_classes+1).cuda(),\n",
    "            # nn.Softmax(),\n",
    "        ).cuda()\n",
    "\n",
    "        self.fc_ps2 = nn.Sequential(\n",
    "            nn.Linear(word_emb_size,num_classes+1).cuda(),\n",
    "            # nn.Softmax(),\n",
    "        ).cuda()\n",
    "    \n",
    "    def forward(self,t,t_max,k1,k2):\n",
    "\n",
    "        k1 = seq_gather([t,k1])\n",
    "\n",
    "        k2 = seq_gather([t,k2])\n",
    "\n",
    "        k = torch.cat([k1,k2],1)\n",
    "        h = seq_and_vec([t,t_max])\n",
    "        h = seq_and_vec([h,k])\n",
    "        h = h.permute(0,2,1)\n",
    "        h = self.conv1(h)\n",
    "        h = h.permute(0,2,1)\n",
    "\n",
    "        po1 = self.fc_ps1(h)\n",
    "        po2 = self.fc_ps2(h)\n",
    "\n",
    "        return [po1.cuda(),po2.cuda()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e397797-d181-4dc0-bd45-2af763344fe0",
   "metadata": {},
   "source": [
    "### evaluate定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbb35fe-aa88-4d40-b683-f8c57416c33b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-02751fc91113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mmyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \"\"\"\n\u001b[1;32m     63\u001b[0m         \u001b[0m下载数据\u001b[0m\u001b[0;31m、\u001b[0m\u001b[0m初始化数据\u001b[0m\u001b[0;31m，\u001b[0m\u001b[0m都可以在这里完成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "class data_generator:\n",
    "    def __init__(self, data, batch_size=64):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def pro_res(self):\n",
    "        idxs = list(range(len(self.data)))\n",
    "        #print(idxs)\n",
    "        np.random.shuffle(idxs)\n",
    "        T, S1, S2, K1, K2, O1, O2, = [], [], [], [], [], [], []\n",
    "        for i in idxs:\n",
    "            d = self.data[i]\n",
    "        text = d['text']\n",
    "        items = {}\n",
    "        for sp in d['spo_list']:\n",
    "            subjectid = text.find(sp[0])\n",
    "            objectid = text.find(sp[2])\n",
    "            if subjectid != -1 and objectid != -1:\n",
    "                key = (subjectid, subjectid+len(sp[0]))\n",
    "                if key not in items:\n",
    "                    items[key] = []\n",
    "                    items[key].append(\n",
    "                        (objectid, objectid+len(sp[2]), predicate2id[sp[1]]))\n",
    "            if items:\n",
    "                T.append([char2id.get(c, 1) for c in text])  # 1是unk，0是padding\n",
    "            # s1, s2 = [[1,0]] * len(text), [[1,0]] * len(text)\n",
    "            s1, s2 = [0] * len(text), [0] * len(text)\n",
    "            for j in items:\n",
    "                # s1[j[0]] = [0,1]\n",
    "                # s2[j[1]-1] = [0,1]\n",
    "                s1[j[0]] = 1\n",
    "                s2[j[1]-1] = 1\n",
    "                #print(items.keys())\n",
    "                k1, k2 = choice(list(items.keys()))\n",
    "                o1, o2 = [0] * len(text), [0] * len(text)  # 0是unk类（共49+1个类）\n",
    "                for j in items[(k1, k2)]:\n",
    "                    o1[j[0]] = j[2]\n",
    "                    o2[j[1]-1] = j[2]\n",
    "                    S1.append(s1)\n",
    "                    S2.append(s2)\n",
    "                    K1.append([k1])\n",
    "                    K2.append([k2-1])\n",
    "                    O1.append(o1)\n",
    "                    O2.append(o2)\n",
    "\n",
    "        T = np.array(seq_padding(T))\n",
    "        S1 = np.array(seq_padding(S1))\n",
    "        S2 = np.array(seq_padding(S2))\n",
    "        O1 = np.array(seq_padding(O1))\n",
    "        O2 = np.array(seq_padding(O2))\n",
    "        K1, K2 = np.array(K1), np.array(K2)\n",
    "        return [T, S1, S2, K1, K2, O1, O2]\n",
    "\n",
    "\n",
    "class myDataset(Data.Dataset):\n",
    "    \"\"\"\n",
    "        下载数据、初始化数据，都可以在这里完成\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, _T, _S1, _S2, _K1, _K2, _O1, _O2):\n",
    "        #xy = np.loadtxt('../dataSet/diabetes.csv.gz', delimiter=',', dtype=np.float32) # 使用numpy读取数据\n",
    "        self.x_data = _T\n",
    "        self.y1_data = _S1\n",
    "        self.y2_data = _S2\n",
    "        self.k1_data = _K1\n",
    "        self.k2_data = _K2\n",
    "        self.o1_data = _O1\n",
    "        self.o2_data = _O2\n",
    "        self.len = len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y1_data[index], self.y2_data[index], self.k1_data[index], self.k2_data[index], self.o1_data[index], self.o2_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    t = np.array([item[0] for item in data], np.int32)\n",
    "    s1 = np.array([item[1] for item in data], np.int32)\n",
    "    s2 = np.array([item[2] for item in data], np.int32)\n",
    "    k1 = np.array([item[3] for item in data], np.int32)\n",
    "\n",
    "    k2 = np.array([item[4] for item in data], np.int32)\n",
    "    o1 = np.array([item[5] for item in data], np.int32)\n",
    "    o2 = np.array([item[6] for item in data], np.int32)\n",
    "    return {\n",
    "        'T': torch.LongTensor(t),  # targets_i\n",
    "        'S1': torch.FloatTensor(s1),\n",
    "        'S2': torch.FloatTensor(s2),\n",
    "        'K1': torch.LongTensor(k1),\n",
    "        'K2': torch.LongTensor(k2),\n",
    "        'O1': torch.LongTensor(o1),\n",
    "        'O2': torch.LongTensor(o2),\n",
    "    }\n",
    "\n",
    "def extract_items(text_in):\n",
    "    R = []\n",
    "    _s = [char2id.get(c, 1) for c in text_in]\n",
    "    _s = np.array([_s])\n",
    "    _k1, _k2, t, t_max, mask = s_m(torch.LongTensor(_s).cuda())\n",
    "    _k1, _k2 = _k1[0, :, 0], _k2[0, :, 0]\n",
    "    _kk1s = []\n",
    "    for i, _kk1 in enumerate(_k1):\n",
    "        if _kk1 > 0.5:\n",
    "            _subject = ''\n",
    "            for j, _kk2 in enumerate(_k2[i:]):\n",
    "                if _kk2 > 0.5:\n",
    "                    _subject = text_in[i: i+j+1]\n",
    "                    break\n",
    "            if _subject:\n",
    "                _k1, _k2 = torch.LongTensor([[i]]), torch.LongTensor(\n",
    "                    [[i+j]])  # np.array([i]), np.array([i+j])\n",
    "                _o1, _o2 = po_m(t.cuda(), t_max.cuda(), _k1.cuda(), _k2.cuda())\n",
    "                _o1, _o2 = _o1.cpu().data.numpy(), _o2.cpu().data.numpy()\n",
    "\n",
    "                _o1, _o2 = np.argmax(_o1[0], 1), np.argmax(_o2[0], 1)\n",
    "\n",
    "                for i, _oo1 in enumerate(_o1):\n",
    "                    if _oo1 > 0:\n",
    "                        for j, _oo2 in enumerate(_o2[i:]):\n",
    "                            if _oo2 == _oo1:\n",
    "                                _object = text_in[i: i+j+1]\n",
    "                                _predicate = id2predicate[_oo1]\n",
    "                                # print((_subject, _predicate, _object))\n",
    "                                R.append((_subject, _predicate, _object))\n",
    "                                break\n",
    "        _kk1s.append(_kk1.data.cpu().numpy())\n",
    "    _kk1s = np.array(_kk1s)\n",
    "    return list(set(R))\n",
    "\n",
    "def evaluate():\n",
    "    A, B, C = 1e-10, 1e-10, 1e-10\n",
    "    cnt = 0\n",
    "    for d in tqdm(iter(dev_data)):\n",
    "        R = set(extract_items(d['text']))\n",
    "        T = set([tuple(i) for i in d['spo_list']])\n",
    "        A += len(R & T)\n",
    "        B += len(R)\n",
    "        C += len(T)\n",
    "        # if cnt % 1000 == 0:\n",
    "        #     print('iter: %d f1: %.4f, precision: %.4f, recall: %.4f\\n' % (cnt, 2 * A / (B + C), A / B, A / C))\n",
    "        cnt += 1\n",
    "    return 2 * A / (B + C), A / B, A / C\n",
    "\n",
    "def get_now_time():\n",
    "    a = time.time()\n",
    "    return time.ctime(a)\n",
    "\n",
    "def seq_padding(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    #print(\"ML\",ML)\n",
    "    return [x + [0] * (ML - len(x)) for x in X]\n",
    "\n",
    "def seq_padding_vec(X):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    #print(\"ML\",ML)\n",
    "    return [x + [[1, 0]] * (ML - len(x)) for x in X]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c9b90-f4e6-43a2-b147-5e8bdbcf620a",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce10f5-4d71-4084-98f1-5fb75cf2bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from tqdm import tqdm\n",
    "import model\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "CHAR_SIZE = 128\n",
    "SENT_LENGTH = 4\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCH_NUM = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data = json.load(open(dataPath + '/train_data_me.json'))\n",
    "dev_data = json.load(open(dataPath + '/dev_data_me.json'))\n",
    "id2predicate, predicate2id = json.load(open(dataPath + '/all_50_schemas_me.json'))\n",
    "id2predicate = {int(i): j for i, j in id2predicate.items()}\n",
    "id2char, char2id = json.load(open(dataPath + '/all_chars_me.json'))\n",
    "num_classes = len(id2predicate)\n",
    "\n",
    "\n",
    "dg = data_generator(train_data)\n",
    "T, S1, S2, K1, K2, O1, O2 = dg.pro_res()\n",
    "# print(\"len\",len(T))\n",
    "\n",
    "torch_dataset = myDataset(T, S1, S2, K1, K2, O1, O2)\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset,      # torch TensorDataset format\n",
    "    batch_size=BATCH_SIZE,      # mini batch size\n",
    "    shuffle=True,               # random shuffle for training\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_fn,      # subprocesses for loading data\n",
    ")\n",
    "\n",
    "# print(\"len\",len(id2char))\n",
    "s_m = model.s_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE).cuda()\n",
    "po_m = model.po_model(len(char2id)+2, CHAR_SIZE, HIDDEN_SIZE, 49).cuda()\n",
    "params = list(s_m.parameters())\n",
    "\n",
    "params += list(po_m.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss().cuda()\n",
    "b_loss = torch.nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "best_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for i in range(EPOCH_NUM):\n",
    "    for step, loader_res in tqdm(iter(enumerate(loader))):\n",
    "        # print(get_now_time())\n",
    "        t_s = loader_res[\"T\"].cuda()\n",
    "        k1 = loader_res[\"K1\"].cuda()\n",
    "        k2 = loader_res[\"K2\"].cuda()\n",
    "        s1 = loader_res[\"S1\"].cuda()\n",
    "        s2 = loader_res[\"S2\"].cuda()\n",
    "        o1 = loader_res[\"O1\"].cuda()\n",
    "        o2 = loader_res[\"O2\"].cuda()\n",
    "\n",
    "        ps_1, ps_2, t, t_max, mask = s_m(t_s)\n",
    "\n",
    "        t, t_max, k1, k2 = t.cuda(), t_max.cuda(), k1.cuda(), k2.cuda()\n",
    "        po_1, po_2 = po_m(t, t_max, k1, k2)\n",
    "\n",
    "        ps_1 = ps_1.cuda()\n",
    "        ps_2 = ps_2.cuda()\n",
    "        po_1 = po_1.cuda()\n",
    "        po_2 = po_2.cuda()\n",
    "\n",
    "        s1 = torch.unsqueeze(s1, 2)\n",
    "        s2 = torch.unsqueeze(s2, 2)\n",
    "\n",
    "        s1_loss = b_loss(ps_1, s1)\n",
    "        s1_loss = torch.sum(s1_loss.mul(mask))/torch.sum(mask)\n",
    "        s2_loss = b_loss(ps_2, s2)\n",
    "        s2_loss = torch.sum(s2_loss.mul(mask))/torch.sum(mask)\n",
    "\n",
    "        po_1 = po_1.permute(0, 2, 1)\n",
    "        po_2 = po_2.permute(0, 2, 1)\n",
    "\n",
    "        o1_loss = loss(po_1, o1)\n",
    "        o1_loss = torch.sum(o1_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "        o2_loss = loss(po_2, o2)\n",
    "        o2_loss = torch.sum(o2_loss.mul(mask[:, :, 0])) / torch.sum(mask)\n",
    "\n",
    "        loss_sum = 2.5 * (s1_loss + s2_loss) + (o1_loss + o2_loss)\n",
    "\n",
    "        # if step % 500 == 0:\n",
    "        #     torch.save(s_m, 'models_real/s_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "        #     torch.save(po_m, 'models_real/po_'+str(step)+\"epoch_\"+str(i)+'.pkl')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_sum.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    torch.save(s_m, 'models_real/s_'+str(i)+'.pkl')\n",
    "    torch.save(po_m, 'models_real/po_'+str(i)+'.pkl')\n",
    "    f1, precision, recall = evaluate()\n",
    "\n",
    "    print(\"epoch:\", i, \"loss:\", loss_sum.data)\n",
    "\n",
    "    if f1 >= best_f1:\n",
    "        best_f1 = f1\n",
    "        best_epoch = i\n",
    "\n",
    "    print('f1: %.4f, precision: %.4f, recall: %.4f, bestf1: %.4f, bestepoch: %d \\n ' %\n",
    "          (f1, precision, recall, best_f1, best_epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
