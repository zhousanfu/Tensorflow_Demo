{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import zhconv #繁体字转换\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)  #设置GPU显存用量按需使用\n",
    "    tf.config.set_visible_devices([gpus[0]],\"GPU\")\n",
    "print('tensorflow version {}'.format(tf.__version__))\n",
    "# tf.keras.layers.experimental.preprocessing.TextVectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/文本分类/sms_pub.csv')[:100000]\n",
    "target = df.pop('label')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df[['message']].values, target.values))\n",
    "\n",
    "dataset.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv make_csv_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "dataset = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern='../data/文本分类/sms_pub.csv',\n",
    "    field_delim=',',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_name=\"label\",\n",
    "    select_columns=['message', 'label'],\n",
    "    shuffle=True\n",
    ")  # .map(split_line, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# print(next(iter(dataset)), '\\n', dataset.take(1))\n",
    "# print(type(dataset))\n",
    "# for feature_batch, label_batch in dataset.take(1):\n",
    "#     print(len(label_batch), len(feature_batch['message']))\n",
    "#     for i in range(1):\n",
    "#       print(label_batch[i], '\\n', feature_batch['message'][i][:2])\n",
    "dataset.element_spec\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### txt text_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "\n",
    "ds_data = tf.keras.utils.text_dataset_from_directory(\n",
    "    directory=['../data/文本分类'],\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "ds_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaan_str(string):\n",
    "    string = re.sub(u'[\\u4e00-\\u9fa5]+', ' ', string)\n",
    "    string = zhconv.convert(string.strip(), 'zh-hans')\n",
    "\n",
    "def load_data(data_file):\n",
    "    lines = list(open(data_file, 'r', encoding='utf-8').readlines())\n",
    "    y = [line[:1] for line in lines]\n",
    "    x = [clean_str(line[1:] for lin in lines)]\n",
    "    return [x, y]\n",
    "\n",
    "    return string\n",
    "\n",
    "\n",
    "load_data(data_file='../data/文本分类/sms_pub.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000   # 仅考虑最高频的10000个词\n",
    "MAX_LEN = 250       # 每个样本保留200个词的长度\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "\n",
    "#构建词典\n",
    "def clean_text(text):\n",
    "    lowercase = tf.strings.lower(text)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    cleaned_punctuation = tf.strings.regex_replace(stripped_html, '[%s]' % re.escape(string.punctuation),'')\n",
    "    return cleaned_punctuation\n",
    "\n",
    "binary_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    # standardize=clean_text, # 标准化是指预处理文本，通常是移除标点符号或 HTML 元素以简化数据集。\n",
    "    # split='whitespace',     # 分词器会按空格分割 (split='whitespace')。\n",
    "    max_tokens=MAX_WORDS-1, # 有一个留给占位符\n",
    "    output_mode='binary',      # 默认向量化模式为 'int'整数索引每个词例一个id, 'binary'来构建词袋模型。\n",
    "    )\n",
    "int_vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    # standardize=clean_text, # 标准化是指预处理文本，通常是移除标点符号或 HTML 元素以简化数据集。\n",
    "    # split='whitespace',     # 分词器会按空格分割 (split='whitespace')。\n",
    "    max_tokens=MAX_WORDS-1, # 有一个留给占位符\n",
    "    output_mode='int',      # 默认向量化模式为 'int'整数索引每个词例一个id, 'binary'来构建词袋模型。\n",
    "    output_sequence_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "ds_dataset = dataset.map(lambda text, label: text)\n",
    "binary_vectorize_layer.adapt(ds_dataset)\n",
    "int_vectorize_layer.adapt(ds_dataset)\n",
    "print(binary_vectorize_layer.get_vocabulary()[0:100])\n",
    "print(int_vectorize_layer.get_vocabulary()[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return int_vectorize_layer(text), label\n",
    "\n",
    "def binary_vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return binary_vectorize_layer(text), label\n",
    "\n",
    "text_batch, label_batch = next(iter(dataset))\n",
    "first_question, first_label = text_batch, label_batch\n",
    "print(\"Question\", first_question)\n",
    "print(\"Label\", first_label)\n",
    "\n",
    "print(\"'binary' vectorized question:\",\n",
    "      binary_vectorize_text(first_question, first_label)[0])\n",
    "print(\"'int' vectorized question:\",\n",
    "      int_vectorize_text(first_question, first_label)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1289 ---> \", int_vectorize_layer.get_vocabulary()[1289])\n",
    "print(\"313 ---> \", int_vectorize_layer.get_vocabulary()[313])\n",
    "print(\"Vocabulary size: {}\".format(len(int_vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_train_ds = dataset.map(binary_vectorize_text)\n",
    "int_train_ds = dataset.map(int_vectorize_text)\n",
    "\n",
    "binary_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.prefetch 会在训练时将数据预处理和模型执行重叠。\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def configure_dataset(dataset):\n",
    "  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "binary_train_ds = configure_dataset(binary_train_ds)\n",
    "int_train_ds = configure_dataset(int_train_ds)\n",
    "\n",
    "binary_train_ds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "binary_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4)\n",
    "    ])\n",
    "binary_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "int_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(MAX_WORDS + 1, 64, mask_zero=True),\n",
    "    tf.keras.layers.Conv1D(64, 5, padding=\"valid\", activation=\"relu\", strides=2),\n",
    "    tf.keras.layers.GlobalMaxPooling1D(),\n",
    "    tf.keras.layers.Dense(2) #num_labels\n",
    "])\n",
    "int_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# history = binary_model.fit(int_train_ds, validation_data=0.2, epochs=10)\n",
    "history = int_model.fit(int_train_ds, validation_data=0.2, epochs=5)\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  int_vectorize_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(dataset.shuffle(500), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
